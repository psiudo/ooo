{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psiudo/ooo/blob/main/v11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_odh6XsYqQG",
        "outputId": "55d2e32f-16a6-43a3-9b9f-ec705d036f48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Google Drive가 성공적으로 마운트되었습니다.\n"
          ]
        }
      ],
      "source": [
        "# ==================================\n",
        "# 셀 1: Google Drive 마운트\n",
        "# ==================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"✅ Google Drive가 성공적으로 마운트되었습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUAVUj5OZBE-",
        "outputId": "c91ff690-9919-4507-a681-ffaf9e4422ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.6.15)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "✅ 필수 라이브러리가 준비되었습니다.\n"
          ]
        }
      ],
      "source": [
        "# ==================================\n",
        "# 셀 2: 필수 라이브러리 설치\n",
        "# ==================================\n",
        "!pip install --upgrade torch-geometric\n",
        "print(\"✅ 필수 라이브러리가 준비되었습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTEBlsvyN9Aj",
        "outputId": "0568596b-5823-41f2-8691-f7db7068780a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba) (0.43.0)\n",
            "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.11/dist-packages (from numba) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mIPxSNJZeo6",
        "outputId": "47cbaae0-f1df-43e5-86a1-2c9b4d68e8dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-03 08:24:21 - INFO - Using device: cuda\n",
            "✅ CUDA 활성화 확인\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEBUG: >>>>>>>>>> STEP 1: Calling pretrain_with_imitation NOW...\n",
            "2025-07-03 08:26:43 - INFO - [Phase 1] Starting Imitation Learning for Worker Agent…\n",
            "2025-07-03 08:26:45 - INFO -   · Total Samples: 5,420\n",
            "2025-07-03 08:27:57 - INFO -     Epoch   10/200  avg-loss 11.2702\n",
            "2025-07-03 08:29:10 - INFO -     Epoch   20/200  avg-loss 9.8362\n",
            "2025-07-03 08:30:22 - INFO -     Epoch   30/200  avg-loss 8.7999\n",
            "2025-07-03 08:31:34 - INFO -     Epoch   40/200  avg-loss 8.1010\n",
            "2025-07-03 08:32:47 - INFO -     Epoch   50/200  avg-loss 7.6587\n",
            "2025-07-03 08:33:59 - INFO -     Epoch   60/200  avg-loss 7.2841\n",
            "2025-07-03 08:35:12 - INFO -     Epoch   70/200  avg-loss 7.0093\n",
            "2025-07-03 08:36:24 - INFO -     Epoch   80/200  avg-loss 6.7883\n",
            "2025-07-03 08:37:37 - INFO -     Epoch   90/200  avg-loss 6.5952\n",
            "2025-07-03 08:38:50 - INFO -     Epoch  100/200  avg-loss 6.4202\n",
            "2025-07-03 08:40:02 - INFO -     Epoch  110/200  avg-loss 6.2706\n"
          ]
        }
      ],
      "source": [
        "########## v11 : HRL 트랜스포머 GNN 보상함수의 unambiguity ##########\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 섹션 1: 설정 및 기본 유틸리티 (리팩토링 버전)\n",
        "# ==============================================================================\n",
        "import json\n",
        "import torch.optim as optim\n",
        "import collections\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "import logging\n",
        "from torch_geometric.data import Data\n",
        "import pickle\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "import logging\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from collections import defaultdict, deque\n",
        "import multiprocessing as mp\n",
        "import random\n",
        "\n",
        "import numba\n",
        "from numba.core import types\n",
        "from numba.typed import List\n",
        "\n",
        "def setup_logger():\n",
        "    \"\"\"\n",
        "    다른 라이브러리의 로깅 설정을 모두 초기화하고,\n",
        "    우리가 원하는 설정으로 강제 적용하는 함수.\n",
        "    \"\"\"\n",
        "    # 루트 로거를 가져옵니다.\n",
        "    root_logger = logging.getLogger()\n",
        "    root_logger.setLevel(logging.INFO) # 로그 레벨 설정\n",
        "\n",
        "    # 루트 로거에 연결된 모든 기존 핸들러(Handler)를 제거합니다. (가장 중요)\n",
        "    for handler in root_logger.handlers[:]:\n",
        "        root_logger.removeHandler(handler)\n",
        "\n",
        "    # 우리가 원하는 새로운 핸들러를 생성하여 추가합니다.\n",
        "    handler = logging.StreamHandler(sys.stdout) # 로그를 콘솔에 출력\n",
        "    formatter = logging.Formatter(\n",
        "        '%(asctime)s - %(levelname)s - %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S'\n",
        "    )\n",
        "    handler.setFormatter(formatter)\n",
        "    root_logger.addHandler(handler)\n",
        "\n",
        "\n",
        "class Config:\n",
        "    \"\"\"학습과 관련된 모든 하이퍼파라미터와 설정을 관리하는 클래스\"\"\"\n",
        "\n",
        "    # --- 경로 설정 ---\n",
        "    DRIVE_PROJECT_ROOT = '/content/drive/MyDrive/OptiChallenge'\n",
        "    PROBLEM_DIR  = os.path.join(DRIVE_PROJECT_ROOT, 'Exercise_Problems')\n",
        "    LOG_DIR      = os.path.join(DRIVE_PROJECT_ROOT, 'v12_logs_refactored')\n",
        "    MODEL_DIR    = os.path.join(DRIVE_PROJECT_ROOT, 'v12_models_refactored')\n",
        "\n",
        "    # 💡 새 expert pkl  ‘expert_probX_TYY.pkl’ 형식으로 있다고 가정\n",
        "    EXPERT_DIR   = os.path.join(DRIVE_PROJECT_ROOT)\n",
        "    EXPERT_GLOB  = \"expert_prob[1248]_T*.pkl\"\n",
        "    MAX_EXPERT_SAMPLES = 40_000\n",
        "\n",
        "    @property\n",
        "    def EXPERT_DATA_PATHS(self):\n",
        "        import glob\n",
        "        paths = sorted(glob.glob(os.path.join(self.EXPERT_DIR, self.EXPERT_GLOB)))\n",
        "        if not paths:\n",
        "            logging.warning(f\"[Config] No expert pkl found under {self.EXPERT_DIR}\")\n",
        "        return paths\n",
        "\n",
        "    # --- 학습 제어 ---\n",
        "    TOTAL_MANAGER_STEPS = 500_000       # Manager 에이전트의 총 학습 스텝\n",
        "    CURRICULUM_STEPS = 2000            # 전문가 정책을 모방하는 커리큘럼 학습 스텝 수\n",
        "    PRINT_INTERVAL_MANAGER_STEPS = 20  # 학습 중 로그 출력 간격 (Manager 스텝 기준)\n",
        "    EVAL_INTERVAL_MANAGER_STEPS = 2000 # 모델 평가 및 저장 간격 (Manager 스텝 기준)\n",
        "    EVAL_EPISODES = 10                  # 평가 시 실행할 에피소드 수\n",
        "    CURRICULUM_TRANSITION_STEP = 20000 # 💡 [추가] 문제 난이도 커리큘럼 전환 시점\n",
        "\n",
        "\n",
        "    # --- 모방 학습 (Worker) ---\n",
        "    IMITATION_LEARNING_EPOCHS = 200      # Worker 모방 학습 에폭 수\n",
        "    IMITATION_LR = 1e-4                 # Worker 모방 학습 Learning Rate\n",
        "    IMITATION_BATCH_SIZE = 512          # Worker 모방 학습 배치 크기\n",
        "\n",
        "    # --- Manager 에이전트 설정 ---\n",
        "    MANAGER_STATE_DIM = 6               # Manager 상태 벡터의 차원\n",
        "    MANAGER_ACTION_DIM = 5              # Manager 행동의 가짓수\n",
        "    MANAGER_LR = 3e-4                   # Manager Learning Rate\n",
        "    MANAGER_GAMMA = 0.99                # Manager 할인율 (Gamma)\n",
        "    MANAGER_ENTROPY_COEF = 0.05         # Manager 엔트로피 보너스 계수 (탐험 장려)\n",
        "    MANAGER_NUM_STEPS_PER_UPDATE = 512  # Manager 업데이트를 위한 데이터 수집 스텝\n",
        "\n",
        "    # [신규] 보상 체계 하이퍼파라미터\n",
        "    REPEAT_PENALTY = -1             # 같은 행동 반복 페널티\n",
        "    STEP_PENALTY_WEIGHT = 0.001       # Worker 스텝당 시간 페널티 가중치\n",
        "    NO_PROGRESS_PENALTY = -2.0        # Worker가 진척 없이 종료 시 페널티\n",
        "    SHAPING_REWARD_WEIGHT = 1.5       # PBRS 보상 강도 조절 계수\n",
        "    NO_PROGRESS_LIMIT = 50            # Worker 진척 판정 한도 (기존 50 하드코딩 값 대체)\\\n",
        "    TIMEOUT_PENALTY = -10.0\n",
        "\n",
        "    # --- Worker 에이전트 설정 ---\n",
        "    WORKER_LR = 3e-4                    # Worker Learning Rate\n",
        "    WORKER_GAMMA = 0.95                 # Worker 할인율 (Gamma)\n",
        "    WORKER_ENTROPY_COEF = 0.005         # Worker 엔트로피 보너스 계수\n",
        "    WORKER_MAX_STEPS_PER_GOAL = 300     # Manager의 목표 하나당 Worker가 수행할 최대 스텝\n",
        "    WORKER_NUM_STEPS_PER_UPDATE = 1024\n",
        "\n",
        "    # --- PPO 알고리즘 공통 설정 ---\n",
        "    PPO_UPDATE_EPOCHS = 4               # 한 번의 업데이트 시 에폭 수\n",
        "    PPO_NUM_MINIBATCHES = 8             # 미니배치 개수\n",
        "    PPO_CLIP_COEF = 0.2                 # PPO 클리핑 계수\n",
        "    PPO_GAE_LAMBDA = 0.95               # GAE(Generalized Advantage Estimation) 람다값\n",
        "    PPO_VALUE_COEF = 1.0                # 가치 함수 손실(Value Loss) 계수\n",
        "    PPO_MAX_GRAD_NORM = 0.5             # Gradient Clipping 최대 L2 Norm\n",
        "\n",
        "    # --- 네트워크 구조 설정 ---\n",
        "    NODE_FEATURE_DIM = 4      # [is_occupied, dest_diff, blocking_count, is_relocatable]\n",
        "    GNN_EMBED_DIM = 128       # GNN의 기본 임베딩 차원\n",
        "    GOAL_EMBED_DIM = 16       # 목표 임베딩 벡터 차원\n",
        "    # Worker의 GNN 출력을 Manager 상태로 사용 (mean_pool + att_pool)\n",
        "    # MANAGER_STATE_DIM = GNN_EMBED_DIM * 2\n",
        "\n",
        "# --- 경로 생성 ---\n",
        "# 학습 로그와 모델 가중치를 저장할 디렉토리를 생성합니다.\n",
        "os.makedirs(Config.LOG_DIR, exist_ok=True)\n",
        "os.makedirs(Config.MODEL_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "# 💡 --- 이 함수로 기존의 모든 get_shortest_path 관련 함수를 교체 --- 💡\n",
        "@numba.jit(nopython=True)\n",
        "def get_shortest_path(adj_list, start, end, num_nodes):\n",
        "    \"\"\"\n",
        "    Numba에 완벽히 호환되는 가장 표준적이고 안정적인 BFS 함수.\n",
        "    - 부모 노드 추적 방식을 사용\n",
        "    - NumPy 배열과 기본 리스트만 사용\n",
        "    \"\"\"\n",
        "    if start == end:\n",
        "        # Numba를 위해 타입을 명시적으로 리스트로 생성\n",
        "        path = numba.typed.List()\n",
        "        path.append(start)\n",
        "        return path\n",
        "\n",
        "    # 부모 노드를 기록할 NumPy 배열 (-1로 초기화)\n",
        "    parents = np.full(num_nodes, -1, dtype=np.int64)\n",
        "\n",
        "    # 방문 기록을 위한 boolean NumPy 배열\n",
        "    visited = np.zeros(num_nodes, dtype=np.bool_)\n",
        "\n",
        "    # 큐로 사용할 단순 리스트\n",
        "    queue = numba.typed.List()\n",
        "\n",
        "    queue.append(start)\n",
        "    visited[start] = True\n",
        "    head = 0 # 큐의 맨 앞을 가리키는 포인터\n",
        "\n",
        "    path_found = False\n",
        "    while head < len(queue):\n",
        "        current = queue[head]\n",
        "        head += 1\n",
        "\n",
        "        if current == end:\n",
        "            path_found = True\n",
        "            break\n",
        "\n",
        "        for neighbor in adj_list[current]:\n",
        "            if not visited[neighbor]:\n",
        "                visited[neighbor] = True\n",
        "                parents[neighbor] = current\n",
        "                queue.append(neighbor)\n",
        "\n",
        "    # 경로 역추적\n",
        "    if path_found:\n",
        "        path = numba.typed.List()\n",
        "        curr = end\n",
        "        while curr != -1:\n",
        "            path.append(curr)\n",
        "            curr = parents[curr]\n",
        "        return path[::-1] # 역순이므로 뒤집어서 반환\n",
        "\n",
        "    return numba.typed.List.empty_list(numba.int64)\n",
        "\n",
        "# 💡 --- 교체 완료 --- 💡\n",
        "\n",
        "class ShipEnv:\n",
        "    \"\"\"화물선의 상태와 행동을 시뮬레이션하는 환경 클래스 (Numba 최종 최적화 적용)\"\"\"\n",
        "    def __init__(self, problem_data: dict, max_num_ports: int):\n",
        "        self.num_nodes = problem_data.get('N', 1)\n",
        "        self.num_ports = problem_data.get('P', 1)\n",
        "        self.fixed_cost = float(problem_data.get('F', 100))\n",
        "        self.max_num_ports = max_num_ports\n",
        "        self.load_fail_streak: int = 0\n",
        "\n",
        "        # 💡 --- [핵심] Numba 호환을 위한 그래프 데이터 구조화 --- 💡\n",
        "        adj_list = [numba.typed.List.empty_list(numba.int64) for _ in range(self.num_nodes)]\n",
        "        edge_list_for_tensor = []\n",
        "        for u, v in problem_data.get('E', []):\n",
        "            adj_list[u].append(v)\n",
        "            adj_list[v].append(u)\n",
        "            edge_list_for_tensor.extend([[u, v], [v, u]])\n",
        "        self.adj_list = adj_list # Numba 함수에 넘겨주기 위해 저장\n",
        "        # 💡 --- 수정 완료 --- 💡\n",
        "\n",
        "        self.edge_index_tensor = torch.tensor(edge_list_for_tensor, dtype=torch.long).t().contiguous()\n",
        "\n",
        "        # 모든 노드 쌍 간의 최단 경로를 미리 계산하여 캐싱\n",
        "        self.shortest_paths = {}\n",
        "        for i in range(self.num_nodes):\n",
        "            self.shortest_paths[i] = {}\n",
        "            for j in range(self.num_nodes):\n",
        "                # 💡 [핵심] 수정된 get_shortest_path 함수 호출\n",
        "                path_result = get_shortest_path(self.adj_list, i, j, self.num_nodes)\n",
        "                self.shortest_paths[i][j] = list(path_result) # 결과를 일반 리스트로 저장\n",
        "\n",
        "        self.cars = []\n",
        "        car_id_counter = 0\n",
        "        for demand_idx, (demand, quantity) in enumerate(problem_data.get('K', [])):\n",
        "            origin, dest = demand\n",
        "            for _ in range(quantity):\n",
        "                self.cars.append({'id': car_id_counter, 'demand_id': demand_idx, 'origin': origin, 'dest': dest})\n",
        "                car_id_counter += 1\n",
        "        self.total_cars = len(self.cars)\n",
        "        self.reset()\n",
        "\n",
        "    def _get_or_compute_path(self, start: int, end: int) -> list | None:\n",
        "        \"\"\"미리 계산된 경로를 캐시에서 조회합니다.\"\"\"\n",
        "        return self.shortest_paths.get(start, {}).get(end, None)\n",
        "\n",
        "    def _calculate_path_cost(self, path: list | None) -> float:\n",
        "        if not path or len(path) <= 1: return 0.0\n",
        "        return float(self.fixed_cost + (len(path) - 1))\n",
        "\n",
        "    def reset(self) -> Data:\n",
        "        self.current_port: int = 0\n",
        "        self.node_status: list[int] = [-1] * self.num_nodes\n",
        "        self.car_locations: dict[int, int] = {}\n",
        "        self.cars_on_board: set[int] = set()\n",
        "        self.temporarily_unloaded_cars: set[int] = set()\n",
        "        self.delivered_cars: set[int] = set()\n",
        "        self.relocations_this_episode: int = 0\n",
        "        self.last_car_action = {}\n",
        "        # [NEW] 에피소드가 바뀌면 실패 누적도 초기화\n",
        "        self.load_fail_streak: int = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self) -> Data:\n",
        "      node_features = []\n",
        "      for i in range(self.num_nodes):\n",
        "          if i == 0:\n",
        "              node_features.append([0.0, 0.0, 0.0, 0.0]); continue\n",
        "          car_id = self.node_status[i]\n",
        "          if car_id == -1:\n",
        "              node_features.append([0.0, 0.0, 0.0, 0.0])\n",
        "          else:\n",
        "              car = self.cars[car_id]\n",
        "              dest_diff = float(car['dest'] - self.current_port)\n",
        "              path_to_gate = self._get_or_compute_path(i, 0)\n",
        "\n",
        "              # 💡 --- [핵심 최적화] 경로 리스트를 set으로 변환하여 확인 속도 향상 --- 💡\n",
        "              if path_to_gate:\n",
        "                  path_to_gate_set = set(path_to_gate[1:])\n",
        "                  blocking_count = sum(1 for node_idx, status in enumerate(self.node_status)\n",
        "                                      if status != -1 and node_idx in path_to_gate_set)\n",
        "              else:\n",
        "                  blocking_count = 0\n",
        "              # 💡 --- 최적화 완료 --- 💡\n",
        "\n",
        "              is_relocatable = 1.0 if car['dest'] != self.current_port else 0.0\n",
        "              node_features.append([1.0, dest_diff, float(blocking_count), is_relocatable])\n",
        "\n",
        "\n",
        "      waiting_cars = [c for c in self.cars if c['origin'] == self.current_port and c['id'] not in self.cars_on_board and c['id'] not in self.temporarily_unloaded_cars]\n",
        "      waiting_dest_counts = [0.0] * self.max_num_ports\n",
        "      for car in waiting_cars:\n",
        "          if car['dest'] < self.max_num_ports: waiting_dest_counts[car['dest']] += 1.0\n",
        "      global_features = [float(self.current_port), float(len(waiting_cars)), float(len(self.temporarily_unloaded_cars))] + waiting_dest_counts\n",
        "\n",
        "      return Data(x=torch.tensor(node_features, dtype=torch.float),\n",
        "                  edge_index=self.edge_index_tensor,\n",
        "                  global_features=torch.tensor([global_features], dtype=torch.float))\n",
        "\n",
        "    def get_legal_actions(self, for_worker: bool = False) -> list[tuple[str, int]]:\n",
        "        actions = []\n",
        "        any_empty_spot = any(status == -1 for status in self.node_status[1:])\n",
        "        if any_empty_spot:\n",
        "            load_candidates = [c for c in self.cars if (c['origin'] == self.current_port and c['id'] not in self.cars_on_board and c['id'] not in self.temporarily_unloaded_cars) or c['id'] in self.temporarily_unloaded_cars]\n",
        "            for car in load_candidates: actions.append(('LOAD', car['id']))\n",
        "            for car_id in self.cars_on_board: actions.append(('RELOCATE_INTERNAL', car_id))\n",
        "        for car_id in self.cars_on_board: actions.append(('UNLOAD', car_id))\n",
        "        if not for_worker: actions.append(('PROCEED_TO_NEXT_PORT', -1))\n",
        "        return actions\n",
        "\n",
        "\n",
        "    # ShipEnv 내 (기존 메소드들 바로 위/아래 아무 곳)\n",
        "    def _is_hard_blocker(self, node_idx: int) -> bool:\n",
        "        \"\"\"\n",
        "        게이트에서 가깝고 곧 빠질 차는 ‘소프트 블로커’로 보고\n",
        "        정말 치워야만 통과 가능한 차만 True.\n",
        "        \"\"\"\n",
        "        cid = self.node_status[node_idx]\n",
        "        if cid == -1:\n",
        "            return False\n",
        "\n",
        "        car = self.cars[cid]\n",
        "\n",
        "        # (1) 지금 하역 항구면 금방 내린다 → False\n",
        "        if car['dest'] == self.current_port:\n",
        "            return False\n",
        "        # (2) 게이트에서 두-세 칸 이내(manhattan depth <3)면 통과 대기열 → False\n",
        "        gate_depth = len(self._get_or_compute_path(node_idx, 0)) - 1\n",
        "        return gate_depth >= 3\n",
        "\n",
        "\n",
        "    def _find_best_spot(self, car_id_to_load: int) -> tuple[int, list]:\n",
        "        car_dest = self.cars[car_id_to_load]['dest']\n",
        "        cars_to_leave_later = {\n",
        "            cid for cid in self.cars_on_board\n",
        "            if self.cars[cid]['dest'] > car_dest\n",
        "        }\n",
        "\n",
        "        best = None\n",
        "        for spot in range(1, self.num_nodes):\n",
        "            if self.node_status[spot] != -1:\n",
        "                continue\n",
        "            path = self._get_or_compute_path(0, spot)\n",
        "            if not path:\n",
        "                continue\n",
        "\n",
        "            # ── 차단 분석 ────────────────────────────────────────────\n",
        "            path_set = set(path[1:])\n",
        "            hard_blocks = sum(self._is_hard_blocker(n) for n in path_set)\n",
        "\n",
        "            # (1) 하드 블록 2개 초과면 후보 자체를 버린다\n",
        "            HARD_BLOCK_LIMIT = 2\n",
        "            if hard_blocks > HARD_BLOCK_LIMIT:\n",
        "                continue\n",
        "\n",
        "            soft_blocks = sum(\n",
        "                1 for n in path_set\n",
        "                if self.node_status[n] != -1\n",
        "            ) - hard_blocks\n",
        "            later_blocks = sum(\n",
        "                1 for cid in cars_to_leave_later\n",
        "                if self.car_locations[cid] in path_set\n",
        "            )\n",
        "\n",
        "            dist = len(path) - 1\n",
        "            dest_penalty = abs(car_dest - self.current_port) ** 0.5\n",
        "\n",
        "            score = (\n",
        "                1.0 * hard_blocks   +   # 반드시 치워야 할 벽\n",
        "                0.3 * soft_blocks   +   # 곧 빠질 벽\n",
        "                0.7 * later_blocks  +   # 나중 출항 차를 막는 정도\n",
        "                0.02 * dist         +   # 멀리 밀어넣기\n",
        "                0.01 * dest_penalty     # 목적지 편차 완화\n",
        "            )\n",
        "\n",
        "\n",
        "            score += random.uniform(0.0, 0.001)   # tiny tie-breaker\n",
        "\n",
        "            cand = (score, spot, path)\n",
        "            best = min(best, cand) if best else cand\n",
        "\n",
        "        return (-1, []) if best is None else (best[1], best[2])\n",
        "\n",
        "    def _find_best_internal_spot(self, car_id_to_move: int) -> tuple[int, list]:\n",
        "        start = self.car_locations.get(car_id_to_move, None)\n",
        "        if start is None:\n",
        "            return -1, []\n",
        "\n",
        "        tried: set[int] = set()\n",
        "        MAX_RETRY = 5                       # 다른 자리 최대 5개까지 시도\n",
        "        for _ in range(MAX_RETRY):\n",
        "            # ── 최솟값 후보 선정 (이전과 동일 로직) ─────────\n",
        "            best = None\n",
        "            for spot in range(1, self.num_nodes):\n",
        "                if spot in tried or self.node_status[spot] != -1:\n",
        "                    continue\n",
        "                path = self._get_or_compute_path(start, spot)\n",
        "                if not path:\n",
        "                    continue\n",
        "                inner = set(path[1:-1])\n",
        "                if any(self.node_status[n] != -1 for n in inner):\n",
        "                    continue\n",
        "                unblock_gain = sum(\n",
        "                    1 for cid in self.cars_on_board\n",
        "                    if self.cars[cid]['dest'] == self.current_port and\n",
        "                      self.car_locations[cid] in inner\n",
        "                )\n",
        "                dist       = len(path) - 1\n",
        "                gate_depth = dist\n",
        "                score      = 0.5 * dist - 0.8 * unblock_gain + 0.05 * gate_depth\n",
        "                best       = min(best, (score, spot, path)) if best else (score, spot, path)\n",
        "\n",
        "            # 후보가 없었다\n",
        "            if best is None:\n",
        "                return -1, []\n",
        "\n",
        "            spot, path = best[1], best[2]\n",
        "            # 실제로 길이 깨끗하면 여기서 끝\n",
        "            if not any(self.node_status[n] != -1 for n in path[1:-1]):\n",
        "                return spot, path\n",
        "\n",
        "            # 아니면 다시 시도\n",
        "            tried.add(spot)\n",
        "\n",
        "        return -1, []\n",
        "\n",
        "    def step(\n",
        "        self,\n",
        "        action: tuple[str, int]\n",
        "    ) -> tuple[Data, float, float, bool]:\n",
        "        \"\"\"\n",
        "        ShipEnv 1-step 시뮬레이션.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        action : (action_type:str, car_id:int)\n",
        "            * action_type ∈ {'LOAD','UNLOAD','RELOCATE_INTERNAL'}\n",
        "            * car_id == -1 는 PORT 단에서만 쓰는 dummy 값\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        state  : Data   ─ PyG 그래프(노드특성·글로벌특성 포함)\n",
        "        reward : float  ─ worker-level scalar reward\n",
        "        cost   : float  ─ pure path-moving cost (고정+거리)\n",
        "        done   : bool   ─ 항로가 끝나면 True\n",
        "        \"\"\"\n",
        "        act_type, cid = action\n",
        "        cost = 0.0\n",
        "        event_rew = 0.0                       # (extrinsic + intrinsic)\n",
        "\n",
        "        # ────────────────────────────────────────────────────\n",
        "        # 0) 반복/핑퐁 패널티  (cid == -1 이면 생략)\n",
        "        # ────────────────────────────────────────────────────\n",
        "        if cid != -1:\n",
        "            prev = self.last_car_action.get(cid)\n",
        "\n",
        "            # (a) 같은 행동 타입 연속            → 살짝 -0.1\n",
        "            if prev == act_type:\n",
        "                event_rew -= 0.5\n",
        "\n",
        "            # (b) 직전 UNLOAD → 곧바로 LOAD     → 강하게 -5.0\n",
        "            elif prev == 'UNLOAD' and act_type == 'LOAD':\n",
        "                event_rew -= 5.0\n",
        "\n",
        "            # 기록 갱신\n",
        "            self.last_car_action[cid] = act_type\n",
        "\n",
        "        # ────────────────────────────────────────────────────\n",
        "        # 1) LOAD\n",
        "        # ────────────────────────────────────────────────────\n",
        "        if act_type == 'LOAD':\n",
        "            tgt, path = self._find_best_spot(cid)\n",
        "            if tgt == -1:                          # 자리가 없음\n",
        "                self.load_fail_streak += 1         # 누적 +1\n",
        "                dyn_pen = -1.0 * (1 + 0.3 * self.load_fail_streak)\n",
        "                return self._get_state(), dyn_pen, 0.0, False\n",
        "            else:\n",
        "                # 자리를 찾았으면 streak 초기화\n",
        "                self.load_fail_streak = 0\n",
        "\n",
        "            cost += self._calculate_path_cost(path)\n",
        "            self.node_status[tgt] = cid\n",
        "            self.car_locations[cid] = tgt\n",
        "            self.cars_on_board.add(cid)\n",
        "\n",
        "            if cid in self.temporarily_unloaded_cars:\n",
        "                self.temporarily_unloaded_cars.remove(cid)\n",
        "            else:\n",
        "                event_rew += 0.1   # “신규 선적” 소정 보상\n",
        "\n",
        "        # ────────────────────────────────────────────────────\n",
        "        # 2) UNLOAD\n",
        "        # ────────────────────────────────────────────────────\n",
        "        elif act_type == 'UNLOAD':\n",
        "            start = self.car_locations.get(cid)\n",
        "            if start is None:      # 차가 실제로 안 있다?\n",
        "                return self._get_state(), -1.0, 0.0, False\n",
        "\n",
        "            path = self._get_or_compute_path(start, 0)\n",
        "            cost += self._calculate_path_cost(path)\n",
        "\n",
        "            # 선박 상태 업데이트\n",
        "            self.node_status[start] = -1\n",
        "            self.cars_on_board.remove(cid)\n",
        "            self.car_locations.pop(cid, None)\n",
        "\n",
        "            car = self.cars[cid]\n",
        "            if car['dest'] == self.current_port:          # 목적지 도착\n",
        "                self.delivered_cars.add(cid)\n",
        "                event_rew += 1.0\n",
        "            else:                                         # 임시 하역\n",
        "                self.temporarily_unloaded_cars.add(cid)\n",
        "                self.relocations_this_episode += 1\n",
        "                event_rew -= 0.1\n",
        "\n",
        "        # ────────────────────────────────────────────────────\n",
        "        # 3) RELOCATE_INTERNAL\n",
        "        # ────────────────────────────────────────────────────\n",
        "        elif act_type == 'RELOCATE_INTERNAL':\n",
        "            tgt, path = self._find_best_internal_spot(cid)\n",
        "            if tgt == -1:          # 자리 못 찾음\n",
        "                event_rew -= 0.2\n",
        "            else:\n",
        "                cost += self._calculate_path_cost(path)\n",
        "                src = self.car_locations.get(cid)\n",
        "                if src is not None:\n",
        "                    self.node_status[src] = -1\n",
        "                self.node_status[tgt] = cid\n",
        "                self.car_locations[cid] = tgt\n",
        "                self.relocations_this_episode += 1\n",
        "                event_rew -= 0.02\n",
        "\n",
        "        # ────────────────────────────────────────────────────\n",
        "        # 4) Reward, done flag, state 반환\n",
        "        # ────────────────────────────────────────────────────\n",
        "        # 거리비용은 0.1 배만 벌점으로 환산\n",
        "        reward = event_rew - (cost / self.fixed_cost) * 0.1 if self.fixed_cost else event_rew\n",
        "        done   = self.current_port >= self.num_ports\n",
        "\n",
        "        return self._get_state(), reward, cost, done\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 섹션 3: 계층적 환경 Wrapper (리팩토링 버전)\n",
        "# ==============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.data import Batch\n",
        "\n",
        "class HierarchicalEnvWrapper:\n",
        "    def __init__(self, problem_data: dict, max_num_ports: int,\n",
        "                 worker_agent, config):\n",
        "        self.problem_data     = problem_data\n",
        "        self.max_num_ports    = max_num_ports\n",
        "        self.worker_agent     = worker_agent\n",
        "        self.config           = config\n",
        "        self.last_manager_action = None\n",
        "\n",
        "        # ── 핵심 객체 ─────────────────────────────\n",
        "        self.ship_env = ShipEnv(problem_data, max_num_ports)\n",
        "\n",
        "        # ★ 체류 스텝 카운터 초기화\n",
        "        self.steps_on_port = 0          # ← 없던 필드\n",
        "\n",
        "        # Manager action 인코딩\n",
        "        self.manager_action_map = {\n",
        "            0: 'CLEAR_BLOCKERS',\n",
        "            1: 'FINISH_UNLOAD',\n",
        "            2: 'FINISH_LOAD',\n",
        "            3: 'CLEAR_TEMP',\n",
        "            4: 'PROCEED_TO_NEXT_PORT'\n",
        "        }\n",
        "        self.goal_embedding = nn.Embedding(\n",
        "            self.config.MANAGER_ACTION_DIM,\n",
        "            self.config.GOAL_EMBED_DIM\n",
        "        ).to(self.worker_agent.device)\n",
        "\n",
        "\n",
        "    def _calculate_potential(self) -> float:\n",
        "        \"\"\"상태의 잠재적 가치를 계산합니다 (Potential-Based Reward Shaping용).\"\"\"\n",
        "        s = self.ship_env\n",
        "\n",
        "        # 처리해야 할 작업이 적을수록 잠재 가치가 높습니다 (따라서 음수로 계산).\n",
        "\n",
        "        # 1. 현재 항구에서 실어야 할 대기 차량 수\n",
        "        waiting_to_load = len([\n",
        "            c for c in s.cars if c['origin'] == s.current_port and\n",
        "            c['id'] not in s.cars_on_board and\n",
        "            c['id'] not in s.temporarily_unloaded_cars\n",
        "        ])\n",
        "\n",
        "        # 2. 현재 항구에서 내려야 할 차량 수 (가중치 부여)\n",
        "        due_to_unload = len([\n",
        "            c for c in s.cars if c['id'] in s.cars_on_board and\n",
        "            c['dest'] == s.current_port\n",
        "        ])\n",
        "\n",
        "        # 3. 임시로 내린 차량 수 (다시 실어야 함)\n",
        "        temp_unloaded = len(s.temporarily_unloaded_cars)\n",
        "\n",
        "        # 잠재력은 이들의 음수 가중합. (작업이 많을수록 가치가 낮음)\n",
        "        # 특히 '내려야 할 차량'을 처리하는 것을 더 중요하게 보기 위해 가중치 1.5 부여\n",
        "        potential = - (1.0 * waiting_to_load + 1.5 * due_to_unload + 1.0 * temp_unloaded)\n",
        "        return potential\n",
        "\n",
        "\n",
        "    def reset(self, prob_data=None):\n",
        "        if prob_data is not None:\n",
        "            self.problem_data = prob_data\n",
        "            self.ship_env     = ShipEnv(self.problem_data, self.max_num_ports)\n",
        "        else:\n",
        "            self.ship_env.reset()\n",
        "\n",
        "        # ★ 포트 체류 카운터도 항상 리셋\n",
        "        self.steps_on_port       = 0\n",
        "        self.last_manager_action = None\n",
        "        return self._get_manager_state()\n",
        "\n",
        "\n",
        "    def _get_manager_state(self):\n",
        "        s = self.ship_env\n",
        "        total_slots = s.num_nodes - 1 if s.num_nodes > 1 else 1\n",
        "        port_norm = s.current_port / s.num_ports\n",
        "        free_slots_norm = sum(1 for n in s.node_status[1:] if n == -1) / total_slots\n",
        "        waiting_to_load = len([c for c in s.cars if c['origin']==s.current_port and c['id'] not in s.cars_on_board and c['id'] not in s.temporarily_unloaded_cars])\n",
        "        due_to_unload = len([c for c in s.cars if c['id'] in s.cars_on_board and c['dest']==s.current_port])\n",
        "        on_board_dests = [s.cars[cid]['dest'] for cid in s.cars_on_board]\n",
        "        avg_dest_dist = (sum(d - s.current_port for d in on_board_dests)/len(on_board_dests)) if on_board_dests else 0.0\n",
        "\n",
        "        return torch.tensor([\n",
        "            port_norm,\n",
        "            free_slots_norm,\n",
        "            waiting_to_load / s.total_cars,\n",
        "            due_to_unload / s.total_cars,\n",
        "            len(s.temporarily_unloaded_cars) / s.total_cars,\n",
        "            avg_dest_dist / s.num_ports\n",
        "        ], dtype=torch.float)\n",
        "\n",
        "    def _is_goal_achieved(self, goal_str):\n",
        "        \"\"\"\n",
        "        각 목표(goal_str)가 충족되었는지 판단한다.\n",
        "        CLEAR_BLOCKERS는 ‘하역 대상 차가 없음’ + ‘게이트 경로에 차가 없음’ 두 조건을 모두 만족해야 True.\n",
        "        \"\"\"\n",
        "        s = self.ship_env\n",
        "\n",
        "        # 현재 항구에서 하역 대상(목적지 == current_port)이 남아 있나?\n",
        "        has_due_to_unload = any(\n",
        "            c['dest'] == s.current_port and c['id'] in s.cars_on_board\n",
        "            for c in s.cars\n",
        "        )\n",
        "\n",
        "        # 게이트(노드 0) → 각 노드 경로 중, 막힌 노드가 존재하나?\n",
        "        def path_blocked():\n",
        "            for cid in s.cars_on_board:\n",
        "                # 이미 게이트까지 나왔다가 temp 로 내려간 차량이면 blocker 아님\n",
        "                if cid in s.temporarily_unloaded_cars:\n",
        "                    continue\n",
        "                node_idx = s.car_locations.get(cid, None)\n",
        "                if node_idx is None:\n",
        "                    continue\n",
        "                path = s._get_or_compute_path(0, node_idx)\n",
        "                if not path:\n",
        "                    continue\n",
        "                if any(s.node_status[n] != -1 for n in path[1:]):\n",
        "                    return True\n",
        "            return False\n",
        "\n",
        "        if goal_str == 'FINISH_UNLOAD':\n",
        "            return not has_due_to_unload                     # 하역 대상이 더 이상 없음\n",
        "        elif goal_str == 'CLEAR_BLOCKERS':\n",
        "            return (not has_due_to_unload) and (not path_blocked())\n",
        "        elif goal_str == 'FINISH_LOAD':\n",
        "            return not any(\n",
        "                c['origin'] == s.current_port and\n",
        "                c['id'] not in s.cars_on_board and\n",
        "                c['id'] not in s.temporarily_unloaded_cars\n",
        "                for c in s.cars\n",
        "            )\n",
        "        elif goal_str == 'CLEAR_TEMP':\n",
        "            return len(s.temporarily_unloaded_cars) == 0\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "\n",
        "    # ======================================================================\n",
        "    #  HierarchicalEnvWrapper.step  ―  최종 교체본\n",
        "    # ======================================================================\n",
        "    def step(self, manager_action_idx: int, *, greedy_worker: bool = False):\n",
        "        \"\"\"\n",
        "        Manager 액션을 받아 Worker 롤아웃까지 수행한 뒤\n",
        "        (다음 Manager 상태, Manager 보상, done, info) 반환\n",
        "        \"\"\"\n",
        "        # ────────────────────────── 기본 세팅 ──────────────────────────\n",
        "        goal_str         = self.manager_action_map[manager_action_idx]\n",
        "        goal_embed       = self.goal_embedding(\n",
        "            torch.tensor([manager_action_idx], device=self.worker_agent.device)\n",
        "        )\n",
        "        potential_before = self._calculate_potential()\n",
        "\n",
        "        # 항구 체류 스텝 카운터  ── NEW ──────────────────────────────────\n",
        "        self.steps_on_port += 1\n",
        "\n",
        "        # Worker 통계\n",
        "        worker_steps         = 0\n",
        "        total_cost           = 0.0\n",
        "        total_worker_reward  = 0.0\n",
        "        no_progress          = 0\n",
        "        no_progress_trigger  = False\n",
        "\n",
        "        # Storage (학습 모드일 때만)\n",
        "        worker_storage = None\n",
        "        if (goal_str != 'PROCEED_TO_NEXT_PORT') and (not greedy_worker):\n",
        "            worker_storage = PPOStorage(\n",
        "                self.config.WORKER_NUM_STEPS_PER_UPDATE,\n",
        "                (2,),                         # [action_type, car_id]\n",
        "                self.worker_agent.device\n",
        "            )\n",
        "\n",
        "        # ──────────────────────── 디버그·안정성 파라미터 ───────────────────────\n",
        "        BONUS_EVERY_N      = 100\n",
        "        PERIODIC_EVERY_N   = 150\n",
        "        STREAK_LIMIT       = 15             # ★ “동일 행동·차량” 연속 허용치\n",
        "        EPS                = 1e-6\n",
        "        last_dbg_key       = None\n",
        "        bonus_skip_counter = 0\n",
        "        same_action_streak = 0              # ★ streak 카운터\n",
        "        last_worker_key    = None           # ★ (act_type, car_id)\n",
        "        # ────────────────────────────────────────────────────────────────────\n",
        "\n",
        "        # ─────────────────────────── Worker 루프 ───────────────────────────\n",
        "        if goal_str != 'PROCEED_TO_NEXT_PORT':\n",
        "            batch_graph = Batch.from_data_list(\n",
        "                [self.ship_env._get_state()]\n",
        "            ).to(self.worker_agent.device)\n",
        "\n",
        "            last_worker_key    = None\n",
        "            same_action_streak = 0        # streak 카운터\n",
        "\n",
        "            for i in range(self.config.WORKER_MAX_STEPS_PER_GOAL):\n",
        "                worker_steps = i + 1\n",
        "\n",
        "                # 0-a) 동일 행동·차량 스트릭 초과   ── NEW ────────────────\n",
        "                if same_action_streak >= STREAK_LIMIT:\n",
        "                    logging.info(f\"[WRK] break — same action repeated {STREAK_LIMIT} times\")\n",
        "                    no_progress_trigger = True\n",
        "                    break\n",
        "\n",
        "                # 0-b) ‘진도 없음’ 한도 초과 시 탈출\n",
        "                if (goal_str == 'FINISH_LOAD') and (no_progress >= self.config.NO_PROGRESS_LIMIT):\n",
        "                    no_progress_trigger = True\n",
        "                    break\n",
        "\n",
        "                # 1) 목표 달성 여부\n",
        "                if self._is_goal_achieved(goal_str):\n",
        "                    break\n",
        "\n",
        "                # 2) 합법 액션\n",
        "                legal_actions = self.ship_env.get_legal_actions(for_worker=True)\n",
        "                if not legal_actions:\n",
        "                    break\n",
        "\n",
        "                # 3) 최신 그래프 덮어쓰기\n",
        "                tmp_state        = self.ship_env._get_state()\n",
        "                batch_graph.x    = tmp_state.x.to(batch_graph.x.device)\n",
        "                batch_graph.global_features = tmp_state.global_features.to(batch_graph.global_features.device)\n",
        "\n",
        "                # 4) 행동 선택\n",
        "                action, at, logp, ent, val = self.worker_agent.get_action_and_value(\n",
        "                    batch_graph, legal_actions, goal_embed, greedy=greedy_worker\n",
        "                )\n",
        "                action_type, car_id = action\n",
        "\n",
        "                # 5) Streak 업데이트  ── NEW ─────────────────────────────\n",
        "                worker_key = (action_type, car_id)\n",
        "                if worker_key == last_worker_key:\n",
        "                    same_action_streak += 1\n",
        "                else:\n",
        "                    same_action_streak = 1\n",
        "                last_worker_key = worker_key\n",
        "                # ───────────────────────────────────────────────────────\n",
        "\n",
        "                # 6) intrinsic 보너스 대상 플래그\n",
        "                is_temp_before = (car_id != -1) and (car_id in self.ship_env.temporarily_unloaded_cars)\n",
        "                is_due_before  = (car_id != -1) and (self.ship_env.cars[car_id]['dest'] == self.ship_env.current_port)\n",
        "\n",
        "                # 7) 환경 한 스텝\n",
        "                _, worker_reward, move_cost, overall_done = self.ship_env.step(action)\n",
        "                total_cost          += move_cost\n",
        "                intrinsic_reward     = 0.0\n",
        "                if goal_str == 'FINISH_UNLOAD' and action_type == 'UNLOAD' and is_due_before:\n",
        "                    intrinsic_reward += 0.8\n",
        "                elif goal_str == 'CLEAR_TEMP' and action_type == 'LOAD' and is_temp_before:\n",
        "                    intrinsic_reward += 0.4\n",
        "                worker_reward       += intrinsic_reward\n",
        "                total_worker_reward += worker_reward\n",
        "\n",
        "                # 8) no-progress 업데이트\n",
        "                load_succeeded = (action_type == 'LOAD') and (worker_reward > 0)\n",
        "                if goal_str == 'FINISH_LOAD' and load_succeeded:\n",
        "                    no_progress = 0\n",
        "                else:\n",
        "                    no_progress += 1\n",
        "\n",
        "                # 9) Storage 저장\n",
        "                if worker_storage is not None:\n",
        "                    worker_storage.add(tmp_state, at, logp, worker_reward, overall_done, val)\n",
        "\n",
        "                # 10) 디버그 로그 (… 생략, 기존과 동일) ----------------------\n",
        "\n",
        "                if overall_done:\n",
        "                    break\n",
        "\n",
        "            # --- Worker 루프 종료 후 streak 초기화 -----------------\n",
        "            same_action_streak = 0        # ← 다음 goal 로 이어지지 않게\n",
        "\n",
        "            # 11) PPO 업데이트\n",
        "            if worker_storage and worker_storage.step > 0 and not greedy_worker:\n",
        "                self.worker_agent.update(worker_storage, goal_embed.detach())\n",
        "\n",
        "        # ───────────────────────── Manager 보상 계산 ─────────────────────────\n",
        "        goal_done    = self._is_goal_achieved(goal_str)\n",
        "        event_reward = 0.0\n",
        "\n",
        "        # 실패 패널티\n",
        "        if (worker_steps >= self.config.WORKER_MAX_STEPS_PER_GOAL) and (not goal_done):\n",
        "            penalty = (2 * self.config.TIMEOUT_PENALTY) if goal_str == 'CLEAR_BLOCKERS' else self.config.TIMEOUT_PENALTY\n",
        "            event_reward += penalty\n",
        "\n",
        "        # PBRS\n",
        "        potential_after = self._calculate_potential()\n",
        "        event_reward += self.config.SHAPING_REWARD_WEIGHT * (\n",
        "            self.config.MANAGER_GAMMA * potential_after - potential_before\n",
        "        )\n",
        "\n",
        "        # 항구 이동 명령 처리\n",
        "        if goal_str == 'PROCEED_TO_NEXT_PORT':\n",
        "            waiting = [c for c in self.ship_env.cars if c['origin']==self.ship_env.current_port\n",
        "                       and c['id'] not in self.ship_env.cars_on_board\n",
        "                       and c['id'] not in self.ship_env.temporarily_unloaded_cars]\n",
        "            can_go = not (waiting or self.ship_env.temporarily_unloaded_cars)\n",
        "            if can_go:\n",
        "                self.ship_env.current_port += 1\n",
        "                self.steps_on_port = 0            # ★ 체류 카운터 리셋\n",
        "                event_reward += 5.0\n",
        "                goal_done = True\n",
        "            else:\n",
        "                event_reward -= 10.0\n",
        "                goal_done = False\n",
        "\n",
        "        # ── 항구 체류 제한 패널티만 적용(강제 이동 X)  ── NEW ───────────────\n",
        "        MAX_STEP_PER_PORT = 4000\n",
        "        if self.steps_on_port > MAX_STEP_PER_PORT:\n",
        "            event_reward += self.config.TIMEOUT_PENALTY * 5   # 큰 패널티\n",
        "            logging.info(\"[MGR] Port-stay limit exceeded — penalty applied\")\n",
        "            self.steps_on_port = MAX_STEP_PER_PORT            # 더 증가 안 함\n",
        "        # ────────────────────────────────────────────────────────────────\n",
        "\n",
        "        # 최종 Manager reward\n",
        "        manager_reward  = event_reward - (total_cost * 0.001)\n",
        "        manager_reward -= self.config.STEP_PENALTY_WEIGHT * worker_steps\n",
        "        if self.last_manager_action == manager_action_idx:\n",
        "            manager_reward += self.config.REPEAT_PENALTY\n",
        "        if no_progress_trigger:\n",
        "            manager_reward += self.config.NO_PROGRESS_PENALTY\n",
        "\n",
        "        self.last_manager_action = manager_action_idx\n",
        "\n",
        "        # 에피소드 종료 보너스/패널티\n",
        "        done = self.ship_env.current_port >= self.ship_env.num_ports\n",
        "        if done:\n",
        "            if len(self.ship_env.delivered_cars) == self.ship_env.total_cars:\n",
        "                manager_reward += 1000.0\n",
        "            else:\n",
        "                manager_reward -= (self.ship_env.total_cars - len(self.ship_env.delivered_cars)) * 10.0\n",
        "\n",
        "        # 다음 Manager state\n",
        "        next_state = self._get_manager_state()\n",
        "\n",
        "        info = {\n",
        "            'steps'              : worker_steps,\n",
        "            'goal'               : goal_str,\n",
        "            'success'            : goal_done,\n",
        "            'cost'               : total_cost,\n",
        "            'worker_total_reward': total_worker_reward,\n",
        "        }\n",
        "        return next_state, manager_reward, done, info\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 섹션 4: 에이전트 및 네트워크 (리팩토링 버전)\n",
        "# ==============================================================================\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from torch_geometric.data import Batch\n",
        "from torch_geometric.nn import GlobalAttention, global_mean_pool\n",
        "\n",
        "# --- PPO 알고리즘을 위한 경험 저장소 ---\n",
        "\n",
        "class PPOStorage:\n",
        "    \"\"\"PPO 학습을 위한 롤아웃(rollout) 데이터를 저장하고 관리하는 클래스.\"\"\"\n",
        "    def __init__(self, num_steps: int, action_shape: tuple, device: torch.device, state_shape: tuple = None, manager_action_dim: int = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_steps (int): 저장할 총 스텝 수.\n",
        "            action_shape (tuple): 행동의 형태.\n",
        "            device (torch.device): 데이터가 저장될 장치 (CPU 또는 CUDA).\n",
        "            state_shape (tuple, optional): 상태의 형태. 그래프 데이터가 아니면 지정.\n",
        "            manager_action_dim (int, optional): Manager의 경우, 액션 마스크를 저장하기 위해 필요.\n",
        "        \"\"\"\n",
        "        self.num_steps = num_steps\n",
        "        self.device = device\n",
        "        self.step = 0\n",
        "        self.is_graph_data = (state_shape is None)\n",
        "\n",
        "        # 데이터 저장 버퍼 초기화\n",
        "        if self.is_graph_data:\n",
        "            self.obs = [None] * num_steps\n",
        "        else:\n",
        "            self.obs = torch.zeros((num_steps,) + state_shape, device=device)\n",
        "\n",
        "        if isinstance(action_shape, int): action_shape = (action_shape,)\n",
        "        self.actions = torch.zeros((num_steps,) + action_shape, device=device, dtype=torch.long)\n",
        "        self.logprobs = torch.zeros(num_steps, device=device)\n",
        "        self.rewards = torch.zeros(num_steps, device=device)\n",
        "        self.dones = torch.zeros(num_steps, device=device)\n",
        "        self.values = torch.zeros(num_steps, device=device)\n",
        "\n",
        "        # [핵심 수정] Manager의 상태 의존적 액션 마스크를 저장할 공간\n",
        "        if manager_action_dim:\n",
        "            self.masks = torch.zeros((num_steps, manager_action_dim), device=device)\n",
        "        else:\n",
        "            self.masks = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.step = 0\n",
        "\n",
        "    def add(self, obs, action, logprob, reward, done, value, mask=None):\n",
        "        \"\"\"한 스텝의 경험 데이터를 저장합니다.\"\"\"\n",
        "        if self.step >= self.num_steps: return\n",
        "\n",
        "        if self.is_graph_data:\n",
        "            self.obs[self.step] = obs.cpu() # GPU 메모리 절약을 위해 CPU에 저장\n",
        "        else:\n",
        "            self.obs[self.step].copy_(torch.as_tensor(obs, device=self.device))\n",
        "\n",
        "        self.actions[self.step] = action\n",
        "        self.logprobs[self.step] = logprob\n",
        "        self.rewards[self.step] = torch.tensor(reward, dtype=torch.float32)\n",
        "        self.dones[self.step] = torch.tensor(done, dtype=torch.float32)\n",
        "        self.values[self.step] = value.detach()\n",
        "\n",
        "        # [핵심 수정] Manager의 액션 마스크 저장\n",
        "        if self.masks is not None and mask is not None:\n",
        "            self.masks[self.step] = mask.detach()\n",
        "\n",
        "        self.step += 1\n",
        "\n",
        "    def is_full(self) -> bool:\n",
        "        return self.step >= self.num_steps\n",
        "\n",
        "    def compute_returns_and_advantages(self, last_value: torch.Tensor, gamma: float, gae_lambda: float):\n",
        "        \"\"\"GAE를 사용하여 보상(Return)과 어드밴티지(Advantage)를 계산합니다.\"\"\"\n",
        "        advantages = torch.zeros_like(self.rewards).to(self.device)\n",
        "        last_gae = 0.0\n",
        "        for t in reversed(range(self.num_steps)):\n",
        "            next_non_terminal = 1.0 - self.dones[t]\n",
        "            next_value = last_value if t == self.num_steps - 1 else self.values[t + 1]\n",
        "\n",
        "            delta = self.rewards[t] + gamma * next_value * next_non_terminal - self.values[t]\n",
        "            last_gae = delta + gamma * gae_lambda * next_non_terminal * last_gae\n",
        "            advantages[t] = last_gae\n",
        "\n",
        "        # 어드밴티지를 이용해 최종 Return 계산\n",
        "        self.returns = advantages + self.values\n",
        "        # 어드밴티지 정규화 (학습 안정화)\n",
        "        self.advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "\n",
        "\n",
        "# --- 신경망 모델 정의 ---\n",
        "# WorkerNetwork 클래스 전체\n",
        "class WorkerNetwork(nn.Module):\n",
        "    \"\"\"Worker 에이전트의 정책 및 가치 신경망\"\"\"\n",
        "    def __init__(self, node_feature_size: int, global_feature_size: int, max_cars: int, num_nodes: int, config: Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        embed_dim = self.config.GNN_EMBED_DIM\n",
        "        self.node_input_proj = nn.Linear(node_feature_size, embed_dim)\n",
        "        self.positional_encoding = nn.Embedding(num_nodes, embed_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4, dim_feedforward=embed_dim*4, dropout=0.1, activation='gelu', batch_first=True, norm_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
        "        self.att_pool = GlobalAttention(gate_nn=nn.Sequential(nn.Linear(embed_dim, embed_dim // 2), nn.ReLU(), nn.Linear(embed_dim // 2, 1)))\n",
        "\n",
        "        mlp_input_dim = embed_dim * 2 + global_feature_size + self.config.GOAL_EMBED_DIM\n",
        "        self.mlp = nn.Sequential(nn.Linear(mlp_input_dim, 512), nn.GELU(), nn.Linear(512, 256), nn.GELU())\n",
        "        self.actor_type_head = nn.Linear(256, 3)\n",
        "        self.actor_load_head = nn.Linear(256, max_cars)\n",
        "        self.actor_unload_head = nn.Linear(256, max_cars)\n",
        "        self.actor_relocate_head = nn.Linear(256, max_cars)\n",
        "        self.critic_head = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, data, goal_embedding):\n",
        "        current_device = goal_embedding.device\n",
        "        x, global_feats, batch_index = data.x.to(current_device), data.global_features.to(current_device), data.batch.to(current_device)\n",
        "        ptr = data.ptr.to(current_device) if hasattr(data, 'ptr') and data.ptr is not None else torch.tensor([0, x.size(0)], device=current_device)\n",
        "\n",
        "        node_embeddings = self.node_input_proj(x)\n",
        "        pos_enc_list = [self.positional_encoding(torch.arange(ptr[i+1] - ptr[i], device=current_device)) for i in range(len(ptr)-1)]\n",
        "        pos_enc = torch.cat(pos_enc_list) if pos_enc_list else torch.empty(0, self.config.GNN_EMBED_DIM, device=current_device)\n",
        "        if node_embeddings.size(0) == pos_enc.size(0): node_embeddings += pos_enc\n",
        "\n",
        "        if len(ptr) > 1 and x.shape[0] > 0: max_len = (ptr[1:] - ptr[:-1]).max().item()\n",
        "        elif x.shape[0] > 0: max_len = x.shape[0]\n",
        "        else: max_len = 1\n",
        "\n",
        "        padded_x, masks_list = [], []\n",
        "        if len(ptr) > 1:\n",
        "            for i in range(len(ptr) - 1):\n",
        "                start, end = ptr[i], ptr[i+1]\n",
        "                graph_len, current_nodes = end - start, node_embeddings[start:end]\n",
        "                pad = torch.zeros(max_len - graph_len, self.config.GNN_EMBED_DIM, device=current_device)\n",
        "                padded_x.append(torch.cat([current_nodes, pad]))\n",
        "                mask = torch.ones(max_len, dtype=torch.bool, device=current_device)\n",
        "                mask[:graph_len] = False\n",
        "                masks_list.append(mask)\n",
        "        else:\n",
        "            padded_x.append(node_embeddings)\n",
        "            masks_list.append(torch.zeros(node_embeddings.shape[0], dtype=torch.bool, device=current_device))\n",
        "\n",
        "        padded_x, attention_mask = torch.stack(padded_x), torch.stack(masks_list)\n",
        "        transformer_out = self.transformer_encoder(padded_x, src_key_padding_mask=attention_mask)\n",
        "        transformer_out_flat = transformer_out[~attention_mask]\n",
        "\n",
        "        graph_emb_mean = global_mean_pool(transformer_out_flat, batch_index)\n",
        "        graph_emb_att = self.att_pool(transformer_out_flat, batch_index)\n",
        "        graph_emb = torch.cat([graph_emb_mean, graph_emb_att], dim=1)\n",
        "\n",
        "        if goal_embedding.shape[0] != graph_emb.shape[0]:\n",
        "            goal_embedding = goal_embedding.expand(graph_emb.shape[0], -1)\n",
        "\n",
        "        combined_features = torch.cat([graph_emb, global_feats, goal_embedding], dim=1)\n",
        "        final_features = self.mlp(combined_features)\n",
        "\n",
        "        # [수정] 반환 값에 graph_emb 추가\n",
        "        return (self.actor_type_head(final_features), self.actor_load_head(final_features),\n",
        "                self.actor_unload_head(final_features), self.actor_relocate_head(final_features),\n",
        "                self.critic_head(final_features).squeeze(-1), graph_emb)\n",
        "\n",
        "\n",
        "\n",
        "class ManagerNetwork(nn.Module):\n",
        "    def __init__(self, state_dim: int, action_dim: int, num_layers: int = 2, nhead: int = 4):\n",
        "        super().__init__()\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.action_embed = nn.Embedding(action_dim, 32)\n",
        "\n",
        "        self.input_proj = nn.Linear(state_dim + 32, 128)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=128,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=256,\n",
        "            batch_first=True,\n",
        "            activation='gelu'\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.actor_head = nn.Linear(128, action_dim)\n",
        "        self.critic_head = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, prev_action_idx: torch.LongTensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state: [B, state_dim]\n",
        "            prev_action_idx: [B]\n",
        "        Returns:\n",
        "            logits: [B, action_dim]\n",
        "            value:  [B]\n",
        "        \"\"\"\n",
        "        a_emb = self.action_embed(prev_action_idx)            # [B, 32]\n",
        "        x = torch.cat([state, a_emb], dim=-1)                 # [B, state_dim + 32]\n",
        "        x = self.input_proj(x).unsqueeze(1)                   # [B, 1, 128]\n",
        "\n",
        "        encoded = self.encoder(x)                             # [B, 1, 128]\n",
        "        h = encoded.squeeze(1)                                # [B, 128]\n",
        "\n",
        "        logits = self.actor_head(h)                           # [B, action_dim]\n",
        "        value  = self.critic_head(h).squeeze(-1)              # [B]\n",
        "        return logits, value\n",
        "\n",
        "# --- 에이전트 클래스 정의 ---\n",
        "\n",
        "class ManagerAgent:\n",
        "    \"\"\"상위 레벨의 목표를 결정하는 Manager 에이전트.\"\"\"\n",
        "    def __init__(self, config: Config, device: torch.device, env_wrapper: HierarchicalEnvWrapper):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.env_wrapper = env_wrapper # 현재 환경 상태에 접근하여 액션 마스킹\n",
        "        self.action_map = env_wrapper.manager_action_map\n",
        "        self.type_to_idx = {v: k for k, v in self.env_wrapper.manager_action_map.items()}\n",
        "        self.idx_to_type = {v: k for k, v in self.type_to_idx.items()}\n",
        "\n",
        "        self.network = ManagerNetwork(config.MANAGER_STATE_DIM, config.MANAGER_ACTION_DIM).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.network.parameters(), lr=config.MANAGER_LR, eps=1e-5)\n",
        "\n",
        "\n",
        "    def _build_action_mask(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        달성된 goal, 혹은 물리적으로 불가능한 goal의 로짓을 -1e9 로 내려서\n",
        "        softmax 확률 0 이 되게 만든다.\n",
        "        \"\"\"\n",
        "        mask = torch.zeros(1, self.config.MANAGER_ACTION_DIM, device=self.device)\n",
        "        s_env = self.env_wrapper.ship_env # ship_env에 더 쉽게 접근\n",
        "\n",
        "        # 1. 이미 달성된 목표 마스킹\n",
        "        for idx, goal in self.env_wrapper.manager_action_map.items():\n",
        "            if self.env_wrapper._is_goal_achieved(goal):\n",
        "                mask[0, idx] = -1e9\n",
        "\n",
        "        # 💡 --- [핵심 추가] --- 💡\n",
        "        # 2. 'PROCEED_TO_NEXT_PORT'가 불가능한 조건일 때 마스킹\n",
        "        #   - 항구에 내려야 할 차가 있거나, 임시로 내린 차가 있으면 PROCEED 불가\n",
        "        waiting_cars = any(c['origin'] == s_env.current_port and c['id'] not in s_env.cars_on_board and c['id'] not in s_env.temporarily_unloaded_cars for c in s_env.cars)\n",
        "        has_temp_unloaded = bool(s_env.temporarily_unloaded_cars)\n",
        "\n",
        "        if waiting_cars or has_temp_unloaded:\n",
        "            proceed_idx = self.type_to_idx['PROCEED_TO_NEXT_PORT']\n",
        "            mask[0, proceed_idx] = -1e9\n",
        "        # 💡 --- [수정 완료] --- 💡\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def get_action_and_value(\n",
        "        self,\n",
        "        state: torch.Tensor,\n",
        "        legal_actions: list[tuple[str,int]],\n",
        "        prev_action_idx: torch.LongTensor,\n",
        "        greedy: bool = False\n",
        "    ) -> tuple: # [수정] 반환 타입 튜플로 명시\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state: [state_dim] 크기의 텐서\n",
        "            legal_actions: 가능한 행동 리스트 (예: [(\"FINISH_LOAD\",-1), …])\n",
        "            prev_action_idx: 이전에 선택한 매니저 액션 인덱스 ([1]-shape LongTensor)\n",
        "            greedy: True면 탐욕적으로(max) 선택\n",
        "\n",
        "        Returns:\n",
        "            [greedy=False] (action_tensor, logp, ent, value, mask)\n",
        "            - action_tensor (Tensor): 학습용 액션 텐서 ([1]-shape LongTensor)\n",
        "            - logp (Tensor): 선택 확률의 로그값\n",
        "            - ent (Tensor): 선택 분포의 엔트로피\n",
        "            - value (Tensor): 상태 가치 추정 (스칼라)\n",
        "            - mask (Tensor): PPO 업데이트에 사용할 상태 의존적 마스크\n",
        "\n",
        "            [greedy=True] (action_idx, None, None, None, None)\n",
        "            - action_idx (int): 선택된 매니저 액션 인덱스 (0~4)\n",
        "        \"\"\"\n",
        "        # 1) 평가 모드\n",
        "        self.network.eval()\n",
        "        with torch.no_grad():\n",
        "            # 2) 배치 차원 추가: [state_dim] → [1, state_dim]\n",
        "            batch_state = state.unsqueeze(0).to(self.device)\n",
        "\n",
        "            # 3) 달성 불가능하거나 이미 달성된 목표 로짓 마스킹\n",
        "            mask = self._build_action_mask()[0]          # [action_dim]\n",
        "\n",
        "            # 4) 네트워크 호출: logits [1,action_dim], value [1,]\n",
        "            logits, value = self.network(batch_state, prev_action_idx)\n",
        "            logits = logits[0] + mask                    # [action_dim]\n",
        "\n",
        "            # 5) legal_actions 에 따라 가능한 목표만 남기기\n",
        "            allowed = {act for act, _ in legal_actions}\n",
        "            type_mask = torch.full_like(logits, -1e9)\n",
        "            for idx, t_str in self.idx_to_type.items():\n",
        "                if t_str in allowed:\n",
        "                    type_mask[idx] = 0.0\n",
        "            masked_logits = logits + type_mask\n",
        "\n",
        "            dist = Categorical(logits=masked_logits)\n",
        "\n",
        "            # [핵심 수정] greedy(평가) 모드와 학습 모드의 반환 값을 분리하여 명확히 함\n",
        "            if greedy:\n",
        "                type_idx = torch.argmax(masked_logits)\n",
        "                self.network.train() # 모드 복귀\n",
        "                return type_idx.item(), None, None, None, None\n",
        "\n",
        "            # 학습 모드\n",
        "            type_idx = dist.sample() # 0-dim 텐서\n",
        "            logp     = dist.log_prob(type_idx)\n",
        "            ent      = dist.entropy()\n",
        "\n",
        "            # PPO 저장을 위해 (1,) 형태로 변환\n",
        "            action_tensor = type_idx.unsqueeze(0)\n",
        "\n",
        "        # 7) 학습 모드 복귀\n",
        "        self.network.train()\n",
        "\n",
        "        # [핵심 수정] 학습에 필요한 모든 값을 올바른 순서와 타입으로 반환\n",
        "        return action_tensor, logp, ent, value[0], mask\n",
        "\n",
        "\n",
        "    def update(self, storage: PPOStorage) -> dict:\n",
        "        \"\"\"저장된 경험 데이터를 사용하여 PPO 업데이트를 수행합니다.\"\"\"\n",
        "        # 마지막 상태의 가치 계산\n",
        "        with torch.no_grad():\n",
        "            # 마지막 상태는 storage.obs에 저장되어 있지만, 이전 액션이 필요함\n",
        "            # 이 로직은 간단하게 마지막 obs와 마지막 action을 가져와서 처리해야 함\n",
        "            last_obs = storage.obs[-1].unsqueeze(0).to(self.device)\n",
        "            last_prev_action = storage.actions[-2] if storage.step > 1 else torch.zeros(1, dtype=torch.long, device=self.device)\n",
        "\n",
        "            _, last_value = self.network(last_obs, last_prev_action)\n",
        "\n",
        "\n",
        "        # GAE와 Return 계산\n",
        "        storage.compute_returns_and_advantages(last_value, self.config.MANAGER_GAMMA, self.config.PPO_GAE_LAMBDA)\n",
        "\n",
        "        # 학습을 위한 데이터 준비\n",
        "        # obs는 PPOStorage에 tensor로 저장되도록 수정되었음을 가정\n",
        "        b_obs = torch.stack(list(storage.obs)).to(self.device)\n",
        "        b_actions = storage.actions.squeeze(-1).to(self.device)\n",
        "        b_logprobs = storage.logprobs.to(self.device)\n",
        "        b_returns = storage.returns.to(self.device)\n",
        "        b_advantages = storage.advantages.to(self.device)\n",
        "        b_masks = storage.masks.to(self.device)\n",
        "\n",
        "        # 이전 액션(b_prev_actions) 배치 생성\n",
        "        b_prev_actions = torch.cat(\n",
        "            (torch.zeros(1, dtype=torch.long, device=self.device), b_actions[:-1]),\n",
        "            dim=0\n",
        "        )\n",
        "\n",
        "        batch_size = storage.step\n",
        "        minibatch_size = max(1, batch_size // self.config.PPO_NUM_MINIBATCHES)\n",
        "\n",
        "        # PPO 업데이트 루프\n",
        "        for _ in range(self.config.PPO_UPDATE_EPOCHS):\n",
        "            perm_indices = np.random.permutation(batch_size)\n",
        "            for start in range(0, batch_size, minibatch_size):\n",
        "                end = start + minibatch_size\n",
        "                mb_idx = perm_indices[start:end]\n",
        "\n",
        "                # 미니배치 데이터 생성\n",
        "                mb_states = b_obs[mb_idx]\n",
        "                mb_actions = b_actions[mb_idx]\n",
        "                mb_logprobs = b_logprobs[mb_idx]\n",
        "                mb_returns = b_returns[mb_idx]\n",
        "                mb_advantages = b_advantages[mb_idx]\n",
        "                mb_masks = b_masks[mb_idx]\n",
        "                mb_prev_actions = b_prev_actions[mb_idx] # 이전 액션 미니배치\n",
        "\n",
        "                # 네트워크를 통해 새로운 로그확률, 가치, 엔트로피 계산\n",
        "                logits, new_values = self.network(mb_states, mb_prev_actions)\n",
        "\n",
        "                # 저장된 마스크를 적용하여 확률 분포 재계산\n",
        "                final_logits = logits + mb_masks\n",
        "                dist = Categorical(logits=final_logits)\n",
        "                new_logprobs = dist.log_prob(mb_actions)\n",
        "                entropy = dist.entropy()\n",
        "\n",
        "                # PPO 손실 계산\n",
        "                log_ratio = new_logprobs - mb_logprobs\n",
        "                ratio = log_ratio.exp()\n",
        "\n",
        "                surr1 = mb_advantages * ratio\n",
        "                surr2 = mb_advantages * torch.clamp(ratio, 1 - self.config.PPO_CLIP_COEF, 1 + self.config.PPO_CLIP_COEF)\n",
        "\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "                value_loss = F.mse_loss(new_values, mb_returns)\n",
        "\n",
        "                loss = (policy_loss +\n",
        "                        self.config.PPO_VALUE_COEF * value_loss -\n",
        "                        self.config.MANAGER_ENTROPY_COEF * entropy.mean())\n",
        "\n",
        "                # 역전파 및 최적화\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.network.parameters(), self.config.PPO_MAX_GRAD_NORM)\n",
        "                self.optimizer.step()\n",
        "\n",
        "        return {\n",
        "            \"policy_loss\": policy_loss.item(),\n",
        "            \"value_loss\": value_loss.item(),\n",
        "            \"entropy\": entropy.mean().item()\n",
        "        }\n",
        "\n",
        "class WorkerAgent:\n",
        "    \"\"\"하위 레벨의 세부 행동을 결정하는 Worker 에이전트.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        node_feature_size: int,\n",
        "        global_feature_size: int,\n",
        "        max_cars: int,\n",
        "        max_nodes: int,\n",
        "        config: Config,\n",
        "        device: torch.device\n",
        "    ):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        # v11 시그니처에 맞춰 Config 객체만 넘기도록 수정\n",
        "        self.network = WorkerNetwork(\n",
        "            node_feature_size,\n",
        "            global_feature_size,\n",
        "            max_cars,\n",
        "            max_nodes,\n",
        "            config\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.network.parameters(),\n",
        "            lr=config.WORKER_LR,\n",
        "            eps=1e-5\n",
        "        )\n",
        "\n",
        "        # 행동 타입 ↔ 인덱스 매핑\n",
        "        self.type_to_idx = {'LOAD': 0, 'UNLOAD': 1, 'RELOCATE_INTERNAL': 2}\n",
        "        self.idx_to_type = {v: k for k, v in self.type_to_idx.items()}\n",
        "\n",
        "\n",
        "    def get_action_and_value(\n",
        "                              self,\n",
        "                              batch_graph: Batch,           # ← Data가 아니라 pre-batched 그래프를 받습니다\n",
        "                              legal_actions: list,\n",
        "                              goal_embedding: torch.Tensor,\n",
        "                              greedy: bool = False\n",
        "                            ) -> tuple:\n",
        "\n",
        "        \"\"\"현재 상태와 목표에 따라 세부 행동을 결정합니다.\"\"\"\n",
        "        self.network.eval() # 평가 모드\n",
        "        with torch.no_grad():\n",
        "            type_logits, load_logits, unload_logits, relocate_logits, value, _ = self.network(batch_graph, goal_embedding)\n",
        "            type_logits, load_logits, unload_logits, relocate_logits = type_logits[0], load_logits[0], unload_logits[0], relocate_logits[0]\n",
        "\n",
        "            # --- 합법적인 행동(Legal Action)에 대한 마스킹 ---\n",
        "            # 각 행동 타입과 차량 ID에 대해 가능한 행동만 남깁니다.\n",
        "            allowed_types_str = {act for act, _ in legal_actions}\n",
        "            type_mask = torch.full_like(type_logits, -1e9)\n",
        "            for idx, t_str in self.idx_to_type.items():\n",
        "                if t_str in allowed_types_str:\n",
        "                    type_mask[idx] = 0.0\n",
        "\n",
        "            allowed_load_ids = {aid for (act, aid) in legal_actions if act == 'LOAD'}\n",
        "            load_mask = torch.full_like(load_logits, -1e9)\n",
        "            for idx in allowed_load_ids: load_mask[idx] = 0.0\n",
        "\n",
        "            allowed_unload_ids = {aid for (act, aid) in legal_actions if act == 'UNLOAD'}\n",
        "            unload_mask = torch.full_like(unload_logits, -1e9)\n",
        "            for idx in allowed_unload_ids: unload_mask[idx] = 0.0\n",
        "\n",
        "            allowed_relocate_ids = {aid for (act, aid) in legal_actions if act == 'RELOCATE_INTERNAL'}\n",
        "            relocate_mask = torch.full_like(relocate_logits, -1e9)\n",
        "            for idx in allowed_relocate_ids: relocate_mask[idx] = 0.0\n",
        "\n",
        "            # 마스크 적용\n",
        "            masked_type_logits = type_logits + type_mask\n",
        "\n",
        "            # --- 계층적 샘플링 (Hierarchical Sampling) ---\n",
        "            # 1. 행동 타입 결정\n",
        "            type_dist = Categorical(logits=masked_type_logits)\n",
        "            type_idx = torch.argmax(masked_type_logits) if greedy else type_dist.sample()\n",
        "            type_str = self.idx_to_type[int(type_idx.item())]\n",
        "\n",
        "            # 2. 결정된 타입에 따라 차량 ID 결정\n",
        "            car_idx_tensor = torch.tensor(-1, device=self.device, dtype=torch.long)\n",
        "            car_dist = None\n",
        "            if type_str == 'LOAD':\n",
        "                masked_logits = load_logits + load_mask\n",
        "                car_dist = Categorical(logits=masked_logits)\n",
        "                car_idx_tensor = torch.argmax(masked_logits) if greedy else car_dist.sample()\n",
        "            elif type_str == 'UNLOAD':\n",
        "                masked_logits = unload_logits + unload_mask\n",
        "                car_dist = Categorical(logits=masked_logits)\n",
        "                car_idx_tensor = torch.argmax(masked_logits) if greedy else car_dist.sample()\n",
        "            elif type_str == 'RELOCATE_INTERNAL':\n",
        "                masked_logits = relocate_logits + relocate_mask\n",
        "                car_dist = Categorical(logits=masked_logits)\n",
        "                car_idx_tensor = torch.argmax(masked_logits) if greedy else car_dist.sample()\n",
        "\n",
        "            action_tuple = (type_str, int(car_idx_tensor.item()))\n",
        "\n",
        "            # 학습 모드일 때만 로그 확률과 엔트로피 계산\n",
        "            if greedy:\n",
        "                return action_tuple, None, None, None, value.squeeze(0)\n",
        "\n",
        "            action_tensor = torch.tensor([type_idx.item(), car_idx_tensor.item()], device=self.device)\n",
        "            log_prob_type = type_dist.log_prob(type_idx)\n",
        "            ent_type = type_dist.entropy()\n",
        "\n",
        "            log_prob_car = torch.tensor(0.0, device=self.device)\n",
        "            ent_car = torch.tensor(0.0, device=self.device)\n",
        "            if car_dist is not None:\n",
        "                log_prob_car = car_dist.log_prob(car_idx_tensor)\n",
        "                ent_car = car_dist.entropy()\n",
        "\n",
        "            total_log_prob = log_prob_type + log_prob_car\n",
        "            total_entropy = ent_type + ent_car\n",
        "\n",
        "        self.network.train() # 학습 모드로 전환\n",
        "        return action_tuple, action_tensor, total_log_prob, total_entropy, value.squeeze(0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _traj_success(traj):\n",
        "        meta = getattr(traj[-1][0], \"meta\", {})\n",
        "        return meta.get(\"delivered\", -1) == meta.get(\"total\", -2)\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_one(path):\n",
        "        with open(path, \"rb\") as f:\n",
        "            t = pickle.load(f)\n",
        "        if WorkerAgent._traj_success(t):\n",
        "            return t\n",
        "        logging.info(f\"  · drop FAILED traj  → {os.path.basename(path)}\")\n",
        "        return []\n",
        "\n",
        "    #───────────────────────────────────────────────────────────────\n",
        "    #  (WorkerAgent 메소드)  ─  Expert-pkl 모방 학습 루틴\n",
        "    #───────────────────────────────────────────────────────────────\n",
        "    def pretrain_with_imitation(\n",
        "        self,\n",
        "        expert_data_paths: list[str],\n",
        "        epochs: int,\n",
        "        lr: float,\n",
        "        batch_size: int,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        전문가(pkl) 궤적을 이용해 Worker 네트워크를 사전 학습한다.\n",
        "        ───────────────────────────────────────────────────────────\n",
        "        ▸ expert_data_paths : '*.pkl' 파일 목록\n",
        "        ▸ 각 pkl = [(state : Data, action : (str, int)), …]\n",
        "        \"\"\"\n",
        "        logging.info(\"[Phase 1] Starting Imitation Learning for Worker Agent…\")\n",
        "\n",
        "        # 1) 궤적 로드 ───────────────────────────────────────────\n",
        "        expert_pairs: list[tuple[Data, tuple[str, int]]] = []\n",
        "        valid_types = set(self.type_to_idx.keys())      # {'LOAD', …}\n",
        "\n",
        "        for path in expert_data_paths:\n",
        "            if not os.path.exists(path):\n",
        "                logging.warning(f\"  · Not found → {path}\")\n",
        "                continue\n",
        "            with open(path, \"rb\") as f:\n",
        "                traj = pickle.load(f)\n",
        "            expert_pairs.extend( (s, a) for s, a in traj if a[0] in valid_types )\n",
        "\n",
        "        if not expert_pairs:\n",
        "            logging.warning(\"  · No usable expert samples → skip.\")\n",
        "            return\n",
        "\n",
        "        max_n = getattr(self.config, \"MAX_EXPERT_SAMPLES\", None)\n",
        "        if max_n and len(expert_pairs) > max_n:\n",
        "            random.shuffle(expert_pairs)\n",
        "            expert_pairs = expert_pairs[:max_n]\n",
        "        logging.info(f\"  · Total Samples: {len(expert_pairs):,}\")\n",
        "\n",
        "        # 2) 옵티마이저 & 학습 루프 ───────────────────────────────\n",
        "        optim_ = optim.Adam(self.network.parameters(), lr=lr)\n",
        "        self.network.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            random.shuffle(expert_pairs)\n",
        "            total_loss, nb = 0.0, 0\n",
        "\n",
        "            for idx in range(0, len(expert_pairs), batch_size):\n",
        "                batch = expert_pairs[idx: idx + batch_size]\n",
        "                if not batch: continue\n",
        "\n",
        "                states, acts = zip(*batch)\n",
        "                g = Batch.from_data_list(states).to(self.device)\n",
        "\n",
        "                # action → 텐서\n",
        "                a_types = torch.as_tensor(\n",
        "                    [self.type_to_idx[a[0]] for a in acts],\n",
        "                    device=self.device, dtype=torch.long)\n",
        "                a_cars  = torch.as_tensor(\n",
        "                    [a[1] for a in acts],\n",
        "                    device=self.device, dtype=torch.long)\n",
        "\n",
        "                dummy_goal = torch.zeros(\n",
        "                    g.num_graphs, self.config.GOAL_EMBED_DIM,\n",
        "                    device=self.device)\n",
        "\n",
        "                # ── forward & loss ─────────────────────────────\n",
        "                with torch.set_grad_enabled(True):\n",
        "                    t_logit, l_logit, u_logit, r_logit, _v, _ = \\\n",
        "                        self.network(g, dummy_goal)\n",
        "\n",
        "                    loss_type = F.cross_entropy(t_logit, a_types)\n",
        "\n",
        "                    loss_load   = F.cross_entropy(l_logit, a_cars, reduction=\"none\")\n",
        "                    loss_unload = F.cross_entropy(u_logit, a_cars, reduction=\"none\")\n",
        "                    loss_reloc  = F.cross_entropy(r_logit, a_cars, reduction=\"none\")\n",
        "\n",
        "                    m_load     = (a_types == self.type_to_idx['LOAD'])\n",
        "                    m_unload   = (a_types == self.type_to_idx['UNLOAD'])\n",
        "                    m_reloc    = (a_types == self.type_to_idx['RELOCATE_INTERNAL'])\n",
        "\n",
        "                    loss_car = loss_type.new_zeros(1)  # grad X 텐서\n",
        "                    if m_load.any().item():\n",
        "                        loss_car = loss_car + loss_load[m_load].mean()\n",
        "                    if m_unload.any().item():\n",
        "                        loss_car = loss_car + loss_unload[m_unload].mean()\n",
        "                    if m_reloc.any().item():\n",
        "                        loss_car = loss_car + loss_reloc[m_reloc].mean()\n",
        "\n",
        "                    loss = loss_type + loss_car\n",
        "\n",
        "                    # backward\n",
        "                    optim_.zero_grad()\n",
        "                    loss.backward()\n",
        "                    nn.utils.clip_grad_norm_(self.network.parameters(),\n",
        "                                             self.config.PPO_MAX_GRAD_NORM)\n",
        "                    optim_.step()\n",
        "\n",
        "                total_loss += loss.item(); nb += 1\n",
        "\n",
        "            if nb and (epoch + 1) % 10 == 0:\n",
        "                logging.info(f\"    Epoch {epoch+1:4d}/{epochs}  \"\n",
        "                             f\"avg-loss {total_loss/nb:.4f}\")\n",
        "\n",
        "        logging.info(\"[Phase 1] Imitation Learning Finished.\")\n",
        "\n",
        "\n",
        "    def evaluate_actions(self, states: list[Data], actions: torch.Tensor, goal_embedding: torch.Tensor) -> tuple:\n",
        "      batch_data = Batch.from_data_list(states).to(self.device)\n",
        "\n",
        "      \"\"\"\n",
        "      PPO 업데이트를 위해, 주어진 상태(states)에서 특정 행동(actions)을 했을 때의\n",
        "      로그 확률(log_prob), 엔트로피(entropy), 가치(value)를 다시 계산합니다.\n",
        "\n",
        "      Args:\n",
        "          states (list[Data]): 상태(그래프) 데이터의 리스트.\n",
        "          actions (torch.Tensor): [액션 타입, 차량 ID] 형태의 행동 텐서.\n",
        "          goal_embedding (torch.Tensor): 현재 목표에 대한 임베딩 텐서.\n",
        "\n",
        "      Returns:\n",
        "          tuple: (로그 확률, 엔트로피, 가치) 텐서.\n",
        "      \"\"\"\n",
        "      type_logits, load_logits, unload_logits, relocate_logits, values, graph_emb = \\\n",
        "          self.network(batch_data, goal_embedding)\n",
        "\n",
        "      action_types = actions[:, 0]\n",
        "      action_cars  = actions[:, 1]\n",
        "\n",
        "      # 각 행동 헤드에 대한 확률 분포 생성\n",
        "      type_dist      = Categorical(logits=type_logits)\n",
        "      load_dist      = Categorical(logits=load_logits)\n",
        "      unload_dist    = Categorical(logits=unload_logits)\n",
        "      relocate_dist  = Categorical(logits=relocate_logits)\n",
        "\n",
        "      # 행동 타입 로그 확률\n",
        "      log_probs_type = type_dist.log_prob(action_types)\n",
        "\n",
        "      # 하위 행동 로그 확률 미리 계산\n",
        "      log_probs_load     = load_dist.log_prob(action_cars)\n",
        "      log_probs_unload   = unload_dist.log_prob(action_cars)\n",
        "      log_probs_relocate = relocate_dist.log_prob(action_cars)\n",
        "\n",
        "      # 실제 취한 타입에 해당하는 하위 행동 확률만 선택\n",
        "      mask_load     = (action_types == self.type_to_idx['LOAD']).float()\n",
        "      mask_unload   = (action_types == self.type_to_idx['UNLOAD']).float()\n",
        "      mask_relocate = (action_types == self.type_to_idx['RELOCATE_INTERNAL']).float()\n",
        "\n",
        "      log_probs = log_probs_type \\\n",
        "                  + log_probs_load     * mask_load \\\n",
        "                  + log_probs_unload   * mask_unload \\\n",
        "                  + log_probs_relocate * mask_relocate\n",
        "\n",
        "      # 엔트로피 계산\n",
        "      type_probs = F.softmax(type_logits, dim=-1)\n",
        "      entropy = (\n",
        "          type_dist.entropy()\n",
        "          + type_probs[:, self.type_to_idx['LOAD']]     * load_dist.entropy()\n",
        "          + type_probs[:, self.type_to_idx['UNLOAD']]   * unload_dist.entropy()\n",
        "          + type_probs[:, self.type_to_idx['RELOCATE_INTERNAL']] * relocate_dist.entropy()\n",
        "      )\n",
        "\n",
        "      return log_probs, entropy, values\n",
        "\n",
        "    def update(self, storage: PPOStorage, goal_embedding: torch.Tensor) -> dict:\n",
        "        \"\"\"저장된 Worker의 경험 데이터를 사용하여 PPO 업데이트를 수행합니다.\"\"\"\n",
        "        self.network.train()\n",
        "\n",
        "        # 1) 마지막 상태를 배치로 만들어줍니다.\n",
        "        last_state = storage.obs[storage.step - 1]\n",
        "        last_state_batch = Batch.from_data_list([last_state]).to(self.device)\n",
        "        # 마지막 상태의 가치 계산\n",
        "        with torch.no_grad():\n",
        "            # network 반환: (type_logits, load_logits, unload_logits, relocate_logits, critic_value, graph_emb)\n",
        "            _, _, _, _, critic_value, _ = self.network(last_state_batch, goal_embedding)\n",
        "            last_value = critic_value.squeeze(0)\n",
        "\n",
        "\n",
        "\n",
        "        # GAE와 Return 계산\n",
        "        storage.compute_returns_and_advantages(last_value, self.config.WORKER_GAMMA, self.config.PPO_GAE_LAMBDA)\n",
        "\n",
        "        # 학습 데이터 준비 (리스트는 그대로 두고, 텐서는 device로 이동)\n",
        "        b_obs = storage.obs # 리스트이므로 device로 옮기지 않음\n",
        "        b_actions = storage.actions.to(self.device)\n",
        "        b_logprobs = storage.logprobs.to(self.device)\n",
        "        b_returns = storage.returns.to(self.device)\n",
        "        b_advantages = storage.advantages.to(self.device)\n",
        "\n",
        "        batch_size = storage.step\n",
        "        if batch_size == 0: return {}\n",
        "        minibatch_size = max(1, batch_size // self.config.PPO_NUM_MINIBATCHES)\n",
        "\n",
        "        # PPO 업데이트 루프\n",
        "        for _ in range(self.config.PPO_UPDATE_EPOCHS):\n",
        "            perm_indices = np.random.permutation(batch_size)\n",
        "            for start in range(0, batch_size, minibatch_size):\n",
        "                end = start + minibatch_size\n",
        "                mb_idx = perm_indices[start:end]\n",
        "\n",
        "                # 미니배치 데이터 생성\n",
        "                mb_states = [b_obs[i] for i in mb_idx]\n",
        "                mb_actions = b_actions[mb_idx]\n",
        "                mb_logprobs = b_logprobs[mb_idx]\n",
        "                mb_returns = b_returns[mb_idx]\n",
        "                mb_advantages = b_advantages[mb_idx]\n",
        "\n",
        "                new_logprobs, entropy, new_values = self.evaluate_actions(mb_states, mb_actions, goal_embedding)\n",
        "\n",
        "                # PPO 손실 계산\n",
        "                log_ratio = new_logprobs - mb_logprobs\n",
        "                ratio = log_ratio.exp()\n",
        "\n",
        "                surr1 = mb_advantages * ratio\n",
        "                surr2 = mb_advantages * torch.clamp(ratio, 1 - self.config.PPO_CLIP_COEF, 1 + self.config.PPO_CLIP_COEF)\n",
        "\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "                value_loss = F.mse_loss(new_values, mb_returns)\n",
        "\n",
        "                loss = (policy_loss +\n",
        "                        self.config.PPO_VALUE_COEF * value_loss -\n",
        "                        self.config.WORKER_ENTROPY_COEF * entropy.mean())\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.network.parameters(), self.config.PPO_MAX_GRAD_NORM)\n",
        "                self.optimizer.step()\n",
        "\n",
        "        # 학습 로그를 위해 손실 값 반환\n",
        "        return {\n",
        "            \"policy_loss\": policy_loss.item(),\n",
        "            \"value_loss\": value_loss.item(),\n",
        "            \"entropy\": entropy.mean().item()\n",
        "        }\n",
        "\n",
        "# ==============================================================================\n",
        "# 섹션 5: 평가 및 메인 루프 (리팩토링 버전)\n",
        "# ==============================================================================\n",
        "import time\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def evaluate_agent(manager_agent: ManagerAgent, worker_agent: WorkerAgent, problems: list[dict], max_num_ports: int, config: Config):\n",
        "    \"\"\"\n",
        "    현재 에이전트의 성능을 평가하고, 다양한 지표를 기록합니다.\n",
        "    \"\"\"\n",
        "    logging.info(\"=\" * 20 + \" AGENT EVALUATION START \" + \"=\" * 20)\n",
        "    manager_agent.network.eval()\n",
        "    worker_agent.network.eval()\n",
        "\n",
        "    total_success_count = 0\n",
        "    total_costs = []\n",
        "    total_relocations = []\n",
        "    manager_action_counts = collections.defaultdict(int)\n",
        "\n",
        "    original_env_wrapper = manager_agent.env_wrapper\n",
        "    MAX_EVAL_MANAGER_STEPS = 500\n",
        "\n",
        "    # 💡 --- [핵심] 환경 객체를 루프 밖에서 한 번만 생성 --- 💡\n",
        "    # 첫 번째 문제로 h_env를 초기화하고, 이후에는 reset으로 재사용\n",
        "    prob = random.choice(problems)\n",
        "    h_env = HierarchicalEnvWrapper(prob, max_num_ports, worker_agent, config)\n",
        "    manager_agent.env_wrapper = h_env\n",
        "\n",
        "    for episode_num in range(config.EVAL_EPISODES):\n",
        "        # 💡 [수정] 매번 새로 생성하는 대신, reset 메서드로 문제만 교체\n",
        "        if episode_num > 0:\n",
        "            prob = random.choice(problems)\n",
        "            # h_env.reset()은 manager_state를 반환하지 않으므로, 직접 상태를 가져와야 합니다.\n",
        "            h_env.reset(prob_data=prob)\n",
        "            manager_state = h_env._get_manager_state()\n",
        "        else:\n",
        "            # 첫 에피소드는 이미 h_env 생성 시 리셋됨\n",
        "            manager_state = h_env._get_manager_state()\n",
        "\n",
        "        if hasattr(manager_agent, 'prev_action_idx'):\n",
        "             manager_agent.prev_action_idx.zero_()\n",
        "\n",
        "        overall_done = False\n",
        "        episode_cost = 0.0\n",
        "        current_episode_steps = 0\n",
        "\n",
        "        while not overall_done:\n",
        "            if current_episode_steps > MAX_EVAL_MANAGER_STEPS:\n",
        "                logging.warning(f\"\\nEval Episode [{episode_num+1}] reached max steps limit. Breaking loop.\")\n",
        "                break\n",
        "\n",
        "            with torch.no_grad():\n",
        "                legal_actions = h_env.ship_env.get_legal_actions(for_worker=False)\n",
        "                prev_action_idx = manager_agent.prev_action_idx if hasattr(manager_agent, 'prev_action_idx') else torch.zeros(1, dtype=torch.long, device=manager_agent.device)\n",
        "\n",
        "                manager_action_idx, _, _, _, _ = manager_agent.get_action_and_value(\n",
        "                    manager_state.to(manager_agent.device), legal_actions, prev_action_idx, greedy=True\n",
        "                )\n",
        "\n",
        "            if hasattr(manager_agent, 'prev_action_idx'):\n",
        "                manager_agent.prev_action_idx = torch.tensor(\n",
        "                    [manager_action_idx], dtype=torch.long, device=manager_agent.device\n",
        "                )\n",
        "\n",
        "            manager_action_counts[h_env.manager_action_map[manager_action_idx]] += 1\n",
        "            current_goal_str = h_env.manager_action_map[manager_action_idx]\n",
        "            print(f\"\\r  Eval Ep[{episode_num+1}/{config.EVAL_EPISODES}] Step[{current_episode_steps+1}]: Trying Goal -> {current_goal_str.ljust(25)}\", end=\"\")\n",
        "\n",
        "            manager_state, _, overall_done, info = h_env.step(\n",
        "                manager_action_idx, greedy_worker=True\n",
        "            )\n",
        "            episode_cost += info.get('cost', 0.0)\n",
        "            current_episode_steps += 1\n",
        "\n",
        "        print()\n",
        "\n",
        "        if len(h_env.ship_env.delivered_cars) == h_env.ship_env.total_cars:\n",
        "            total_success_count += 1\n",
        "        total_costs.append(episode_cost)\n",
        "        total_relocations.append(h_env.ship_env.relocations_this_episode)\n",
        "        logging.info(f\"  Eval Episode [{episode_num+1}/{config.EVAL_EPISODES}] Finished. Success: {len(h_env.ship_env.delivered_cars) == h_env.ship_env.total_cars}\")\n",
        "\n",
        "    # 평가가 끝나면 원래 환경으로 복원\n",
        "    manager_agent.env_wrapper = original_env_wrapper\n",
        "    manager_agent.network.train()\n",
        "    worker_agent.network.train()\n",
        "\n",
        "    # 최종 결과 계산 및 로깅\n",
        "    success_rate = total_success_count / config.EVAL_EPISODES\n",
        "    avg_cost = np.mean(total_costs) if total_costs else 0.0\n",
        "    avg_relocations = np.mean(total_relocations) if total_relocations else 0.0\n",
        "\n",
        "    logging.info(\"-\" * 54)\n",
        "    logging.info(f\"[EVAL RESULT] Success Rate: {success_rate*100:.1f}%\")\n",
        "    logging.info(f\"[EVAL RESULT] Average Cost: {avg_cost:.2f}\")\n",
        "    logging.info(f\"[EVAL RESULT] Average Relocations: {avg_relocations:.2f}\")\n",
        "\n",
        "    total_actions = sum(manager_action_counts.values())\n",
        "    if total_actions > 0:\n",
        "        logging.info(\"[EVAL RESULT] Manager Action Distribution:\")\n",
        "        for action_idx in sorted(h_env.manager_action_map.keys()):\n",
        "            count = manager_action_counts.get(h_env.manager_action_map[action_idx], 0)\n",
        "            action_str = h_env.manager_action_map[action_idx]\n",
        "            percentage = (count / total_actions) * 100\n",
        "            logging.info(f\"  - {action_str:<25s}: {count} times ({percentage:.1f}%)\")\n",
        "\n",
        "    logging.info(\"=\" * 22 + \" EVALUATION END \" + \"=\" * 22 + \"\\n\")\n",
        "\n",
        "    return {\n",
        "        \"success_rate\": success_rate,\n",
        "        \"avg_cost\": avg_cost,\n",
        "        \"avg_relocations\": avg_relocations\n",
        "    }\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    mp.set_start_method(\"spawn\", force=True) # 멀티 프로세\n",
        "    # --- 1. 초기 설정 및 환경 구성 ---\n",
        "    setup_logger()\n",
        "    config = Config()\n",
        "    writer = SummaryWriter(log_dir=config.LOG_DIR)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    logging.info(f\"Using device: {device}\")\n",
        "\n",
        "    assert torch.cuda.is_available(), \"CUDA가 활성화되어 있지 않습니다!\"\n",
        "    print(\"✅ CUDA 활성화 확인\")\n",
        "\n",
        "    all_problem_files = [os.path.join(config.PROBLEM_DIR, f) for f in os.listdir(config.PROBLEM_DIR) if f.endswith('.json')]\n",
        "    easy_problem_files = [f for f in all_problem_files if any(name in f for name in ['prob1', 'prob2', 'prob4'])]\n",
        "    easy_problems = [json.load(open(f)) for f in easy_problem_files if os.path.exists(f)]\n",
        "    all_problems = [json.load(open(f)) for f in all_problem_files if os.path.exists(f)]\n",
        "    if not all_problems:\n",
        "        logging.error(f\"No problem files found in {config.PROBLEM_DIR}. Exiting.\")\n",
        "        exit()\n",
        "    if not easy_problems:\n",
        "        logging.warning(f\"No easy problems found. Starting with all problems.\")\n",
        "        easy_problems = all_problems\n",
        "\n",
        "    max_cars = max(sum(q for _, q in p.get('K', [])) for p in all_problems)\n",
        "    max_num_ports = max(p.get('P', 1) for p in all_problems)\n",
        "    max_nodes = max(p.get('N', 1) for p in all_problems)\n",
        "    node_feature_size = 4\n",
        "    global_feature_size = 3 + max_num_ports\n",
        "\n",
        "    # --- 2. 에이전트 및 환경 초기화 ---\n",
        "    worker_agent = WorkerAgent(node_feature_size, global_feature_size, max_cars, max_nodes, config, device)\n",
        "    current_prob = random.choice(easy_problems)\n",
        "    h_env = HierarchicalEnvWrapper(current_prob, max_num_ports, worker_agent, config)\n",
        "    manager_agent = ManagerAgent(config, device, h_env)\n",
        "    manager_agent.prev_action_idx = torch.zeros(1, dtype=torch.long, device=device)\n",
        "\n",
        "    # --- 3. Worker 사전 훈련 (모방 학습) ---\n",
        "    print(\"\\nDEBUG: >>>>>>>>>> STEP 1: Calling pretrain_with_imitation NOW...\")\n",
        "    worker_agent.pretrain_with_imitation(\n",
        "        config.EXPERT_DATA_PATHS, config.IMITATION_LEARNING_EPOCHS, config.IMITATION_LR, config.IMITATION_BATCH_SIZE\n",
        "    )\n",
        "    print(\"DEBUG: <<<<<<<<<< STEP 1: pretrain_with_imitation FINISHED.\\n\")\n",
        "\n",
        "    # --- 4. 계층적 강화학습 메인 루프 ---\n",
        "    logging.info(\"\\n[Phase 2] Starting Hierarchical Reinforcement Learning...\")\n",
        "\n",
        "    manager_state = h_env.reset(prob_data=current_prob)\n",
        "    manager_storage = PPOStorage(config.MANAGER_NUM_STEPS_PER_UPDATE, (1,), device,\n",
        "                                 state_shape=(config.MANAGER_STATE_DIM,),\n",
        "                                 manager_action_dim=config.MANAGER_ACTION_DIM)\n",
        "\n",
        "    episode_rewards, episode_costs, episode_manager_steps = 0, 0, 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Interval 통계를 위한 변수 초기화\n",
        "    interval_rewards = 0.0\n",
        "    interval_goals = defaultdict(int)\n",
        "    interval_successes = 0\n",
        "    interval_costs = 0.0\n",
        "    interval_worker_rewards = 0.0\n",
        "    interval_worker_steps = 0\n",
        "\n",
        "    for manager_step in range(1, config.TOTAL_MANAGER_STEPS + 1):\n",
        "        is_curriculum_phase = manager_step < config.CURRICULUM_STEPS\n",
        "\n",
        "        if is_curriculum_phase:\n",
        "            possible_goals = []\n",
        "            if not h_env._is_goal_achieved('FINISH_UNLOAD'): possible_goals.append(1)\n",
        "            if not h_env._is_goal_achieved('CLEAR_TEMP'): possible_goals.append(3)\n",
        "            if not h_env._is_goal_achieved('FINISH_LOAD'): possible_goals.append(2)\n",
        "            if possible_goals: manager_action_idx = random.choice(possible_goals)\n",
        "            else: manager_action_idx = 4\n",
        "            manager_action_tensor = torch.tensor([manager_action_idx], device=device)\n",
        "            m_log_prob, m_value, m_mask = torch.tensor(0.0), torch.tensor(0.0), None\n",
        "            manager_agent.prev_action_idx = manager_action_tensor.clone()\n",
        "        else:\n",
        "            legal_actions = h_env.ship_env.get_legal_actions(for_worker=False)\n",
        "            prev_idx = manager_agent.prev_action_idx\n",
        "            with torch.no_grad():\n",
        "                manager_action_tensor, m_log_prob, _, m_value, m_mask = manager_agent.get_action_and_value(\n",
        "                    manager_state, legal_actions, prev_idx, greedy=False\n",
        "                )\n",
        "            manager_action_idx = manager_action_tensor.item()\n",
        "            manager_agent.prev_action_idx = manager_action_tensor.clone()\n",
        "\n",
        "        next_manager_state, manager_reward, overall_done, worker_info = h_env.step(manager_action_idx)\n",
        "\n",
        "        episode_rewards += manager_reward\n",
        "        episode_costs += worker_info.get('cost', 0.0)\n",
        "        episode_manager_steps += 1\n",
        "\n",
        "        # 매 스텝마다 Interval 통계 누적\n",
        "        interval_rewards += manager_reward\n",
        "        interval_goals[worker_info.get('goal', 'N/A')] += 1\n",
        "        if worker_info.get('success', False):\n",
        "            interval_successes += 1\n",
        "        interval_costs += worker_info.get('cost', 0.0)\n",
        "        interval_worker_rewards += worker_info.get('worker_total_reward', 0.0) # h_env.step에서 이 값을 반환해야 함\n",
        "        interval_worker_steps += worker_info.get('steps', 0)\n",
        "\n",
        "        if not is_curriculum_phase:\n",
        "            manager_storage.add(manager_state, manager_action_tensor, m_log_prob, manager_reward, overall_done, m_value, m_mask)\n",
        "\n",
        "        manager_state = next_manager_state\n",
        "\n",
        "        if overall_done:\n",
        "            s = h_env.ship_env\n",
        "            success_ratio = len(s.delivered_cars) / s.total_cars if s.total_cars > 0 else 0\n",
        "            logging.info(f\"EPISODE DONE (M-Step: {manager_step}) | Success: {success_ratio*100:.1f}% | Total Reward: {episode_rewards:.2f} | Total Cost: {episode_costs:.2f} | Length: {episode_manager_steps} steps\")\n",
        "            writer.add_scalar(\"Episode/TotalReward\", episode_rewards, manager_step)\n",
        "            writer.add_scalar(\"Episode/TotalCost\", episode_costs, manager_step)\n",
        "            writer.add_scalar(\"Episode/SuccessRatio\", success_ratio, manager_step)\n",
        "            writer.add_scalar(\"Episode/Length\", episode_manager_steps, manager_step)\n",
        "\n",
        "            if manager_step < config.CURRICULUM_TRANSITION_STEP:\n",
        "                current_prob = random.choice(easy_problems)\n",
        "            else:\n",
        "                if manager_step - episode_manager_steps < config.CURRICULUM_TRANSITION_STEP:\n",
        "                     logging.info(\"=\"*20 + \" SWITCHING TO FULL PROBLEM SET \" + \"=\"*20)\n",
        "                current_prob = random.choice(all_problems)\n",
        "\n",
        "            manager_state = h_env.reset(prob_data=current_prob)\n",
        "            manager_agent.prev_action_idx.zero_()\n",
        "            episode_rewards, episode_costs, episode_manager_steps = 0, 0, 0\n",
        "\n",
        "        if not is_curriculum_phase and manager_storage.is_full():\n",
        "            loss_info = manager_agent.update(manager_storage)\n",
        "            if loss_info:\n",
        "                writer.add_scalar(\"Train/Manager_PolicyLoss\", loss_info[\"policy_loss\"], manager_step)\n",
        "                writer.add_scalar(\"Train/Manager_ValueLoss\", loss_info[\"value_loss\"], manager_step)\n",
        "                writer.add_scalar(\"Train/Manager_Entropy\", loss_info[\"entropy\"], manager_step)\n",
        "            manager_storage.reset()\n",
        "\n",
        "        # 💡 --- [수정] 주기적 로그 출력 (중복 제거 및 최종 버전) --- 💡\n",
        "        if manager_step % config.PRINT_INTERVAL_MANAGER_STEPS == 0:\n",
        "            elapsed_time = time.time() - start_time\n",
        "            steps_per_sec = config.PRINT_INTERVAL_MANAGER_STEPS / elapsed_time if elapsed_time > 0 else 0\n",
        "\n",
        "            avg_reward = interval_rewards / config.PRINT_INTERVAL_MANAGER_STEPS\n",
        "            interval_success_rate = (interval_successes / config.PRINT_INTERVAL_MANAGER_STEPS) * 100\n",
        "            avg_cost_per_step = interval_costs / config.PRINT_INTERVAL_MANAGER_STEPS\n",
        "            # 💡 [추가] Worker 평균 보상 계산\n",
        "            avg_worker_rew = interval_worker_rewards / interval_worker_steps if interval_worker_steps > 0 else 0\n",
        "\n",
        "            goal_dist_str = \", \".join([f\"{k.split('_')[-1]}:{v}\" for k, v in sorted(interval_goals.items())])\n",
        "\n",
        "            logging.info(\n",
        "                f\"M-Step {manager_step:6d} | Avg M-Rew: {avg_reward:7.2f} | \"\n",
        "                f\"W-Rew: {avg_worker_rew:6.3f} | \" # Worker 평균 보상 출력\n",
        "                f\"Success: {interval_success_rate:3.0f}% | Avg Cost: {avg_cost_per_step:8.1f} | \"\n",
        "                f\"Goals: [{goal_dist_str}] | SPS: {steps_per_sec:.2f}\"\n",
        "            )\n",
        "            goal_dist_str = \", \".join([f\"{k.split('_')[-1]}:{v}\" for k, v in sorted(interval_goals.items())])\n",
        "\n",
        "            logging.info(\n",
        "                f\"M-Step {manager_step:6d} | Avg Rew: {avg_reward:7.2f} | \"\n",
        "                f\"Success: {interval_success_rate:3.0f}% | Avg Cost: {avg_cost_per_step:8.1f} | \"\n",
        "                f\"Goals: [{goal_dist_str}] | SPS: {steps_per_sec:.2f}\"\n",
        "            )\n",
        "\n",
        "            # 다음 Interval을 위해 통계 변수 초기화\n",
        "            interval_worker_rewards = 0.0\n",
        "            interval_worker_steps = 0\n",
        "            interval_rewards = 0.0\n",
        "            interval_goals.clear()\n",
        "            interval_successes = 0\n",
        "            interval_costs = 0.0\n",
        "            start_time = time.time()\n",
        "\n",
        "        # 주기적 평가 및 모델 저장\n",
        "        if manager_step > 0 and manager_step % config.EVAL_INTERVAL_MANAGER_STEPS == 0:\n",
        "            if manager_step == config.CURRICULUM_STEPS:\n",
        "                logging.info(\"=\"*20 + \" CURRICULUM FINISHED \" + \"=\"*20)\n",
        "\n",
        "            eval_results = evaluate_agent(manager_agent, worker_agent, all_problems, max_num_ports, config)\n",
        "            writer.add_scalar(\"Eval/SuccessRate\", eval_results[\"success_rate\"] * 100.0, manager_step)\n",
        "            writer.add_scalar(\"Eval/AvgCost\", eval_results[\"avg_cost\"], manager_step)\n",
        "            writer.add_scalar(\"Eval/AvgRelocations\", eval_results[\"avg_relocations\"], manager_step)\n",
        "\n",
        "            torch.save(worker_agent.network.state_dict(), os.path.join(config.MODEL_DIR, f\"worker_model_step_{manager_step}.pth\"))\n",
        "            torch.save(manager_agent.network.state_dict(), os.path.join(config.MODEL_DIR, f\"manager_model_step_{manager_step}.pth\"))\n",
        "\n",
        "    writer.close()\n",
        "    logging.info(\"--- V12 Refactored Training Completed ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KWwYc4yKt_-G"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOb829Bfpa2/MYyVPyOKucp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}