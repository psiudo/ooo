{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psiudo/ooo/blob/main/v11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_odh6XsYqQG",
        "outputId": "38962e49-cf7c-4423-96ab-35666904ba7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "âœ… Google Driveê°€ ì„±ê³µì ìœ¼ë¡œ ë§ˆìš´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "# ==================================\n",
        "# ì…€ 1: Google Drive ë§ˆìš´íŠ¸\n",
        "# ==================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"âœ… Google Driveê°€ ì„±ê³µì ìœ¼ë¡œ ë§ˆìš´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUAVUj5OZBE-",
        "outputId": "d5e122f5-526a-4079-8e8e-c1306627e071",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.6.15)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba) (0.43.0)\n",
            "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.11/dist-packages (from numba) (2.0.2)\n",
            "âœ… í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "# ==================================\n",
        "# ì…€ 2: í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "# ==================================\n",
        "!pip install --upgrade torch-geometric\n",
        "!pip install numba\n",
        "print(\"âœ… í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "############################## BFS ìºì‹œ & íœ´ë¦¬ìŠ¤í‹±ì´ ëª¨ë“  í™”ë¬¼ì„ ì¸ë„í•˜ëŠ”ì§€ ê²€ì¦ ##############################\n",
        "\n",
        "\n",
        "#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0) ê¸°ë³¸ ì¤€ë¹„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import json, time, random, numba, numpy as np\n",
        "from numba.typed import List\n",
        "from tqdm.auto   import tqdm\n",
        "\n",
        "DRIVE_ROOT   = \"/content/drive/MyDrive/OptiChallenge\"   # â† ê°œì¸ ê²½ë¡œ\n",
        "prob = json.load(open(f\"{DRIVE_ROOT}/Exercise_Problems/prob10.json\"))\n",
        "\n",
        "#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1) numba-BFS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "@numba.njit\n",
        "def _bfs(adj, s, t, n):\n",
        "    if s == t:\n",
        "        L = List.empty_list(numba.int64); L.append(s); return L\n",
        "    par = np.full(n, -1, np.int64)\n",
        "    q   = List.empty_list(numba.int64); q.append(s); par[s] = s\n",
        "    hd = 0\n",
        "    while hd < len(q):\n",
        "        cur = q[hd]; hd += 1\n",
        "        for nb in adj[cur]:\n",
        "            if par[nb] == -1:\n",
        "                par[nb] = cur\n",
        "                if nb == t:\n",
        "                    out = List.empty_list(numba.int64)\n",
        "                    x = t\n",
        "                    while True:\n",
        "                        out.append(x)\n",
        "                        if x == s: break\n",
        "                        x = par[x]\n",
        "                    rev = List.empty_list(numba.int64)\n",
        "                    for i in range(len(out)-1, -1, -1): rev.append(out[i])\n",
        "                    return rev\n",
        "                q.append(nb)\n",
        "    return List.empty_list(numba.int64)\n",
        "\n",
        "#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2) ShipEnv (RELOC ì œê±°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class ShipEnv:\n",
        "    def __init__(self, pb):\n",
        "        self.N, self.P = pb[\"N\"], pb[\"P\"]; self.F = float(pb[\"F\"])\n",
        "        adj = [List.empty_list(numba.int64) for _ in range(self.N)]\n",
        "        for u, v in pb[\"E\"]: adj[u].append(v); adj[v].append(u)\n",
        "        self.adj = adj\n",
        "\n",
        "        self.cars, cid = [], 0\n",
        "        for (o, d), q in pb[\"K\"]:\n",
        "            for _ in range(q):\n",
        "                self.cars.append({\"id\": cid, \"origin\": o, \"dest\": d}); cid += 1\n",
        "        self.total = len(self.cars)\n",
        "\n",
        "        self.sp = {}\n",
        "        bar = tqdm(total=self.N**2, desc=\"BFS-cache\", unit=\"pair\")\n",
        "        for i in range(self.N):\n",
        "            row = {}\n",
        "            for j in range(self.N):\n",
        "                row[j] = list(_bfs(adj, i, j, self.N)); bar.update()\n",
        "            self.sp[i] = row\n",
        "        bar.close()\n",
        "        self.reset()\n",
        "\n",
        "    # helpers\n",
        "    def _path (self,s,t): return self.sp[s][t]\n",
        "    def _depth(self,i ):   return 0 if i==0 else len(self._path(0,i))-1\n",
        "    #-------------------------------------------------------------\n",
        "    def reset(self):\n",
        "        self.port = 0\n",
        "        self.node = [-1]*self.N\n",
        "        self.loc  = {}\n",
        "        self.on, self.tmp, self.done = set(), set(), set()\n",
        "\n",
        "    #-------------------------------------------------------------\n",
        "    def _nearest_empty(self):\n",
        "        for i in range(1, self.N):\n",
        "            if self.node[i] == -1: return i\n",
        "        return -1                       # ì„ ë‚´ ë§Œì°¨ (ì—°ìŠµìš© ë¬¸ì œì—ì„  ì•ˆ ë‚˜ì˜´)\n",
        "\n",
        "    #-------------------------------------------------------------\n",
        "    def load(self, cid):\n",
        "        tgt = self._nearest_empty()\n",
        "        if tgt == -1: return\n",
        "        self.node[tgt] = cid\n",
        "        self.loc[cid]  = tgt\n",
        "        self.on.add(cid)\n",
        "        self.tmp.discard(cid)\n",
        "\n",
        "    def unload(self, cid):\n",
        "        idx = self.loc[cid]\n",
        "        self.node[idx] = -1\n",
        "        self.on.remove(cid)\n",
        "        self.loc.pop(cid)\n",
        "        (self.done if self.cars[cid][\"dest\"] == self.port else self.tmp).add(cid)\n",
        "\n",
        "#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3) Port-ë‹¨ìœ„ ë§¤ë‹ˆì € â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def process_port(env: ShipEnv):\n",
        "    # 1) ëª©ì ì§€ = í˜„ì¬ í•­êµ¬ì¸ ì°¨ë“¤ ë¨¼ì € í•˜ì—­\n",
        "    changed = True\n",
        "    while changed:\n",
        "        changed = False\n",
        "        for cid in list(env.on):\n",
        "            if env.cars[cid][\"dest\"] == env.port:\n",
        "                env.unload(cid); changed = True\n",
        "\n",
        "    # 2) ëŒ€ê¸° + ì„ì‹œ í•˜ì—­ â†’  ë„ì°©æ¸¯ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ë‹¤ì‹œ ì„ ì \n",
        "    waiting = [c[\"id\"] for c in env.cars\n",
        "               if (c[\"origin\"] == env.port and\n",
        "                   c[\"id\"] not in env.on and\n",
        "                   c[\"id\"] not in env.tmp and\n",
        "                   c[\"id\"] not in env.done)]\n",
        "    waiting += list(env.tmp)            # ì„ì‹œ í•˜ì—­ ì°¨ë„ í•¨ê»˜\n",
        "    waiting.sort(key=lambda cid: env.cars[cid][\"dest\"])\n",
        "\n",
        "    for cid in waiting:\n",
        "        env.load(cid)\n",
        "\n",
        "#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4) ì „ì²´ ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "t0 = time.perf_counter()\n",
        "env = ShipEnv(prob)         # BFS ìºì‹œ + JIT\n",
        "t1 = time.perf_counter()\n",
        "\n",
        "for _ in tqdm(range(env.P), desc=\"Ports\", unit=\"port\"):\n",
        "    process_port(env)\n",
        "    env.port += 1\n",
        "t2 = time.perf_counter()\n",
        "\n",
        "#â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5) ê²°ê³¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print(f\"\\nDelivered {len(env.done)} / {env.total}\")\n",
        "print(f\"Â· init   : {t1 - t0:5.2f}s\")\n",
        "print(f\"Â· rollout: {t2 - t1:5.2f}s\")\n",
        "print(f\"Â· total  : {t2 - t0:5.2f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174,
          "referenced_widgets": [
            "10b3083f07594018a2d7083f320c2a1f",
            "f3fbc29bb1fb40f4b8401d67163debcd",
            "3b18db1c78b74f138500c1cd8a6c64eb",
            "0b8243e20c2f4c19b6a0b6ea9f3ed3bb",
            "3984f7b3222d4e459531ec76993d6564",
            "098bddb2790a43eb8130699124e3f0cd",
            "2cece6120ff34048b93a960adcca2f2d",
            "10af330243c24df6a0f066e2763eb05a",
            "9da3066d65744e69b34160f904ac73e9",
            "ade220bad3a64b13a525cdaff6852a38",
            "140fcfcac8054861b811b901ce6286db",
            "70982f5ede254274836c763a2b18fa85",
            "6d908a324add4e198054b0e8ec3a8d37",
            "7b90f8811e6b4fcba6ad9a081ae4d776",
            "19acf994b0da47a783444480a9536a62",
            "ec8dabdb014746c7b62526b22075a208",
            "9467fb546f794957a628bd9156e1ca9b",
            "dc984cd885d44bf1a3cdc0a5bb64bf98",
            "b30db2159c64495bb46eec3e9d73e8d3",
            "e17892e2a78c4246a042c81235e96160",
            "49b92639664040c1b0a1099460bbdb91",
            "e76fe8e4eca94d34a4552d054837e8da"
          ]
        },
        "id": "91raDutGM0ac",
        "outputId": "08b945c7-adae-42cd-e0fb-349e3c06fb94"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "BFS-cache:   0%|          | 0/131769 [00:00<?, ?pair/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10b3083f07594018a2d7083f320c2a1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Ports:   0%|          | 0/15 [00:00<?, ?port/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70982f5ede254274836c763a2b18fa85"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Delivered 812 / 812\n",
            "Â· init   : 144.25s\n",
            "Â· rollout:  0.02s\n",
            "Â· total  : 144.27s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mIPxSNJZeo6",
        "outputId": "1d9cf005-73aa-45d2-8b54-a22f128636c0"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-04 04:09:18 - INFO - Using device: cuda\n",
            "âœ… CUDA í™œì„±í™” í™•ì¸\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEBUG: >>>>>>>>>> STEP 1: Calling pretrain_with_imitation NOW...\n",
            "2025-07-04 04:09:19 - INFO - [Phase 1] Starting Imitation Learning for Worker Agentâ€¦\n",
            "2025-07-04 04:09:21 - INFO -   Â· Total Samples: 5,420\n",
            "2025-07-04 04:10:51 - INFO -     Epoch   10/50  avg-loss 11.2757\n",
            "2025-07-04 04:12:21 - INFO -     Epoch   20/50  avg-loss 9.8394\n",
            "2025-07-04 04:13:50 - INFO -     Epoch   30/50  avg-loss 8.8763\n",
            "2025-07-04 04:15:19 - INFO -     Epoch   40/50  avg-loss 8.1849\n",
            "2025-07-04 04:16:48 - INFO -     Epoch   50/50  avg-loss 7.7122\n",
            "2025-07-04 04:16:48 - INFO - [Phase 1] Imitation Learning Finished.\n",
            "DEBUG: <<<<<<<<<< STEP 1: pretrain_with_imitation FINISHED.\n",
            "\n",
            "2025-07-04 04:16:48 - INFO - \n",
            "[Phase 2] Starting Hierarchical Reinforcement Learning...\n",
            "2025-07-04 04:17:23 - INFO - M-Step     20 | Avg M-Rew:   -8.50 | W-Rew: -101.219 | Success:  65% | Avg Cost:  13640.0 | Goals: [TEMP:7, LOAD:13] | SPS: 0.58\n",
            "2025-07-04 04:17:23 - INFO - M-Step     20 | Avg Rew:   -8.50 | Success:  65% | Avg Cost:  13640.0 | Goals: [TEMP:7, LOAD:13] | SPS: 0.58\n",
            "2025-07-04 04:18:02 - INFO - M-Step     40 | Avg M-Rew:  -26.35 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.50\n",
            "2025-07-04 04:18:02 - INFO - M-Step     40 | Avg Rew:  -26.35 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.50\n",
            "2025-07-04 04:18:41 - INFO - M-Step     60 | Avg M-Rew:  -22.30 | W-Rew: -103.034 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.52\n",
            "2025-07-04 04:18:41 - INFO - M-Step     60 | Avg Rew:  -22.30 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.52\n",
            "2025-07-04 04:19:21 - INFO - M-Step     80 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.50\n",
            "2025-07-04 04:19:21 - INFO - M-Step     80 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.50\n",
            "2025-07-04 04:20:02 - INFO - M-Step    100 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.48\n",
            "2025-07-04 04:20:02 - INFO - M-Step    100 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.48\n",
            "2025-07-04 04:20:36 - INFO - M-Step    120 | Avg M-Rew:  -21.83 | W-Rew: -102.911 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.58\n",
            "2025-07-04 04:20:36 - INFO - M-Step    120 | Avg Rew:  -21.83 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.58\n",
            "2025-07-04 04:21:11 - INFO - M-Step    140 | Avg M-Rew:  -21.83 | W-Rew: -102.911 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.58\n",
            "2025-07-04 04:21:11 - INFO - M-Step    140 | Avg Rew:  -21.83 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.58\n",
            "2025-07-04 04:21:49 - INFO - M-Step    160 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:21:49 - INFO - M-Step    160 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:22:30 - INFO - M-Step    180 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 04:22:30 - INFO - M-Step    180 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 04:23:08 - INFO - M-Step    200 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:23:08 - INFO - M-Step    200 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:23:53 - INFO - M-Step    220 | Avg M-Rew:  -31.88 | W-Rew: -103.196 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:23:53 - INFO - M-Step    220 | Avg Rew:  -31.88 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:24:36 - INFO - M-Step    240 | Avg M-Rew:  -30.20 | W-Rew: -103.164 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.46\n",
            "2025-07-04 04:24:36 - INFO - M-Step    240 | Avg Rew:  -30.20 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.46\n",
            "2025-07-04 04:25:17 - INFO - M-Step    260 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.48\n",
            "2025-07-04 04:25:17 - INFO - M-Step    260 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.48\n",
            "2025-07-04 04:26:03 - INFO - M-Step    280 | Avg M-Rew:  -33.55 | W-Rew: -103.226 | Success:  30% | Avg Cost:  22725.0 | Goals: [TEMP:14, LOAD:6] | SPS: 0.43\n",
            "2025-07-04 04:26:03 - INFO - M-Step    280 | Avg Rew:  -33.55 | Success:  30% | Avg Cost:  22725.0 | Goals: [TEMP:14, LOAD:6] | SPS: 0.43\n",
            "2025-07-04 04:26:34 - INFO - M-Step    300 | Avg M-Rew:  -18.48 | W-Rew: -102.732 | Success:  75% | Avg Cost:  11362.5 | Goals: [TEMP:5, LOAD:15] | SPS: 0.66\n",
            "2025-07-04 04:26:34 - INFO - M-Step    300 | Avg Rew:  -18.48 | Success:  75% | Avg Cost:  11362.5 | Goals: [TEMP:5, LOAD:15] | SPS: 0.66\n",
            "2025-07-04 04:27:17 - INFO - M-Step    320 | Avg M-Rew:  -30.20 | W-Rew: -103.164 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:27:17 - INFO - M-Step    320 | Avg Rew:  -30.20 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:27:54 - INFO - M-Step    340 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:27:54 - INFO - M-Step    340 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:28:32 - INFO - M-Step    360 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:28:32 - INFO - M-Step    360 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:29:22 - INFO - M-Step    380 | Avg M-Rew:  -36.90 | W-Rew: -103.275 | Success:  20% | Avg Cost:  25250.0 | Goals: [TEMP:16, LOAD:4] | SPS: 0.40\n",
            "2025-07-04 04:29:22 - INFO - M-Step    380 | Avg Rew:  -36.90 | Success:  20% | Avg Cost:  25250.0 | Goals: [TEMP:16, LOAD:4] | SPS: 0.40\n",
            "2025-07-04 04:29:58 - INFO - M-Step    400 | Avg M-Rew:  -23.50 | W-Rew: -102.978 | Success:  60% | Avg Cost:  15150.0 | Goals: [TEMP:8, LOAD:12] | SPS: 0.56\n",
            "2025-07-04 04:29:58 - INFO - M-Step    400 | Avg Rew:  -23.50 | Success:  60% | Avg Cost:  15150.0 | Goals: [TEMP:8, LOAD:12] | SPS: 0.56\n",
            "2025-07-04 04:30:36 - INFO - M-Step    420 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:30:36 - INFO - M-Step    420 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:31:15 - INFO - M-Step    440 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:31:15 - INFO - M-Step    440 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:31:53 - INFO - M-Step    460 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:31:53 - INFO - M-Step    460 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:32:27 - INFO - M-Step    480 | Avg M-Rew:  -21.83 | W-Rew: -102.911 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.58\n",
            "2025-07-04 04:32:27 - INFO - M-Step    480 | Avg Rew:  -21.83 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.58\n",
            "2025-07-04 04:33:01 - INFO - M-Step    500 | Avg M-Rew:  -21.83 | W-Rew: -102.911 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.59\n",
            "2025-07-04 04:33:01 - INFO - M-Step    500 | Avg Rew:  -21.83 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.59\n",
            "2025-07-04 04:33:41 - INFO - M-Step    520 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:33:41 - INFO - M-Step    520 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:34:18 - INFO - M-Step    540 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:34:18 - INFO - M-Step    540 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:35:00 - INFO - M-Step    560 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.48\n",
            "2025-07-04 04:35:00 - INFO - M-Step    560 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.48\n",
            "2025-07-04 04:35:41 - INFO - M-Step    580 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 04:35:41 - INFO - M-Step    580 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 04:36:15 - INFO - M-Step    600 | Avg M-Rew:  -21.83 | W-Rew: -102.911 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.59\n",
            "2025-07-04 04:36:15 - INFO - M-Step    600 | Avg Rew:  -21.83 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.59\n",
            "2025-07-04 04:37:00 - INFO - M-Step    620 | Avg M-Rew:  -31.88 | W-Rew: -103.196 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:37:00 - INFO - M-Step    620 | Avg Rew:  -31.88 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:37:39 - INFO - M-Step    640 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:37:39 - INFO - M-Step    640 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:38:24 - INFO - M-Step    660 | Avg M-Rew:  -31.88 | W-Rew: -103.196 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:38:24 - INFO - M-Step    660 | Avg Rew:  -31.88 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:39:00 - INFO - M-Step    680 | Avg M-Rew:  -23.50 | W-Rew: -102.978 | Success:  60% | Avg Cost:  15150.0 | Goals: [TEMP:8, LOAD:12] | SPS: 0.55\n",
            "2025-07-04 04:39:00 - INFO - M-Step    680 | Avg Rew:  -23.50 | Success:  60% | Avg Cost:  15150.0 | Goals: [TEMP:8, LOAD:12] | SPS: 0.55\n",
            "2025-07-04 04:39:34 - INFO - M-Step    700 | Avg M-Rew:  -21.83 | W-Rew: -102.911 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.59\n",
            "2025-07-04 04:39:34 - INFO - M-Step    700 | Avg Rew:  -21.83 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.59\n",
            "2025-07-04 04:40:12 - INFO - M-Step    720 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:40:12 - INFO - M-Step    720 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:40:56 - INFO - M-Step    740 | Avg M-Rew:  -31.88 | W-Rew: -103.196 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:40:56 - INFO - M-Step    740 | Avg Rew:  -31.88 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:41:34 - INFO - M-Step    760 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:41:34 - INFO - M-Step    760 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:42:13 - INFO - M-Step    780 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:42:13 - INFO - M-Step    780 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:42:52 - INFO - M-Step    800 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:42:52 - INFO - M-Step    800 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:43:32 - INFO - M-Step    820 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:43:32 - INFO - M-Step    820 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:44:17 - INFO - M-Step    840 | Avg M-Rew:  -31.88 | W-Rew: -103.196 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:44:17 - INFO - M-Step    840 | Avg Rew:  -31.88 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:44:55 - INFO - M-Step    860 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:44:55 - INFO - M-Step    860 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:45:32 - INFO - M-Step    880 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:45:32 - INFO - M-Step    880 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:46:10 - INFO - M-Step    900 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:46:10 - INFO - M-Step    900 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:46:51 - INFO - M-Step    920 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 04:46:51 - INFO - M-Step    920 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 04:47:32 - INFO - M-Step    940 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.48\n",
            "2025-07-04 04:47:32 - INFO - M-Step    940 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.48\n",
            "2025-07-04 04:48:12 - INFO - M-Step    960 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:48:12 - INFO - M-Step    960 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:48:57 - INFO - M-Step    980 | Avg M-Rew:  -31.88 | W-Rew: -103.196 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.44\n",
            "2025-07-04 04:48:57 - INFO - M-Step    980 | Avg Rew:  -31.88 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.44\n",
            "2025-07-04 04:49:38 - INFO - M-Step   1000 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 04:49:38 - INFO - M-Step   1000 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 04:50:21 - INFO - M-Step   1020 | Avg M-Rew:  -30.20 | W-Rew: -103.164 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:50:21 - INFO - M-Step   1020 | Avg Rew:  -30.20 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:51:06 - INFO - M-Step   1040 | Avg M-Rew:  -31.88 | W-Rew: -103.196 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:51:06 - INFO - M-Step   1040 | Avg Rew:  -31.88 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:51:44 - INFO - M-Step   1060 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:51:44 - INFO - M-Step   1060 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:52:27 - INFO - M-Step   1080 | Avg M-Rew:  -30.20 | W-Rew: -103.164 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.46\n",
            "2025-07-04 04:52:27 - INFO - M-Step   1080 | Avg Rew:  -30.20 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.46\n",
            "2025-07-04 04:53:09 - INFO - M-Step   1100 | Avg M-Rew:  -30.20 | W-Rew: -103.164 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:53:09 - INFO - M-Step   1100 | Avg Rew:  -30.20 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:53:49 - INFO - M-Step   1120 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:53:49 - INFO - M-Step   1120 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:54:32 - INFO - M-Step   1140 | Avg M-Rew:  -30.20 | W-Rew: -103.164 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:54:32 - INFO - M-Step   1140 | Avg Rew:  -30.20 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:55:18 - INFO - M-Step   1160 | Avg M-Rew:  -33.55 | W-Rew: -103.226 | Success:  30% | Avg Cost:  22725.0 | Goals: [TEMP:14, LOAD:6] | SPS: 0.43\n",
            "2025-07-04 04:55:18 - INFO - M-Step   1160 | Avg Rew:  -33.55 | Success:  30% | Avg Cost:  22725.0 | Goals: [TEMP:14, LOAD:6] | SPS: 0.43\n",
            "2025-07-04 04:55:56 - INFO - M-Step   1180 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:55:56 - INFO - M-Step   1180 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:56:33 - INFO - M-Step   1200 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:56:33 - INFO - M-Step   1200 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:57:16 - INFO - M-Step   1220 | Avg M-Rew:  -30.20 | W-Rew: -103.164 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:57:16 - INFO - M-Step   1220 | Avg Rew:  -30.20 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:57:55 - INFO - M-Step   1240 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:57:55 - INFO - M-Step   1240 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:58:31 - INFO - M-Step   1260 | Avg M-Rew:  -23.50 | W-Rew: -102.978 | Success:  60% | Avg Cost:  15150.0 | Goals: [TEMP:8, LOAD:12] | SPS: 0.56\n",
            "2025-07-04 04:58:31 - INFO - M-Step   1260 | Avg Rew:  -23.50 | Success:  60% | Avg Cost:  15150.0 | Goals: [TEMP:8, LOAD:12] | SPS: 0.56\n",
            "2025-07-04 04:58:56 - INFO - M-Step   1280 | Avg M-Rew:  -13.45 | W-Rew: -102.243 | Success:  90% | Avg Cost:   7575.0 | Goals: [TEMP:2, LOAD:18] | SPS: 0.80\n",
            "2025-07-04 04:58:56 - INFO - M-Step   1280 | Avg Rew:  -13.45 | Success:  90% | Avg Cost:   7575.0 | Goals: [TEMP:2, LOAD:18] | SPS: 0.80\n",
            "2025-07-04 04:59:39 - INFO - M-Step   1300 | Avg M-Rew:  -30.20 | W-Rew: -103.164 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:59:39 - INFO - M-Step   1300 | Avg Rew:  -30.20 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 05:00:20 - INFO - M-Step   1320 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 05:00:20 - INFO - M-Step   1320 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 05:01:07 - INFO - M-Step   1340 | Avg M-Rew:  -33.55 | W-Rew: -103.226 | Success:  30% | Avg Cost:  22725.0 | Goals: [TEMP:14, LOAD:6] | SPS: 0.43\n",
            "2025-07-04 05:01:07 - INFO - M-Step   1340 | Avg Rew:  -33.55 | Success:  30% | Avg Cost:  22725.0 | Goals: [TEMP:14, LOAD:6] | SPS: 0.43\n",
            "2025-07-04 05:01:52 - INFO - M-Step   1360 | Avg M-Rew:  -31.88 | W-Rew: -103.196 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 05:01:52 - INFO - M-Step   1360 | Avg Rew:  -31.88 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 05:02:33 - INFO - M-Step   1380 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 05:02:33 - INFO - M-Step   1380 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 05:03:15 - INFO - M-Step   1400 | Avg M-Rew:  -30.20 | W-Rew: -103.164 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 05:03:15 - INFO - M-Step   1400 | Avg Rew:  -30.20 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 05:03:56 - INFO - M-Step   1420 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 05:03:56 - INFO - M-Step   1420 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 05:04:38 - INFO - M-Step   1440 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 05:04:38 - INFO - M-Step   1440 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n"
          ]
        }
      ],
      "source": [
        "########## v11 : HRL íŠ¸ëœìŠ¤í¬ë¨¸ GNN ë³´ìƒí•¨ìˆ˜ì˜ unambiguity ##########\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# ì„¹ì…˜ 1: ì„¤ì • ë° ê¸°ë³¸ ìœ í‹¸ë¦¬í‹° (ë¦¬íŒ©í† ë§ ë²„ì „)\n",
        "# ==============================================================================\n",
        "import json\n",
        "import torch.optim as optim\n",
        "import collections\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "import logging\n",
        "from torch_geometric.data import Data\n",
        "import pickle\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "import logging\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from collections import defaultdict, deque\n",
        "import multiprocessing as mp\n",
        "import random\n",
        "import time, collections, torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numba\n",
        "from numba.core import types\n",
        "from numba.typed import List\n",
        "import time, collections\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time, collections\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# âš¡ ì…€: EpisodeTracker â€“ ì¶©ëŒ ìˆ˜ì •ë³¸\n",
        "# =========================================\n",
        "\n",
        "\n",
        "class EpisodeTracker:\n",
        "    def __init__(self, h_env, log_dir, max_same_streak=15):\n",
        "        self.env   = h_env\n",
        "        ts         = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.tb    = SummaryWriter(f\"{log_dir}/tracker_{ts}\")\n",
        "        self.eps   = 0          # ì—í”¼ì†Œë“œ ë²ˆí˜¸\n",
        "        self.t     = 0          # âœ… ìŠ¤í… ì¹´ìš´í„° (ì´ì „ self.step â†’ self.t ë¡œ ë³€ê²½)\n",
        "        self.buf   = collections.defaultdict(list)\n",
        "        self.max_same_streak = max_same_streak\n",
        "\n",
        "    # ë‚´ë¶€ env ì†ì„± íˆ¬ëª… íŒ¨ìŠ¤-ìŠ¤ë£¨\n",
        "    def __getattr__(self, name):\n",
        "        return getattr(self.env, name)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Manager ë ˆë²¨ì—ì„œ reset í˜¸ì¶œ\n",
        "    # -------------------------------\n",
        "    def reset(self, *a, **kw):\n",
        "        self._flush()\n",
        "        self.eps += 1\n",
        "        self.t   = 0\n",
        "        return self.env.reset(*a, **kw)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Manager ë ˆë²¨ì—ì„œ step í˜¸ì¶œ\n",
        "    # -------------------------------\n",
        "    def step(self, manager_action_idx, *, greedy_worker=False):\n",
        "        self.t += 1\n",
        "        s_next, m_rew, done, info = \\\n",
        "            self.env.step(manager_action_idx, greedy_worker=greedy_worker)\n",
        "\n",
        "        # â”€â”€ ì¶”ê°€ í†µê³„ ìˆ˜ì§‘ ì˜ˆì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        s_env  = self.env.ship_env\n",
        "        legal  = s_env.get_legal_actions(for_worker=True)\n",
        "        valid  = [a for a in legal\n",
        "                  if not (a[0]=='RELOCATE_INTERNAL' and\n",
        "                          s_env._find_best_internal_spot(a[1])[0] == -1)]\n",
        "\n",
        "        self.buf['legal_cnt'].append(len(legal))\n",
        "        self.buf['valid_cnt'].append(len(valid))\n",
        "        self.buf['manager_r'].append(m_rew)\n",
        "        self.buf['worker_r'].append(info.get('worker_total_reward', 0.0))\n",
        "        self.buf['cost'].append(info.get('cost', 0.0))\n",
        "        self.buf['same_streak_break'].append(\n",
        "            1 if info.get('same_action_streak', 0) >= self.max_same_streak else 0)\n",
        "\n",
        "        if done:\n",
        "            self._flush()\n",
        "        return s_next, m_rew, done, info\n",
        "    # -------------------------------\n",
        "\n",
        "    def _flush(self):\n",
        "        if not self.buf:\n",
        "            return\n",
        "        avg = lambda k: sum(self.buf[k]) / max(1, len(self.buf[k]))\n",
        "        e   = self.eps\n",
        "        self.tb.add_scalar(\"Episode/avg_legal_actions\",  avg('legal_cnt'),     e)\n",
        "        self.tb.add_scalar(\"Episode/avg_valid_actions\",  avg('valid_cnt'),     e)\n",
        "        self.tb.add_scalar(\"Episode/avg_manager_reward\", avg('manager_r'),     e)\n",
        "        self.tb.add_scalar(\"Episode/avg_worker_reward\",  avg('worker_r'),      e)\n",
        "        self.tb.add_scalar(\"Episode/avg_cost\",           avg('cost'),          e)\n",
        "        self.tb.add_scalar(\"Episode/same_streak_breaks\", sum(self.buf['same_streak_break']), e)\n",
        "        self.tb.flush()\n",
        "        self.buf.clear()\n",
        "\n",
        "\n",
        "def setup_logger():\n",
        "    \"\"\"\n",
        "    ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ë¡œê¹… ì„¤ì •ì„ ëª¨ë‘ ì´ˆê¸°í™”í•˜ê³ ,\n",
        "    ìš°ë¦¬ê°€ ì›í•˜ëŠ” ì„¤ì •ìœ¼ë¡œ ê°•ì œ ì ìš©í•˜ëŠ” í•¨ìˆ˜.\n",
        "    \"\"\"\n",
        "    # ë£¨íŠ¸ ë¡œê±°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
        "    root_logger = logging.getLogger()\n",
        "    root_logger.setLevel(logging.INFO) # ë¡œê·¸ ë ˆë²¨ ì„¤ì •\n",
        "\n",
        "    # ë£¨íŠ¸ ë¡œê±°ì— ì—°ê²°ëœ ëª¨ë“  ê¸°ì¡´ í•¸ë“¤ëŸ¬(Handler)ë¥¼ ì œê±°í•©ë‹ˆë‹¤. (ê°€ì¥ ì¤‘ìš”)\n",
        "    for handler in root_logger.handlers[:]:\n",
        "        root_logger.removeHandler(handler)\n",
        "\n",
        "    # ìš°ë¦¬ê°€ ì›í•˜ëŠ” ìƒˆë¡œìš´ í•¸ë“¤ëŸ¬ë¥¼ ìƒì„±í•˜ì—¬ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "    handler = logging.StreamHandler(sys.stdout) # ë¡œê·¸ë¥¼ ì½˜ì†”ì— ì¶œë ¥\n",
        "    formatter = logging.Formatter(\n",
        "        '%(asctime)s - %(levelname)s - %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S'\n",
        "    )\n",
        "    handler.setFormatter(formatter)\n",
        "    root_logger.addHandler(handler)\n",
        "\n",
        "\n",
        "class Config:\n",
        "    \"\"\"í•™ìŠµê³¼ ê´€ë ¨ëœ ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤\"\"\"\n",
        "\n",
        "    # --- ê²½ë¡œ ì„¤ì • ---\n",
        "    DRIVE_PROJECT_ROOT = '/content/drive/MyDrive/OptiChallenge'\n",
        "    PROBLEM_DIR  = os.path.join(DRIVE_PROJECT_ROOT, 'Exercise_Problems')\n",
        "    LOG_DIR      = os.path.join(DRIVE_PROJECT_ROOT, 'v12_logs_refactored')\n",
        "    MODEL_DIR    = os.path.join(DRIVE_PROJECT_ROOT, 'v12_models_refactored')\n",
        "\n",
        "    # ğŸ’¡ ìƒˆ expert pkl  â€˜expert_probX_TYY.pklâ€™ í˜•ì‹ìœ¼ë¡œ ìˆë‹¤ê³  ê°€ì •\n",
        "    EXPERT_DIR   = os.path.join(DRIVE_PROJECT_ROOT)\n",
        "    EXPERT_GLOB  = \"expert_prob[1248]_T*.pkl\"\n",
        "    MAX_EXPERT_SAMPLES = 40_000\n",
        "\n",
        "    @property\n",
        "    def EXPERT_DATA_PATHS(self):\n",
        "        import glob\n",
        "        paths = sorted(glob.glob(os.path.join(self.EXPERT_DIR, self.EXPERT_GLOB)))\n",
        "        if not paths:\n",
        "            logging.warning(f\"[Config] No expert pkl found under {self.EXPERT_DIR}\")\n",
        "        return paths\n",
        "\n",
        "    # --- í•™ìŠµ ì œì–´ ---\n",
        "    TOTAL_MANAGER_STEPS = 500_000       # Manager ì—ì´ì „íŠ¸ì˜ ì´ í•™ìŠµ ìŠ¤í…\n",
        "    CURRICULUM_STEPS = 2000            # ì „ë¬¸ê°€ ì •ì±…ì„ ëª¨ë°©í•˜ëŠ” ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ìŠ¤í… ìˆ˜\n",
        "    PRINT_INTERVAL_MANAGER_STEPS = 20  # í•™ìŠµ ì¤‘ ë¡œê·¸ ì¶œë ¥ ê°„ê²© (Manager ìŠ¤í… ê¸°ì¤€)\n",
        "    EVAL_INTERVAL_MANAGER_STEPS = 2000 # ëª¨ë¸ í‰ê°€ ë° ì €ì¥ ê°„ê²© (Manager ìŠ¤í… ê¸°ì¤€)\n",
        "    EVAL_EPISODES = 10                  # í‰ê°€ ì‹œ ì‹¤í–‰í•  ì—í”¼ì†Œë“œ ìˆ˜\n",
        "    CURRICULUM_TRANSITION_STEP = 20000 # ğŸ’¡ [ì¶”ê°€] ë¬¸ì œ ë‚œì´ë„ ì»¤ë¦¬í˜ëŸ¼ ì „í™˜ ì‹œì \n",
        "    FIXED_FEE = 100.0\n",
        "    ALPHA_MOVE_DIST  = 1.0     # ê±°ë¦¬ 1ì¹¸ë‹¹ ë³€ë™ë¹„\n",
        "    BETA_RELOC_COST  = 50.0    # ì¬ë°°ì¹˜(route 2ê°œ) ê³ ì • íŒ¨ë„í‹°\n",
        "    SHAPING_REWARD_WEIGHT = FIXED_FEE\n",
        "\n",
        "    # --- ëª¨ë°© í•™ìŠµ (Worker) ---\n",
        "    IMITATION_LEARNING_EPOCHS = 50      # Worker ëª¨ë°© í•™ìŠµ ì—í­ ìˆ˜\n",
        "    IMITATION_LR = 1e-4                 # Worker ëª¨ë°© í•™ìŠµ Learning Rate\n",
        "    IMITATION_BATCH_SIZE = 512          # Worker ëª¨ë°© í•™ìŠµ ë°°ì¹˜ í¬ê¸°\n",
        "\n",
        "    # --- Manager ì—ì´ì „íŠ¸ ì„¤ì • ---\n",
        "    MANAGER_STATE_DIM = 6               # Manager ìƒíƒœ ë²¡í„°ì˜ ì°¨ì›\n",
        "    MANAGER_ACTION_DIM = 5              # Manager í–‰ë™ì˜ ê°€ì§“ìˆ˜\n",
        "    MANAGER_LR = 3e-4                   # Manager Learning Rate\n",
        "    MANAGER_GAMMA = 0.99                # Manager í• ì¸ìœ¨ (Gamma)\n",
        "    MANAGER_ENTROPY_COEF = 0.05         # Manager ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ ê³„ìˆ˜ (íƒí—˜ ì¥ë ¤)\n",
        "    MANAGER_NUM_STEPS_PER_UPDATE = 512  # Manager ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í…\n",
        "\n",
        "    # [ì‹ ê·œ] ë³´ìƒ ì²´ê³„ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "    REPEAT_PENALTY = -1             # ê°™ì€ í–‰ë™ ë°˜ë³µ í˜ë„í‹°\n",
        "    STEP_PENALTY_WEIGHT = 0.001       # Worker ìŠ¤í…ë‹¹ ì‹œê°„ í˜ë„í‹° ê°€ì¤‘ì¹˜\n",
        "    NO_PROGRESS_PENALTY = -2.0        # Workerê°€ ì§„ì²™ ì—†ì´ ì¢…ë£Œ ì‹œ í˜ë„í‹°\n",
        "    NO_PROGRESS_LIMIT = 50            # Worker ì§„ì²™ íŒì • í•œë„ (ê¸°ì¡´ 50 í•˜ë“œì½”ë”© ê°’ ëŒ€ì²´)\\\n",
        "    TIMEOUT_PENALTY = -10.0\n",
        "\n",
        "    # --- Worker ì—ì´ì „íŠ¸ ì„¤ì • ---\n",
        "    WORKER_LR = 3e-4                    # Worker Learning Rate\n",
        "    WORKER_GAMMA = 0.95                 # Worker í• ì¸ìœ¨ (Gamma)\n",
        "    WORKER_ENTROPY_COEF = 0.02         # Worker ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ ê³„ìˆ˜\n",
        "    WORKER_MAX_STEPS_PER_GOAL = 300     # Managerì˜ ëª©í‘œ í•˜ë‚˜ë‹¹ Workerê°€ ìˆ˜í–‰í•  ìµœëŒ€ ìŠ¤í…\n",
        "    WORKER_NUM_STEPS_PER_UPDATE = 1024\n",
        "\n",
        "    # --- PPO ì•Œê³ ë¦¬ì¦˜ ê³µí†µ ì„¤ì • ---\n",
        "    PPO_UPDATE_EPOCHS = 4               # í•œ ë²ˆì˜ ì—…ë°ì´íŠ¸ ì‹œ ì—í­ ìˆ˜\n",
        "    PPO_NUM_MINIBATCHES = 8             # ë¯¸ë‹ˆë°°ì¹˜ ê°œìˆ˜\n",
        "    PPO_CLIP_COEF = 0.2                 # PPO í´ë¦¬í•‘ ê³„ìˆ˜\n",
        "    PPO_GAE_LAMBDA = 0.95               # GAE(Generalized Advantage Estimation) ëŒë‹¤ê°’\n",
        "    PPO_VALUE_COEF = 1.0                # ê°€ì¹˜ í•¨ìˆ˜ ì†ì‹¤(Value Loss) ê³„ìˆ˜\n",
        "    PPO_MAX_GRAD_NORM = 0.5             # Gradient Clipping ìµœëŒ€ L2 Norm\n",
        "\n",
        "    # --- ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ì„¤ì • ---\n",
        "    NODE_FEATURE_DIM = 4      # [is_occupied, dest_diff, blocking_count, is_relocatable]\n",
        "    GNN_EMBED_DIM = 128       # GNNì˜ ê¸°ë³¸ ì„ë² ë”© ì°¨ì›\n",
        "    GOAL_EMBED_DIM = 16       # ëª©í‘œ ì„ë² ë”© ë²¡í„° ì°¨ì›\n",
        "    # Workerì˜ GNN ì¶œë ¥ì„ Manager ìƒíƒœë¡œ ì‚¬ìš© (mean_pool + att_pool)\n",
        "    # MANAGER_STATE_DIM = GNN_EMBED_DIM * 2\n",
        "\n",
        "# --- ê²½ë¡œ ìƒì„± ---\n",
        "# í•™ìŠµ ë¡œê·¸ì™€ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "os.makedirs(Config.LOG_DIR, exist_ok=True)\n",
        "os.makedirs(Config.MODEL_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "# ğŸ’¡ --- ì´ í•¨ìˆ˜ë¡œ ê¸°ì¡´ì˜ ëª¨ë“  get_shortest_path ê´€ë ¨ í•¨ìˆ˜ë¥¼ êµì²´ --- ğŸ’¡\n",
        "@numba.jit(nopython=True)\n",
        "def get_shortest_path(adj_list, start, end, num_nodes):\n",
        "    \"\"\"\n",
        "    Numbaì— ì™„ë²½íˆ í˜¸í™˜ë˜ëŠ” ê°€ì¥ í‘œì¤€ì ì´ê³  ì•ˆì •ì ì¸ BFS í•¨ìˆ˜.\n",
        "    - ë¶€ëª¨ ë…¸ë“œ ì¶”ì  ë°©ì‹ì„ ì‚¬ìš©\n",
        "    - NumPy ë°°ì—´ê³¼ ê¸°ë³¸ ë¦¬ìŠ¤íŠ¸ë§Œ ì‚¬ìš©\n",
        "    \"\"\"\n",
        "    if start == end:\n",
        "        # Numbaë¥¼ ìœ„í•´ íƒ€ì…ì„ ëª…ì‹œì ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„±\n",
        "        path = numba.typed.List()\n",
        "        path.append(start)\n",
        "        return path\n",
        "\n",
        "    # ë¶€ëª¨ ë…¸ë“œë¥¼ ê¸°ë¡í•  NumPy ë°°ì—´ (-1ë¡œ ì´ˆê¸°í™”)\n",
        "    parents = np.full(num_nodes, -1, dtype=np.int64)\n",
        "\n",
        "    # ë°©ë¬¸ ê¸°ë¡ì„ ìœ„í•œ boolean NumPy ë°°ì—´\n",
        "    visited = np.zeros(num_nodes, dtype=np.bool_)\n",
        "\n",
        "    # íë¡œ ì‚¬ìš©í•  ë‹¨ìˆœ ë¦¬ìŠ¤íŠ¸\n",
        "    queue = numba.typed.List()\n",
        "\n",
        "    queue.append(start)\n",
        "    visited[start] = True\n",
        "    head = 0 # íì˜ ë§¨ ì•ì„ ê°€ë¦¬í‚¤ëŠ” í¬ì¸í„°\n",
        "\n",
        "    path_found = False\n",
        "    while head < len(queue):\n",
        "        current = queue[head]\n",
        "        head += 1\n",
        "\n",
        "        if current == end:\n",
        "            path_found = True\n",
        "            break\n",
        "\n",
        "        for neighbor in adj_list[current]:\n",
        "            if not visited[neighbor]:\n",
        "                visited[neighbor] = True\n",
        "                parents[neighbor] = current\n",
        "                queue.append(neighbor)\n",
        "\n",
        "    # ê²½ë¡œ ì—­ì¶”ì \n",
        "    if path_found:\n",
        "        path = numba.typed.List()\n",
        "        curr = end\n",
        "        while curr != -1:\n",
        "            path.append(curr)\n",
        "            curr = parents[curr]\n",
        "        return path[::-1] # ì—­ìˆœì´ë¯€ë¡œ ë’¤ì§‘ì–´ì„œ ë°˜í™˜\n",
        "\n",
        "    return numba.typed.List.empty_list(numba.int64)\n",
        "\n",
        "# ğŸ’¡ --- êµì²´ ì™„ë£Œ --- ğŸ’¡\n",
        "\n",
        "class ShipEnv:\n",
        "    \"\"\"í™”ë¬¼ì„ ì˜ ìƒíƒœì™€ í–‰ë™ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í™˜ê²½ í´ë˜ìŠ¤ (Numba ìµœì¢… ìµœì í™” ì ìš©)\"\"\"\n",
        "    def __init__(self, problem_data: dict, max_num_ports: int):\n",
        "        self.num_nodes = problem_data.get('N', 1)\n",
        "        self.num_ports = problem_data.get('P', 1)\n",
        "        # self.fixed_cost = float(problem_data.get('F', 100))\n",
        "        self.fixed_cost = float(problem_data['F'])     # json ì— ë°˜ë“œì‹œ ìˆìŒ\n",
        "        self.max_num_ports = max_num_ports\n",
        "        self.load_fail_streak: int = 0\n",
        "\n",
        "        # ğŸ’¡ --- [í•µì‹¬] Numba í˜¸í™˜ì„ ìœ„í•œ ê·¸ë˜í”„ ë°ì´í„° êµ¬ì¡°í™” --- ğŸ’¡\n",
        "        adj_list = [numba.typed.List.empty_list(numba.int64) for _ in range(self.num_nodes)]\n",
        "        edge_list_for_tensor = []\n",
        "        for u, v in problem_data.get('E', []):\n",
        "            adj_list[u].append(v)\n",
        "            adj_list[v].append(u)\n",
        "            edge_list_for_tensor.extend([[u, v], [v, u]])\n",
        "        self.adj_list = adj_list # Numba í•¨ìˆ˜ì— ë„˜ê²¨ì£¼ê¸° ìœ„í•´ ì €ì¥\n",
        "        # ğŸ’¡ --- ìˆ˜ì • ì™„ë£Œ --- ğŸ’¡\n",
        "\n",
        "        self.edge_index_tensor = torch.tensor(edge_list_for_tensor, dtype=torch.long).t().contiguous()\n",
        "\n",
        "        # ëª¨ë“  ë…¸ë“œ ìŒ ê°„ì˜ ìµœë‹¨ ê²½ë¡œë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ ìºì‹±\n",
        "        self.shortest_paths = {}\n",
        "        for i in range(self.num_nodes):\n",
        "            self.shortest_paths[i] = {}\n",
        "            for j in range(self.num_nodes):\n",
        "                # ğŸ’¡ [í•µì‹¬] ìˆ˜ì •ëœ get_shortest_path í•¨ìˆ˜ í˜¸ì¶œ\n",
        "                path_result = get_shortest_path(self.adj_list, i, j, self.num_nodes)\n",
        "                self.shortest_paths[i][j] = list(path_result) # ê²°ê³¼ë¥¼ ì¼ë°˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
        "\n",
        "        self.cars = []\n",
        "        car_id_counter = 0\n",
        "        for demand_idx, (demand, quantity) in enumerate(problem_data.get('K', [])):\n",
        "            origin, dest = demand\n",
        "            for _ in range(quantity):\n",
        "                self.cars.append({'id': car_id_counter, 'demand_id': demand_idx, 'origin': origin, 'dest': dest})\n",
        "                car_id_counter += 1\n",
        "        self.total_cars = len(self.cars)\n",
        "        self.reset()\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _move_cost(dist: int, *, is_reloc: bool) -> float:\n",
        "      \"\"\"\n",
        "      ëª¨ë“  ì´ë™ ê²½ë¡œì˜ ì´ ë¹„ìš©ì„ ê³„ì‚°í•œë‹¤.\n",
        "        â€¢ ì„ ì  / í•˜ì—­ / ìœ„ì¹˜ë³€ê²½ / ì„ì‹œí•˜ì—­-ì¬ì ì¬ ëª¨ë‘ ë™ì¼ ê³µì‹\n",
        "        â€¢ is_reloc=True ì´ë©´ Î²(ì¬ë°°ì¹˜ íŒ¨ë„í‹°)ë§Œ í•œ ë²ˆ ë” ë”í•œë‹¤\n",
        "      \"\"\"\n",
        "      base = Config.FIXED_FEE + Config.ALPHA_MOVE_DIST * dist\n",
        "      return base + (Config.BETA_RELOC_COST if is_reloc else 0.0)\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # ğŸ’¡  Gate-depth  (ê²Œì´íŠ¸ì—ì„œ ëª‡ ì¹¸ ì•ˆìª½ì¸ê°€?)\n",
        "    # ------------------------------------------------------------\n",
        "    def _gate_depth(self, node_idx: int) -> int:\n",
        "        \"\"\"\n",
        "        ë…¸ë“œê°€ ê²Œì´íŠ¸(0)ì—ì„œ ë–¨ì–´ì§„ ê¹Šì´(=edge ìˆ˜)ë¥¼ ë¹ ë¥´ê²Œ ê³„ì‚°.\n",
        "        â€¢ ìºì‹±ëœ ìµœë‹¨ê²½ë¡œë¥¼ ì“°ë¯€ë¡œ O(1)\n",
        "        â€¢ gate(0) ìì²´ëŠ” depth 0\n",
        "        \"\"\"\n",
        "        if node_idx == 0:\n",
        "            return 0\n",
        "        path = self._get_or_compute_path(0, node_idx)\n",
        "        return (len(path) - 1) if path else 1_000_000   # unreachable guard\n",
        "\n",
        "\n",
        "    # ===================================================================\n",
        "    # âŠ  ShipEnv  â”€â”€ ì´ˆê²½ëŸ‰ ì¸ì ‘Â­í™•ì¸ í—¬í¼\n",
        "    #     (adj_list ë§Œìœ¼ë¡œ O(1) ì²´í¬, path ê³„ì‚° í˜¸ì¶œ â†“)\n",
        "    # ===================================================================\n",
        "    def _is_adjacent(self, n1: int, n2: int) -> bool:\n",
        "        \"\"\"\n",
        "        ë‘ ë…¸ë“œê°€ ë°”ë¡œ ì—°ê²°ë¼ ìˆëŠ”ì§€ ë¹ ë¥´ê²Œ í™•ì¸í•œë‹¤.\n",
        "        â–¸ _get_or_compute_path() í˜¸ì¶œì„ í”¼í•˜ë©´ì„œ\n",
        "          â€œë°”ë¡œ ì˜†ì¹¸â€ ì—¬ë¶€ë§Œ ì•Œê³  ì‹¶ì„ ë•Œ ì‚¬ìš©.\n",
        "        \"\"\"\n",
        "        return n2 in self.adj_list[n1]\n",
        "\n",
        "\n",
        "    def _get_or_compute_path(self, start: int, end: int) -> list | None:\n",
        "        \"\"\"ë¯¸ë¦¬ ê³„ì‚°ëœ ê²½ë¡œë¥¼ ìºì‹œì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n",
        "        return self.shortest_paths.get(start, {}).get(end, None)\n",
        "\n",
        "\n",
        "    def reset(self) -> Data:\n",
        "        self.current_port: int = 0\n",
        "        self.node_status: list[int] = [-1] * self.num_nodes\n",
        "        self.car_locations: dict[int, int] = {}\n",
        "        self.cars_on_board: set[int] = set()\n",
        "        self.temporarily_unloaded_cars: set[int] = set()\n",
        "        self.delivered_cars: set[int] = set()\n",
        "        self.relocations_this_episode: int = 0\n",
        "        self.last_car_action = {}\n",
        "        # [NEW] ì—í”¼ì†Œë“œê°€ ë°”ë€Œë©´ ì‹¤íŒ¨ ëˆ„ì ë„ ì´ˆê¸°í™”\n",
        "        self.load_fail_streak: int = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self) -> Data:\n",
        "      node_features = []\n",
        "      for i in range(self.num_nodes):\n",
        "          if i == 0:\n",
        "              node_features.append([0.0, 0.0, 0.0, 0.0]); continue\n",
        "          car_id = self.node_status[i]\n",
        "          if car_id == -1:\n",
        "              node_features.append([0.0, 0.0, 0.0, 0.0])\n",
        "          else:\n",
        "              car = self.cars[car_id]\n",
        "              dest_diff = float(car['dest'] - self.current_port)\n",
        "              path_to_gate = self._get_or_compute_path(i, 0)\n",
        "\n",
        "              # ğŸ’¡ --- [í•µì‹¬ ìµœì í™”] ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ setìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í™•ì¸ ì†ë„ í–¥ìƒ --- ğŸ’¡\n",
        "              if path_to_gate:\n",
        "                  path_to_gate_set = set(path_to_gate[1:])\n",
        "                  blocking_count = sum(1 for node_idx, status in enumerate(self.node_status)\n",
        "                                      if status != -1 and node_idx in path_to_gate_set)\n",
        "              else:\n",
        "                  blocking_count = 0\n",
        "              # ğŸ’¡ --- ìµœì í™” ì™„ë£Œ --- ğŸ’¡\n",
        "\n",
        "              is_relocatable = 1.0 if car['dest'] != self.current_port else 0.0\n",
        "              node_features.append([1.0, dest_diff, float(blocking_count), is_relocatable])\n",
        "\n",
        "\n",
        "      waiting_cars = [c for c in self.cars if c['origin'] == self.current_port and c['id'] not in self.cars_on_board and c['id'] not in self.temporarily_unloaded_cars]\n",
        "      waiting_dest_counts = [0.0] * self.max_num_ports\n",
        "      for car in waiting_cars:\n",
        "          if car['dest'] < self.max_num_ports: waiting_dest_counts[car['dest']] += 1.0\n",
        "      global_features = [float(self.current_port), float(len(waiting_cars)), float(len(self.temporarily_unloaded_cars))] + waiting_dest_counts\n",
        "\n",
        "      return Data(x=torch.tensor(node_features, dtype=torch.float),\n",
        "                  edge_index=self.edge_index_tensor,\n",
        "                  global_features=torch.tensor([global_features], dtype=torch.float))\n",
        "\n",
        "    # ===================================================================\n",
        "    # â‹  ShipEnv.get_legal_actions  â€•  ì™„ì „ êµì²´ë³¸\n",
        "    #     - RELOCATE_INTERNAL í›„ë³´ëŠ” â€˜ì‹¤ì œë¡œ ë¹ˆìë¦¬ë¡œ ì´ë™ ê°€ëŠ¥í•œ ì°¨â€™ë§Œ\n",
        "    #     - LOAD / UNLOAD ê°€ëŠ¥ ì¡°ê±´ì„ ì¢€ ë” ë‚ ì¹´ë¡­ê²Œ í•„í„°ë§\n",
        "    # ===================================================================\n",
        "    def get_legal_actions(self, *, for_worker: bool = False) -> list[tuple[str, int]]:\n",
        "        \"\"\"\n",
        "        í™˜ê²½ ìƒíƒœì—ì„œ ì·¨í•  ìˆ˜ ìˆëŠ” í•©ë²•-í–‰ë™ ëª©ë¡ì„ ë°˜í™˜í•œë‹¤.\n",
        "          Â· for_worker=False  â†’  Manager ë ˆë²¨(=PROCEED í¬í•¨)\n",
        "          Â· for_worker=True   â†’  Worker ë ˆë²¨\n",
        "        \"\"\"\n",
        "        actions: list[tuple[str, int]] = []\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ UNLOAD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # â‘  í˜„ì¬ í•­êµ¬ê°€ ëª©ì ì§€ì¸ ì°¨            â†’ ë¬´ì¡°ê±´ í—ˆìš©\n",
        "        # â‘¡ ê²Œì´íŠ¸ë¥¼ ë§‰ê³  ìˆëŠ” ì°¨              â†’ ì¼ë‹¨ í—ˆìš©\n",
        "        for cid in list(self.cars_on_board):\n",
        "            car = self.cars[cid]\n",
        "            if car['dest'] == self.current_port:\n",
        "                actions.append(('UNLOAD', cid))\n",
        "            else:\n",
        "                # ê²Œì´íŠ¸ í†µë¡œë¥¼ ë§‰ê³  ìˆìœ¼ë©´ UNLOAD í›„ë³´\n",
        "                node_idx = self.car_locations.get(cid, -1)\n",
        "                if node_idx != -1:\n",
        "                    path = self._get_or_compute_path(node_idx, 0)\n",
        "                    # ìê¸° ìœ„ì¹˜ ì œì™¸, ê²½ë¡œ ì¤‘ê°„ì— ì°¨ê°€ ì—†ìœ¼ë©´ ê³§ë°”ë¡œ ëº„ ìˆ˜ ìˆìŒ\n",
        "                    if path and all(self.node_status[n] == -1 for n in path[1:-1]):\n",
        "                        actions.append(('UNLOAD', cid))\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LOAD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        any_empty_spot = any(status == -1 for status in self.node_status[1:])\n",
        "        if any_empty_spot:\n",
        "            # ì„ ì  ëŒ€ê¸° + ì„ì‹œ í•˜ì—­ ì°¨ëŸ‰ ëª¨ë‘ LOAD í›„ë³´\n",
        "            for c in self.cars:\n",
        "                cid = c['id']\n",
        "                if (c['origin'] == self.current_port and\n",
        "                    cid not in self.cars_on_board and\n",
        "                    cid not in self.temporarily_unloaded_cars):\n",
        "                    actions.append(('LOAD', cid))\n",
        "            for cid in self.temporarily_unloaded_cars:\n",
        "                actions.append(('LOAD', cid))\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RELOCATE_INTERNAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # â€œë‹¤ë¥¸ ë¹ˆ ì¹¸ìœ¼ë¡œ ì‹¤ì œë¡œ ì˜®ê¸¸ ìë¦¬â€ê°€ ìˆëŠ” ì°¨ë§Œ í›„ë³´ë¡œ ì˜¬ë¦°ë‹¤\n",
        "        for cid in list(self.cars_on_board):\n",
        "            tgt, _ = self._find_best_internal_spot(cid)\n",
        "            if tgt != -1:\n",
        "                actions.append(('RELOCATE_INTERNAL', cid))\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Manager ì „ìš© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        if not for_worker:\n",
        "            actions.append(('PROCEED_TO_NEXT_PORT', -1))\n",
        "\n",
        "        return actions\n",
        "\n",
        "\n",
        "    # ShipEnv ë‚´ (ê¸°ì¡´ ë©”ì†Œë“œë“¤ ë°”ë¡œ ìœ„/ì•„ë˜ ì•„ë¬´ ê³³)\n",
        "    def _is_hard_blocker(self, node_idx: int) -> bool:\n",
        "        \"\"\"\n",
        "        ê²Œì´íŠ¸ì—ì„œ ê°€ê¹ê³  ê³§ ë¹ ì§ˆ ì°¨ëŠ” â€˜ì†Œí”„íŠ¸ ë¸”ë¡œì»¤â€™ë¡œ ë³´ê³ \n",
        "        ì •ë§ ì¹˜ì›Œì•¼ë§Œ í†µê³¼ ê°€ëŠ¥í•œ ì°¨ë§Œ True.\n",
        "        \"\"\"\n",
        "        cid = self.node_status[node_idx]\n",
        "        if cid == -1:\n",
        "            return False\n",
        "\n",
        "        car = self.cars[cid]\n",
        "\n",
        "        # (1) ì§€ê¸ˆ í•˜ì—­ í•­êµ¬ë©´ ê¸ˆë°© ë‚´ë¦°ë‹¤ â†’ False\n",
        "        if car['dest'] == self.current_port:\n",
        "            return False\n",
        "        # (2) ê²Œì´íŠ¸ì—ì„œ ë‘-ì„¸ ì¹¸ ì´ë‚´(manhattan depth <3)ë©´ í†µê³¼ ëŒ€ê¸°ì—´ â†’ False\n",
        "        gate_depth = len(self._get_or_compute_path(node_idx, 0)) - 1\n",
        "        return gate_depth >= 3\n",
        "\n",
        "    # ==================================================================\n",
        "    #  ShipEnv._find_best_spot  â”€  LOAD ì‹œ ìƒˆ ìë¦¬ ì„ ì • (ê°„ë‹¨ ë²„ì „)\n",
        "    # ==================================================================\n",
        "    def _find_best_spot(self, car_id: int) -> tuple[int, list]:\n",
        "        \"\"\"\n",
        "        ì„ ì í•  ì°¨ëŸ‰ì„ ê¹Šì´-ë‹¨ì¡° ê·œì¹™ê³¼ í†µë¡œ ì²­ì •ë„ì— ë”°ë¼ ê°€ì¥\n",
        "        ì¢‹ì€ ë¹ˆ ì¹¸ì— ë°°ì¹˜í•œë‹¤.  ì‹¤íŒ¨ ì‹œ (-1, []) ë°˜í™˜.\n",
        "        \"\"\"\n",
        "        best = None\n",
        "        car_dest = self.cars[car_id]['dest']\n",
        "\n",
        "        earlier_depths = [\n",
        "            self._gate_depth(self.car_locations[c])\n",
        "            for c in self.cars_on_board\n",
        "            if self.cars[c]['dest'] < car_dest\n",
        "        ]\n",
        "        max_earlier = max(earlier_depths) if earlier_depths else -1\n",
        "\n",
        "        later_depths = [\n",
        "            self._gate_depth(self.car_locations[c])\n",
        "            for c in self.cars_on_board\n",
        "            if self.cars[c]['dest'] > car_dest\n",
        "        ]\n",
        "        min_later = min(later_depths) if later_depths else 10**6\n",
        "\n",
        "        for spot in range(1, self.num_nodes):\n",
        "            if self.node_status[spot] != -1:\n",
        "                continue\n",
        "            depth = self._gate_depth(spot)\n",
        "            if not (max_earlier < depth <= min_later):\n",
        "                continue      # ê¹Šì´ ë‹¨ì¡° ìœ„ë°˜\n",
        "\n",
        "            path = self._get_or_compute_path(0, spot)\n",
        "            if not path:\n",
        "                continue\n",
        "            inner = path[1:]\n",
        "\n",
        "            # í†µë¡œì— ë‹¤ë¥¸ ì°¨ê°€ ìˆìœ¼ë©´ ê±´ë„ˆëœ€\n",
        "            if any(self.node_status[n] != -1 for n in inner):\n",
        "                continue\n",
        "\n",
        "            score = depth + random.random() * 1e-3  # ê°€ì¥ ì–•ì€ ê±°ë¦¬ ìš°ì„ \n",
        "            if best is None or score < best[0]:\n",
        "                best = (score, spot, path)\n",
        "\n",
        "        return (-1, []) if best is None else (best[1], best[2])\n",
        "\n",
        "\n",
        "    # ======================================================================\n",
        "    # âŠ     # âŒ ShipEnv._find_best_internal_spot (ì •ë¦¬Â·ë‹¨ì¡° ì¡°ê±´ í¬í•¨)\n",
        "    # ======================================================================\n",
        "    def _find_best_internal_spot(self, car_id: int) -> tuple[int, list]:\n",
        "        start = self.car_locations.get(car_id, -1)\n",
        "        if start == -1:\n",
        "            return -1, []\n",
        "\n",
        "        # â”€â”€ ê¹Šì´ ë‹¨ì¡° ê¸°ë³¸ê°’ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        cur_depth = self._gate_depth(start)\n",
        "        later_depths = [\n",
        "            self._gate_depth(self.car_locations[c])\n",
        "            for c in self.cars_on_board\n",
        "            if self.cars[c]['dest'] > self.cars[car_id]['dest']\n",
        "        ]\n",
        "        min_later_depth = min(later_depths) if later_depths else 1_000_000\n",
        "\n",
        "        def evaluate_spot(spot: int) -> tuple[float, list] | None:\n",
        "            \"\"\"ìŠ¤ì½”ì–´ê°€ ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ìë¦¬, None â†’ ë¶ˆê°€\"\"\"\n",
        "            path = self._get_or_compute_path(start, spot)\n",
        "            if not path:\n",
        "                return None\n",
        "            inner = path[1:-1]\n",
        "\n",
        "            # ë¸”ë¡œì»¤Â·ë‹¨ì¡° ì¡°ê±´\n",
        "            if any(self._is_hard_blocker(n) for n in inner):\n",
        "                return None\n",
        "            if any(self.node_status[n] != -1 for n in inner):\n",
        "                return None\n",
        "            depth = len(path) - 1\n",
        "            if depth > min_later_depth:      # ë‹¨ì¡° ìœ„ë°˜\n",
        "                return None\n",
        "\n",
        "            # ìŠ¤ì½”ì–´ë§: ê±°ë¦¬ + ê²Œì´íŠ¸ ë¹„ìš°ê¸° íš¨ê³¼\n",
        "            unblock_gain = sum(\n",
        "                1 for cid in self.cars_on_board\n",
        "                if self.cars[cid]['dest'] == self.current_port and\n",
        "                   self.car_locations[cid] in inner\n",
        "            )\n",
        "            score = 0.5 * depth - 0.8 * unblock_gain + random.random() * 1e-3\n",
        "            return score, path\n",
        "\n",
        "        best = None\n",
        "        for spot in range(1, self.num_nodes):\n",
        "            if self.node_status[spot] != -1:\n",
        "                continue\n",
        "            res = evaluate_spot(spot)\n",
        "            if res and (best is None or res[0] < best[0]):\n",
        "                best = (res[0], spot, res[1])\n",
        "\n",
        "        if best:\n",
        "            _, spot, path = best\n",
        "            return spot, path\n",
        "\n",
        "        # â”€â”€ fallback: ê°€ì¥ ê°€ê¹Œìš´ ë¹ˆ ì¹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        cand = [\n",
        "            (len(self._get_or_compute_path(start, s)) - 1, s)\n",
        "            for s in range(1, self.num_nodes)\n",
        "            if self.node_status[s] == -1 and self._get_or_compute_path(start, s)\n",
        "        ]\n",
        "        if cand:\n",
        "            cand.sort()\n",
        "            spot = cand[0][1]\n",
        "            path = self._get_or_compute_path(start, spot)\n",
        "            return spot, path\n",
        "\n",
        "        return -1, []\n",
        "\n",
        "\n",
        "    def step(\n",
        "        self,\n",
        "        action: tuple[str, int]\n",
        "    ) -> tuple[Data, float, float, bool]:\n",
        "        \"\"\"\n",
        "        ShipEnv 1-step ì‹œë®¬ë ˆì´ì…˜.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        action : (action_type:str, car_id:int)\n",
        "            * action_type âˆˆ {'LOAD','UNLOAD','RELOCATE_INTERNAL'}\n",
        "            * car_id == -1 ëŠ” PORT ë‹¨ì—ì„œë§Œ ì“°ëŠ” dummy ê°’\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        state  : Data   â”€ PyG ê·¸ë˜í”„(ë…¸ë“œíŠ¹ì„±Â·ê¸€ë¡œë²ŒíŠ¹ì„± í¬í•¨)\n",
        "        reward : float  â”€ worker-level scalar reward\n",
        "        cost   : float  â”€ pure path-moving cost (ê³ ì •+ê±°ë¦¬)\n",
        "        done   : bool   â”€ í•­ë¡œê°€ ëë‚˜ë©´ True\n",
        "        \"\"\"\n",
        "        act_type, cid = action\n",
        "        cost = 0.0\n",
        "        event_rew = 0.0                       # (extrinsic + intrinsic)\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # 0) ë°˜ë³µ/í•‘í íŒ¨ë„í‹°  (cid == -1 ì´ë©´ ìƒëµ)\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        if cid != -1:\n",
        "            prev = self.last_car_action.get(cid)\n",
        "\n",
        "            # (a) ê°™ì€ í–‰ë™ íƒ€ì… ì—°ì†            â†’ ì‚´ì§ -0.1\n",
        "            if prev == act_type:\n",
        "                event_rew -= 0.5\n",
        "\n",
        "            # (b) ì§ì „ UNLOAD â†’ ê³§ë°”ë¡œ LOAD     â†’ ê°•í•˜ê²Œ -5.0\n",
        "            elif prev == 'UNLOAD' and act_type == 'LOAD':\n",
        "                event_rew -= 5.0\n",
        "\n",
        "            # ê¸°ë¡ ê°±ì‹ \n",
        "            self.last_car_action[cid] = act_type\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # 1) LOAD\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        if act_type == 'LOAD':\n",
        "            tgt, path = self._find_best_spot(cid)\n",
        "            if tgt == -1:                          # ìë¦¬ê°€ ì—†ìŒ\n",
        "                self.load_fail_streak += 1         # ëˆ„ì  +1\n",
        "                dyn_pen = -1.0 * (1 + 0.3 * self.load_fail_streak)\n",
        "                return self._get_state(), dyn_pen, 0.0, False\n",
        "            else:\n",
        "                # ìë¦¬ë¥¼ ì°¾ì•˜ìœ¼ë©´ streak ì´ˆê¸°í™”\n",
        "                self.load_fail_streak = 0\n",
        "\n",
        "            cost += self._move_cost(len(path)-1, is_reloc=False)\n",
        "            self.node_status[tgt] = cid\n",
        "            self.car_locations[cid] = tgt\n",
        "            self.cars_on_board.add(cid)\n",
        "\n",
        "            if cid in self.temporarily_unloaded_cars:\n",
        "                self.temporarily_unloaded_cars.remove(cid)\n",
        "            else:\n",
        "                event_rew += 0.1   # â€œì‹ ê·œ ì„ ì â€ ì†Œì • ë³´ìƒ\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # 2) UNLOAD\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        elif act_type == 'UNLOAD':\n",
        "            start = self.car_locations.get(cid)\n",
        "            if start is None:      # ì°¨ê°€ ì‹¤ì œë¡œ ì•ˆ ìˆë‹¤?\n",
        "                return self._get_state(), -1.0, 0.0, False\n",
        "\n",
        "            path = self._get_or_compute_path(start, 0)\n",
        "            cost += self._move_cost(len(path)-1, is_reloc=False)\n",
        "\n",
        "            # ì„ ë°• ìƒíƒœ ì—…ë°ì´íŠ¸\n",
        "            self.node_status[start] = -1\n",
        "            self.cars_on_board.remove(cid)\n",
        "            self.car_locations.pop(cid, None)\n",
        "\n",
        "            car = self.cars[cid]\n",
        "            if car['dest'] == self.current_port:          # ëª©ì ì§€ ë„ì°©\n",
        "                self.delivered_cars.add(cid)\n",
        "                event_rew += 1.0\n",
        "            else:                                         # ì„ì‹œ í•˜ì—­\n",
        "                self.temporarily_unloaded_cars.add(cid)\n",
        "                self.relocations_this_episode += 1\n",
        "                event_rew -= 0.1\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # 3) RELOCATE_INTERNAL\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        elif act_type == 'RELOCATE_INTERNAL':\n",
        "            tgt, path = self._find_best_internal_spot(cid)\n",
        "            if tgt == -1:          # ìë¦¬ ëª» ì°¾ìŒ\n",
        "                event_rew -= 0.2\n",
        "            else:\n",
        "                cost += self._move_cost(len(path)-1, is_reloc=True)\n",
        "                src = self.car_locations.get(cid)\n",
        "                if src is not None:\n",
        "                    self.node_status[src] = -1\n",
        "                self.node_status[tgt] = cid\n",
        "                self.car_locations[cid] = tgt\n",
        "                self.relocations_this_episode += 1\n",
        "                event_rew -= 0.02\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # 4) Reward, done flag, state ë°˜í™˜\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # â–¶â–¶  COST ê°€ ê³§ ìŒì˜ ë³´ìƒ  â—€â—€\n",
        "\n",
        "        reward = event_rew - cost        # cost ìì²´(ì–‘ìˆ˜)ë¥¼ ê·¸ëŒ€ë¡œ ìŒë³´ìƒ\n",
        "        done = (self.current_port >= self.num_ports) and (len(self.cars_on_board) == 0)\n",
        "\n",
        "        return self._get_state(), reward, cost, done\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# ì„¹ì…˜ 3: ê³„ì¸µì  í™˜ê²½ Wrapper (ë¦¬íŒ©í† ë§ ë²„ì „)\n",
        "# ==============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.data import Batch\n",
        "\n",
        "class HierarchicalEnvWrapper:\n",
        "    def __init__(self, problem_data: dict, max_num_ports: int,\n",
        "                 worker_agent, config):\n",
        "        self.problem_data     = problem_data\n",
        "        self.max_num_ports    = max_num_ports\n",
        "        self.worker_agent     = worker_agent\n",
        "        self.config           = config\n",
        "        self.last_manager_action = None\n",
        "\n",
        "        # â”€â”€ í•µì‹¬ ê°ì²´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        self.ship_env = ShipEnv(problem_data, max_num_ports)\n",
        "\n",
        "        # â˜… ì²´ë¥˜ ìŠ¤í… ì¹´ìš´í„° ì´ˆê¸°í™”\n",
        "        self.steps_on_port = 0          # â† ì—†ë˜ í•„ë“œ\n",
        "\n",
        "        # Manager action ì¸ì½”ë”©\n",
        "        self.manager_action_map = {\n",
        "            0: 'CLEAR_BLOCKERS',\n",
        "            1: 'FINISH_UNLOAD',\n",
        "            2: 'FINISH_LOAD',\n",
        "            3: 'CLEAR_TEMP',\n",
        "            4: 'PROCEED_TO_NEXT_PORT'\n",
        "        }\n",
        "        self.goal_embedding = nn.Embedding(\n",
        "            self.config.MANAGER_ACTION_DIM,\n",
        "            self.config.GOAL_EMBED_DIM\n",
        "        ).to(self.worker_agent.device)\n",
        "\n",
        "    def _calculate_potential(self) -> float:\n",
        "        s = self.ship_env\n",
        "        # â‘  ë„ì°©ì§€ â†‘ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ í˜„ì¬ ì„ ë‚´ ì°¨ëŸ‰ì„ ì •ë ¬\n",
        "        ordered = sorted(\n",
        "            [(self.ship_env.cars[cid]['dest'], cid) for cid in s.cars_on_board]\n",
        "        )\n",
        "\n",
        "        rank = 0\n",
        "        surplus = 0\n",
        "        for dest, cid in ordered:\n",
        "            depth = s._gate_depth(s.car_locations[cid])\n",
        "            surplus += max(0, depth - rank)   # â€˜ì´ìƒì ìœ¼ë¡œ í•„ìš”í•œ ê±°ë¦¬(rank)â€™ë¥¼ ì´ˆê³¼í•œ ë§Œí¼\n",
        "            rank += 1\n",
        "\n",
        "        # ì„ì‹œ í•˜ì—­ì€ ê¹Šì´ì— 0.5ì¹¸ì§œë¦¬ ë¶ˆì´ìµì„ ì£¼ë©´ ì¶©ë¶„\n",
        "        surplus += 0.5 * len(s.temporarily_unloaded_cars)\n",
        "\n",
        "        # **ê°’ì´ ì‘ì„ìˆ˜ë¡ ì¢‹ì€ ìƒíƒœ** â†’ ê·¸ëŒ€ë¡œ ìŒìˆ˜ ë¶€í˜¸ ì•ˆ ì¨ë„ ë¨\n",
        "        return surplus\n",
        "\n",
        "\n",
        "    def reset(self, prob_data=None):\n",
        "        if prob_data is not None:\n",
        "            self.problem_data = prob_data\n",
        "            self.ship_env     = ShipEnv(self.problem_data, self.max_num_ports)\n",
        "        else:\n",
        "            self.ship_env.reset()\n",
        "\n",
        "        # â˜… í¬íŠ¸ ì²´ë¥˜ ì¹´ìš´í„°ë„ í•­ìƒ ë¦¬ì…‹\n",
        "        self.steps_on_port       = 0\n",
        "        self.last_manager_action = None\n",
        "        return self._get_manager_state()\n",
        "\n",
        "\n",
        "    def _get_manager_state(self):\n",
        "        s = self.ship_env\n",
        "        total_slots = s.num_nodes - 1 if s.num_nodes > 1 else 1\n",
        "        port_norm = s.current_port / s.num_ports\n",
        "        free_slots_norm = sum(1 for n in s.node_status[1:] if n == -1) / total_slots\n",
        "        waiting_to_load = len([c for c in s.cars if c['origin']==s.current_port and c['id'] not in s.cars_on_board and c['id'] not in s.temporarily_unloaded_cars])\n",
        "        due_to_unload = len([c for c in s.cars if c['id'] in s.cars_on_board and c['dest']==s.current_port])\n",
        "        on_board_dests = [s.cars[cid]['dest'] for cid in s.cars_on_board]\n",
        "        avg_dest_dist = (sum(d - s.current_port for d in on_board_dests)/len(on_board_dests)) if on_board_dests else 0.0\n",
        "\n",
        "        return torch.tensor([\n",
        "            port_norm,\n",
        "            free_slots_norm,\n",
        "            waiting_to_load / s.total_cars,\n",
        "            due_to_unload / s.total_cars,\n",
        "            len(s.temporarily_unloaded_cars) / s.total_cars,\n",
        "            avg_dest_dist / s.num_ports\n",
        "        ], dtype=torch.float)\n",
        "\n",
        "    def _is_goal_achieved(self, goal_str: str) -> bool:\n",
        "        \"\"\"\n",
        "        ê° ëª©í‘œ(goal_str)ê°€ ì¶©ì¡±ë˜ì—ˆëŠ”ì§€ íŒë‹¨í•œë‹¤.\n",
        "\n",
        "        CLEAR_BLOCKERS = â‘  â€˜í•˜ì—­ ëŒ€ìƒ ì°¨ê°€ ì—†ìŒâ€™\n",
        "                       + â‘¡ ê²Œì´íŠ¸(ë…¸ë“œ 0)â†’ê° ì°¨ëŸ‰ê¹Œì§€ì˜ **í†µë¡œ**ì—\n",
        "                            ë‹¤ë¥¸ ì°¨ê°€ ì „í˜€ ì—†ì„ ë•Œ(True)\n",
        "\n",
        "        â”€â”€ ìˆ˜ì • í•µì‹¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        ê¸°ì¡´ path_blocked()ëŠ” ìì‹ ì˜ ìë¦¬(node_idx)ë„\n",
        "        â€˜ë§‰íŒ ë…¸ë“œâ€™ë¡œ ê³„ì‚°í•´ í•­ìƒ Trueê°€ ë˜ì—ˆë‹¤.\n",
        "        â‡’ ë§ˆì§€ë§‰ ë…¸ë“œ(ìê¸° ìë¦¬)ëŠ” ì œì™¸í•˜ê³  ê²€ì‚¬í•œë‹¤.\n",
        "        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        \"\"\"\n",
        "        s = self.ship_env\n",
        "\n",
        "        # â‘  í˜„ì¬ í•­êµ¬ì—ì„œ í•˜ì—­í•´ì•¼ í•  ì°¨ê°€ ë‚¨ì•„ ìˆëŠ”ê°€?\n",
        "        has_due_to_unload = any(\n",
        "            c['dest'] == s.current_port and c['id'] in s.cars_on_board\n",
        "            for c in s.cars\n",
        "        )\n",
        "\n",
        "        # â‘¡ ê²Œì´íŠ¸ â†’ ê° ì°¨ëŸ‰ìœ¼ë¡œ ì´ì–´ì§€ëŠ” í†µë¡œì— â€˜ë‹¤ë¥¸â€™ ì°¨ê°€ ìˆëŠ”ê°€?\n",
        "        def path_blocked() -> bool:\n",
        "            for cid in s.cars_on_board:\n",
        "                node_idx = s.car_locations[cid]\n",
        "                path = s._get_or_compute_path(0, node_idx)\n",
        "                if not path: continue\n",
        "                # ë„ì°© í•­êµ¬ê°€ â€œí˜„ì¬ í•­êµ¬ë³´ë‹¤ ë¹ ë¥¸ ì°¨â€ê°€ ê²½ë¡œ ìœ„ì— ìˆìœ¼ë©´ ë¸”ë¡œí‚¹\n",
        "                for n in path[1:-1]:\n",
        "                    other = s.node_status[n]\n",
        "                    if other == -1: continue\n",
        "                    if s.cars[other]['dest'] < s.current_port:   # í•µì‹¬ ì¶”ê°€\n",
        "                        return True\n",
        "            return False\n",
        "\n",
        "\n",
        "        if goal_str == 'FINISH_UNLOAD':\n",
        "            return not has_due_to_unload\n",
        "\n",
        "        elif goal_str == 'CLEAR_BLOCKERS':\n",
        "            return (not has_due_to_unload) and (not path_blocked())\n",
        "\n",
        "        elif goal_str == 'FINISH_LOAD':\n",
        "            return not any(\n",
        "                c['origin'] == s.current_port and\n",
        "                c['id'] not in s.cars_on_board and\n",
        "                c['id'] not in s.temporarily_unloaded_cars\n",
        "                for c in s.cars\n",
        "            )\n",
        "\n",
        "        elif goal_str == 'CLEAR_TEMP':\n",
        "            return len(s.temporarily_unloaded_cars) == 0\n",
        "\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "\n",
        "\n",
        "    # ======================================================================\n",
        "    #  HierarchicalEnvWrapper.step  â€•  ìµœì¢… êµì²´ë³¸\n",
        "    # ======================================================================\n",
        "    def step(self, manager_action_idx: int, *, greedy_worker: bool = False):\n",
        "        \"\"\"\n",
        "        Manager ì•¡ì…˜ì„ ë°›ì•„ Worker ë¡¤ì•„ì›ƒê¹Œì§€ ìˆ˜í–‰í•œ ë’¤\n",
        "        (ë‹¤ìŒ Manager ìƒíƒœ, Manager ë³´ìƒ, done, info) ë°˜í™˜\n",
        "        \"\"\"\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ë³¸ ì„¸íŒ… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        goal_str         = self.manager_action_map[manager_action_idx]\n",
        "        goal_embed       = self.goal_embedding(\n",
        "            torch.tensor([manager_action_idx], device=self.worker_agent.device)\n",
        "        )\n",
        "        potential_before = self._calculate_potential()\n",
        "\n",
        "        # í•­êµ¬ ì²´ë¥˜ ìŠ¤í… ì¹´ìš´í„°  â”€â”€ NEW â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        self.steps_on_port += 1\n",
        "\n",
        "        # Worker í†µê³„\n",
        "        worker_steps         = 0\n",
        "        total_cost           = 0.0\n",
        "        total_worker_reward  = 0.0\n",
        "        no_progress          = 0\n",
        "        no_progress_trigger  = False\n",
        "\n",
        "        # Storage (í•™ìŠµ ëª¨ë“œì¼ ë•Œë§Œ)\n",
        "        worker_storage = None\n",
        "        if (goal_str != 'PROCEED_TO_NEXT_PORT') and (not greedy_worker):\n",
        "            worker_storage = PPOStorage(\n",
        "                self.config.WORKER_NUM_STEPS_PER_UPDATE,\n",
        "                (2,),                         # [action_type, car_id]\n",
        "                self.worker_agent.device\n",
        "            )\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë””ë²„ê·¸Â·ì•ˆì •ì„± íŒŒë¼ë¯¸í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        BONUS_EVERY_N      = 100\n",
        "        PERIODIC_EVERY_N   = 150\n",
        "        # STREAK_LIMIT       = 15             # â˜… â€œë™ì¼ í–‰ë™Â·ì°¨ëŸ‰â€ ì—°ì† í—ˆìš©ì¹˜\n",
        "        EPS                = 1e-6\n",
        "        last_dbg_key       = None\n",
        "        bonus_skip_counter = 0\n",
        "        same_action_streak = 0              # â˜… streak ì¹´ìš´í„°\n",
        "        last_worker_key    = None           # â˜… (act_type, car_id)\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Worker ë£¨í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        if goal_str != 'PROCEED_TO_NEXT_PORT':\n",
        "            batch_graph = Batch.from_data_list(\n",
        "                [self.ship_env._get_state()]\n",
        "            ).to(self.worker_agent.device)\n",
        "\n",
        "            last_worker_key    = None\n",
        "            same_action_streak = 0        # streak ì¹´ìš´í„°\n",
        "\n",
        "            for i in range(self.config.WORKER_MAX_STEPS_PER_GOAL):\n",
        "                worker_steps = i + 1\n",
        "\n",
        "                # # 0-a) ë™ì¼ í–‰ë™Â·ì°¨ëŸ‰ ìŠ¤íŠ¸ë¦­ ì´ˆê³¼   â”€â”€ NEW â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "                # if same_action_streak >= STREAK_LIMIT:\n",
        "                #     logging.info(f\"[WRK] break â€” same action repeated {STREAK_LIMIT} times\")\n",
        "                #     no_progress_trigger = True\n",
        "                #     break\n",
        "\n",
        "                # 0-b) â€˜ì§„ë„ ì—†ìŒâ€™ í•œë„ ì´ˆê³¼ ì‹œ íƒˆì¶œ\n",
        "                if (goal_str == 'FINISH_LOAD') and (no_progress >= self.config.NO_PROGRESS_LIMIT):\n",
        "                    no_progress_trigger = True\n",
        "                    break\n",
        "\n",
        "                # 1) ëª©í‘œ ë‹¬ì„± ì—¬ë¶€\n",
        "                if self._is_goal_achieved(goal_str):\n",
        "                    break\n",
        "\n",
        "                # 2) í•©ë²• ì•¡ì…˜\n",
        "                legal_actions = self.ship_env.get_legal_actions(for_worker=True)\n",
        "                if not legal_actions:\n",
        "                    break\n",
        "\n",
        "                # 3) ìµœì‹  ê·¸ë˜í”„ ë®ì–´ì“°ê¸°\n",
        "                tmp_state        = self.ship_env._get_state()\n",
        "                batch_graph.x    = tmp_state.x.to(batch_graph.x.device)\n",
        "                batch_graph.global_features = tmp_state.global_features.to(batch_graph.global_features.device)\n",
        "\n",
        "                # 4) í–‰ë™ ì„ íƒ\n",
        "                action, at, logp, ent, val = self.worker_agent.get_action_and_value(\n",
        "                    batch_graph, legal_actions, goal_embed, greedy=greedy_worker\n",
        "                )\n",
        "                action_type, car_id = action\n",
        "\n",
        "                # 5) Streak ì—…ë°ì´íŠ¸  â”€â”€ NEW â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "                worker_key = (action_type, car_id)\n",
        "                if worker_key == last_worker_key:\n",
        "                    same_action_streak += 1\n",
        "                else:\n",
        "                    same_action_streak = 1\n",
        "                last_worker_key = worker_key\n",
        "                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "                # 6) intrinsic ë³´ë„ˆìŠ¤ ëŒ€ìƒ í”Œë˜ê·¸\n",
        "                is_temp_before = (car_id != -1) and (car_id in self.ship_env.temporarily_unloaded_cars)\n",
        "                is_due_before  = (car_id != -1) and (self.ship_env.cars[car_id]['dest'] == self.ship_env.current_port)\n",
        "\n",
        "                # 7) í™˜ê²½ í•œ ìŠ¤í…\n",
        "                _, worker_reward, move_cost, overall_done = self.ship_env.step(action)\n",
        "                total_cost          += move_cost\n",
        "                intrinsic_reward     = 0.0\n",
        "                if goal_str == 'FINISH_UNLOAD' and action_type == 'UNLOAD' and is_due_before:\n",
        "                    intrinsic_reward += 0.8\n",
        "                elif goal_str == 'CLEAR_TEMP' and action_type == 'LOAD' and is_temp_before:\n",
        "                    intrinsic_reward += 0.4\n",
        "                worker_reward       += intrinsic_reward\n",
        "                total_worker_reward += worker_reward\n",
        "\n",
        "                # 8) no-progress ì—…ë°ì´íŠ¸\n",
        "                load_succeeded = (action_type == 'LOAD') and (worker_reward > 0)\n",
        "                if goal_str == 'FINISH_LOAD' and load_succeeded:\n",
        "                    no_progress = 0\n",
        "                else:\n",
        "                    no_progress += 1\n",
        "\n",
        "                # 9) Storage ì €ì¥\n",
        "                if worker_storage is not None:\n",
        "                    worker_storage.add(tmp_state, at, logp, worker_reward, overall_done, val)\n",
        "\n",
        "                # 10) ë””ë²„ê·¸ ë¡œê·¸ (â€¦ ìƒëµ, ê¸°ì¡´ê³¼ ë™ì¼) ----------------------\n",
        "\n",
        "                if overall_done:\n",
        "                    break\n",
        "\n",
        "            # --- Worker ë£¨í”„ ì¢…ë£Œ í›„ streak ì´ˆê¸°í™” -----------------\n",
        "            same_action_streak = 0        # â† ë‹¤ìŒ goal ë¡œ ì´ì–´ì§€ì§€ ì•Šê²Œ\n",
        "\n",
        "            # 11) PPO ì—…ë°ì´íŠ¸\n",
        "            if worker_storage and worker_storage.step > 0 and not greedy_worker:\n",
        "                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "                # RuntimeError(\"â€¦ grad_fn â€¦\") ë°©ì§€ìš©\n",
        "                # ë¡¤ì•„ì›ƒ ë•ŒëŠ” no-grad ë¡œ ëŒì•˜ìœ¼ë¯€ë¡œ,\n",
        "                # ì—…ë°ì´íŠ¸ ë°”ë¡œ ì§ì „ì— grad ë¥¼ ë‹¤ì‹œ ì¼œ ì£¼ëŠ” í•œ ì¤„.\n",
        "                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "                with torch.enable_grad():                     # â† ì¶”ê°€\n",
        "                    self.worker_agent.update(worker_storage,  # â† ê¸°ì¡´ í˜¸ì¶œ\n",
        "                                            goal_embed.detach())\n",
        "\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Manager ë³´ìƒ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        goal_done    = self._is_goal_achieved(goal_str)\n",
        "        event_reward = 0.0\n",
        "\n",
        "        # ì‹¤íŒ¨ íŒ¨ë„í‹°\n",
        "        if (worker_steps >= self.config.WORKER_MAX_STEPS_PER_GOAL) and (not goal_done):\n",
        "            penalty = (2 * self.config.TIMEOUT_PENALTY) if goal_str == 'CLEAR_BLOCKERS' else self.config.TIMEOUT_PENALTY\n",
        "            event_reward += penalty\n",
        "\n",
        "        # PBRS\n",
        "        potential_after = self._calculate_potential()\n",
        "        event_reward += self.config.SHAPING_REWARD_WEIGHT * (\n",
        "            self.config.MANAGER_GAMMA * potential_after - potential_before\n",
        "        )\n",
        "\n",
        "        # í•­êµ¬ ì´ë™ ëª…ë ¹ ì²˜ë¦¬\n",
        "        if goal_str == 'PROCEED_TO_NEXT_PORT':\n",
        "            waiting = [c for c in self.ship_env.cars if c['origin']==self.ship_env.current_port\n",
        "                       and c['id'] not in self.ship_env.cars_on_board\n",
        "                       and c['id'] not in self.ship_env.temporarily_unloaded_cars]\n",
        "            can_go = not (waiting or self.ship_env.temporarily_unloaded_cars)\n",
        "            if can_go:\n",
        "                self.ship_env.current_port += 1\n",
        "                self.steps_on_port = 0            # â˜… ì²´ë¥˜ ì¹´ìš´í„° ë¦¬ì…‹\n",
        "                event_reward += 5.0\n",
        "                goal_done = True\n",
        "            else:\n",
        "                event_reward -= 10.0\n",
        "                goal_done = False\n",
        "\n",
        "        # â”€â”€ í•­êµ¬ ì²´ë¥˜ ì œí•œ íŒ¨ë„í‹°ë§Œ ì ìš©(ê°•ì œ ì´ë™ X)  â”€â”€ NEW â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        MAX_STEP_PER_PORT = 4000\n",
        "        if self.steps_on_port > MAX_STEP_PER_PORT:\n",
        "            event_reward += self.config.TIMEOUT_PENALTY * 5   # í° íŒ¨ë„í‹°\n",
        "            logging.info(\"[MGR] Port-stay limit exceeded â€” penalty applied\")\n",
        "            self.steps_on_port = MAX_STEP_PER_PORT            # ë” ì¦ê°€ ì•ˆ í•¨\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "        # ìµœì¢… Manager reward\n",
        "        manager_reward  = event_reward - (total_cost * 0.001)\n",
        "        manager_reward -= self.config.STEP_PENALTY_WEIGHT * worker_steps\n",
        "        # if self.last_manager_action == manager_action_idx:\n",
        "        #     manager_reward += self.config.REPEAT_PENALTY\n",
        "        if no_progress_trigger:\n",
        "            manager_reward += self.config.NO_PROGRESS_PENALTY\n",
        "            goal_done = True              # â† ë” ì´ìƒ ê°™ì€ goal ë°˜ë³µ ê¸ˆì§€\n",
        "\n",
        "        self.last_manager_action = manager_action_idx\n",
        "\n",
        "        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ë³´ë„ˆìŠ¤/íŒ¨ë„í‹°\n",
        "        done = self.ship_env.current_port >= self.ship_env.num_ports\n",
        "        if done:\n",
        "            if len(self.ship_env.delivered_cars) == self.ship_env.total_cars:\n",
        "                manager_reward += 1000.0\n",
        "            else:\n",
        "                manager_reward -= (self.ship_env.total_cars - len(self.ship_env.delivered_cars)) * 10.0\n",
        "\n",
        "        # ë‹¤ìŒ Manager state\n",
        "        next_state = self._get_manager_state()\n",
        "\n",
        "        info = {\n",
        "            'steps'              : worker_steps,\n",
        "            'goal'               : goal_str,\n",
        "            'success'            : goal_done,\n",
        "            'cost'               : total_cost,\n",
        "            'worker_total_reward': total_worker_reward,\n",
        "        }\n",
        "        return next_state, manager_reward, done, info\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# ì„¹ì…˜ 4: ì—ì´ì „íŠ¸ ë° ë„¤íŠ¸ì›Œí¬ (ë¦¬íŒ©í† ë§ ë²„ì „)\n",
        "# ==============================================================================\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from torch_geometric.data import Batch\n",
        "from torch_geometric.nn import GlobalAttention, global_mean_pool\n",
        "\n",
        "# --- PPO ì•Œê³ ë¦¬ì¦˜ì„ ìœ„í•œ ê²½í—˜ ì €ì¥ì†Œ ---\n",
        "\n",
        "class PPOStorage:\n",
        "    \"\"\"PPO í•™ìŠµì„ ìœ„í•œ ë¡¤ì•„ì›ƒ(rollout) ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤.\"\"\"\n",
        "    def __init__(self, num_steps: int, action_shape: tuple, device: torch.device, state_shape: tuple = None, manager_action_dim: int = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_steps (int): ì €ì¥í•  ì´ ìŠ¤í… ìˆ˜.\n",
        "            action_shape (tuple): í–‰ë™ì˜ í˜•íƒœ.\n",
        "            device (torch.device): ë°ì´í„°ê°€ ì €ì¥ë  ì¥ì¹˜ (CPU ë˜ëŠ” CUDA).\n",
        "            state_shape (tuple, optional): ìƒíƒœì˜ í˜•íƒœ. ê·¸ë˜í”„ ë°ì´í„°ê°€ ì•„ë‹ˆë©´ ì§€ì •.\n",
        "            manager_action_dim (int, optional): Managerì˜ ê²½ìš°, ì•¡ì…˜ ë§ˆìŠ¤í¬ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´ í•„ìš”.\n",
        "        \"\"\"\n",
        "        self.num_steps = num_steps\n",
        "        self.device = device\n",
        "        self.step = 0\n",
        "        self.is_graph_data = (state_shape is None)\n",
        "\n",
        "        # ë°ì´í„° ì €ì¥ ë²„í¼ ì´ˆê¸°í™”\n",
        "        if self.is_graph_data:\n",
        "            self.obs = [None] * num_steps\n",
        "        else:\n",
        "            self.obs = torch.zeros((num_steps,) + state_shape, device=device)\n",
        "\n",
        "        if isinstance(action_shape, int): action_shape = (action_shape,)\n",
        "        self.actions = torch.zeros((num_steps,) + action_shape, device=device, dtype=torch.long)\n",
        "        self.logprobs = torch.zeros(num_steps, device=device)\n",
        "        self.rewards = torch.zeros(num_steps, device=device)\n",
        "        self.dones = torch.zeros(num_steps, device=device)\n",
        "        self.values = torch.zeros(num_steps, device=device)\n",
        "\n",
        "        # [í•µì‹¬ ìˆ˜ì •] Managerì˜ ìƒíƒœ ì˜ì¡´ì  ì•¡ì…˜ ë§ˆìŠ¤í¬ë¥¼ ì €ì¥í•  ê³µê°„\n",
        "        if manager_action_dim:\n",
        "            self.masks = torch.zeros((num_steps, manager_action_dim), device=device)\n",
        "        else:\n",
        "            self.masks = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.step = 0\n",
        "\n",
        "    def add(self, obs, action, logprob, reward, done, value, mask=None):\n",
        "        \"\"\"í•œ ìŠ¤í…ì˜ ê²½í—˜ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
        "        if self.step >= self.num_steps: return\n",
        "\n",
        "        if self.is_graph_data:\n",
        "            self.obs[self.step] = obs # GPU ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ CPUì— ì €ì¥\n",
        "        else:\n",
        "            self.obs[self.step].copy_(torch.as_tensor(obs, device=self.device))\n",
        "\n",
        "        self.actions[self.step] = action\n",
        "        self.logprobs[self.step] = logprob\n",
        "        self.rewards[self.step] = torch.tensor(reward, dtype=torch.float32)\n",
        "        self.dones[self.step] = torch.tensor(done, dtype=torch.float32)\n",
        "        self.values[self.step] = value.detach()\n",
        "\n",
        "        # [í•µì‹¬ ìˆ˜ì •] Managerì˜ ì•¡ì…˜ ë§ˆìŠ¤í¬ ì €ì¥\n",
        "        if self.masks is not None and mask is not None:\n",
        "            self.masks[self.step] = mask.detach()\n",
        "\n",
        "        self.step += 1\n",
        "\n",
        "    def is_full(self) -> bool:\n",
        "        return self.step >= self.num_steps\n",
        "\n",
        "    def compute_returns_and_advantages(self, last_value: torch.Tensor, gamma: float, gae_lambda: float):\n",
        "        \"\"\"GAEë¥¼ ì‚¬ìš©í•˜ì—¬ ë³´ìƒ(Return)ê³¼ ì–´ë“œë°´í‹°ì§€(Advantage)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\"\"\"\n",
        "        advantages = torch.zeros_like(self.rewards).to(self.device)\n",
        "        last_gae = 0.0\n",
        "        for t in reversed(range(self.num_steps)):\n",
        "            next_non_terminal = 1.0 - self.dones[t]\n",
        "            next_value = last_value if t == self.num_steps - 1 else self.values[t + 1]\n",
        "\n",
        "            delta = self.rewards[t] + gamma * next_value * next_non_terminal - self.values[t]\n",
        "            last_gae = delta + gamma * gae_lambda * next_non_terminal * last_gae\n",
        "            advantages[t] = last_gae\n",
        "\n",
        "        # ì–´ë“œë°´í‹°ì§€ë¥¼ ì´ìš©í•´ ìµœì¢… Return ê³„ì‚°\n",
        "        self.returns = advantages + self.values\n",
        "        # ì–´ë“œë°´í‹°ì§€ ì •ê·œí™” (í•™ìŠµ ì•ˆì •í™”)\n",
        "        self.advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "\n",
        "\n",
        "# --- ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜ ---\n",
        "# WorkerNetwork í´ë˜ìŠ¤ ì „ì²´\n",
        "class WorkerNetwork(nn.Module):\n",
        "    \"\"\"Worker ì—ì´ì „íŠ¸ì˜ ì •ì±… ë° ê°€ì¹˜ ì‹ ê²½ë§\"\"\"\n",
        "    def __init__(self, node_feature_size: int, global_feature_size: int, max_cars: int, num_nodes: int, config: Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        embed_dim = self.config.GNN_EMBED_DIM\n",
        "        self.node_input_proj = nn.Linear(node_feature_size, embed_dim)\n",
        "        self.positional_encoding = nn.Embedding(num_nodes, embed_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4, dim_feedforward=embed_dim*4, dropout=0.1, activation='gelu', batch_first=True, norm_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
        "        self.att_pool = GlobalAttention(gate_nn=nn.Sequential(nn.Linear(embed_dim, embed_dim // 2), nn.ReLU(), nn.Linear(embed_dim // 2, 1)))\n",
        "\n",
        "        mlp_input_dim = embed_dim * 2 + global_feature_size + self.config.GOAL_EMBED_DIM\n",
        "        self.mlp = nn.Sequential(nn.Linear(mlp_input_dim, 512), nn.GELU(), nn.Linear(512, 256), nn.GELU())\n",
        "        self.actor_type_head = nn.Linear(256, 3)\n",
        "        self.actor_load_head = nn.Linear(256, max_cars)\n",
        "        self.actor_unload_head = nn.Linear(256, max_cars)\n",
        "        self.actor_relocate_head = nn.Linear(256, max_cars)\n",
        "        self.critic_head = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, data, goal_embedding):\n",
        "        current_device = goal_embedding.device\n",
        "        x, global_feats, batch_index = data.x.to(current_device), data.global_features.to(current_device), data.batch.to(current_device)\n",
        "        ptr = data.ptr.to(current_device) if hasattr(data, 'ptr') and data.ptr is not None else torch.tensor([0, x.size(0)], device=current_device)\n",
        "\n",
        "        node_embeddings = self.node_input_proj(x)\n",
        "        pos_enc_list = [self.positional_encoding(torch.arange(ptr[i+1] - ptr[i], device=current_device)) for i in range(len(ptr)-1)]\n",
        "        pos_enc = torch.cat(pos_enc_list) if pos_enc_list else torch.empty(0, self.config.GNN_EMBED_DIM, device=current_device)\n",
        "        if node_embeddings.size(0) == pos_enc.size(0): node_embeddings += pos_enc\n",
        "\n",
        "        if len(ptr) > 1 and x.shape[0] > 0: max_len = (ptr[1:] - ptr[:-1]).max().item()\n",
        "        elif x.shape[0] > 0: max_len = x.shape[0]\n",
        "        else: max_len = 1\n",
        "\n",
        "        padded_x, masks_list = [], []\n",
        "        if len(ptr) > 1:\n",
        "            for i in range(len(ptr) - 1):\n",
        "                start, end = ptr[i], ptr[i+1]\n",
        "                graph_len, current_nodes = end - start, node_embeddings[start:end]\n",
        "                pad = torch.zeros(max_len - graph_len, self.config.GNN_EMBED_DIM, device=current_device)\n",
        "                padded_x.append(torch.cat([current_nodes, pad]))\n",
        "                mask = torch.ones(max_len, dtype=torch.bool, device=current_device)\n",
        "                mask[:graph_len] = False\n",
        "                masks_list.append(mask)\n",
        "        else:\n",
        "            padded_x.append(node_embeddings)\n",
        "            masks_list.append(torch.zeros(node_embeddings.shape[0], dtype=torch.bool, device=current_device))\n",
        "\n",
        "        padded_x, attention_mask = torch.stack(padded_x), torch.stack(masks_list)\n",
        "        transformer_out = self.transformer_encoder(padded_x, src_key_padding_mask=attention_mask)\n",
        "        transformer_out_flat = transformer_out[~attention_mask]\n",
        "\n",
        "        graph_emb_mean = global_mean_pool(transformer_out_flat, batch_index)\n",
        "        graph_emb_att = self.att_pool(transformer_out_flat, batch_index)\n",
        "        graph_emb = torch.cat([graph_emb_mean, graph_emb_att], dim=1)\n",
        "\n",
        "        if goal_embedding.shape[0] != graph_emb.shape[0]:\n",
        "            goal_embedding = goal_embedding.expand(graph_emb.shape[0], -1)\n",
        "\n",
        "        combined_features = torch.cat([graph_emb, global_feats, goal_embedding], dim=1)\n",
        "        final_features = self.mlp(combined_features)\n",
        "\n",
        "        # [ìˆ˜ì •] ë°˜í™˜ ê°’ì— graph_emb ì¶”ê°€\n",
        "        return (self.actor_type_head(final_features), self.actor_load_head(final_features),\n",
        "                self.actor_unload_head(final_features), self.actor_relocate_head(final_features),\n",
        "                self.critic_head(final_features).squeeze(-1), graph_emb)\n",
        "\n",
        "\n",
        "\n",
        "class ManagerNetwork(nn.Module):\n",
        "    def __init__(self, state_dim: int, action_dim: int, num_layers: int = 2, nhead: int = 4):\n",
        "        super().__init__()\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.action_embed = nn.Embedding(action_dim, 32)\n",
        "\n",
        "        self.input_proj = nn.Linear(state_dim + 32, 128)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=128,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=256,\n",
        "            batch_first=True,\n",
        "            activation='gelu'\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.actor_head = nn.Linear(128, action_dim)\n",
        "        self.critic_head = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, prev_action_idx: torch.LongTensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state: [B, state_dim]\n",
        "            prev_action_idx: [B]\n",
        "        Returns:\n",
        "            logits: [B, action_dim]\n",
        "            value:  [B]\n",
        "        \"\"\"\n",
        "        a_emb = self.action_embed(prev_action_idx)            # [B, 32]\n",
        "        x = torch.cat([state, a_emb], dim=-1)                 # [B, state_dim + 32]\n",
        "        x = self.input_proj(x).unsqueeze(1)                   # [B, 1, 128]\n",
        "\n",
        "        encoded = self.encoder(x)                             # [B, 1, 128]\n",
        "        h = encoded.squeeze(1)                                # [B, 128]\n",
        "\n",
        "        logits = self.actor_head(h)                           # [B, action_dim]\n",
        "        value  = self.critic_head(h).squeeze(-1)              # [B]\n",
        "        return logits, value\n",
        "\n",
        "# --- ì—ì´ì „íŠ¸ í´ë˜ìŠ¤ ì •ì˜ ---\n",
        "\n",
        "class ManagerAgent:\n",
        "    \"\"\"ìƒìœ„ ë ˆë²¨ì˜ ëª©í‘œë¥¼ ê²°ì •í•˜ëŠ” Manager ì—ì´ì „íŠ¸.\"\"\"\n",
        "    def __init__(self, config: Config, device: torch.device, env_wrapper: HierarchicalEnvWrapper):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.env_wrapper = env_wrapper # í˜„ì¬ í™˜ê²½ ìƒíƒœì— ì ‘ê·¼í•˜ì—¬ ì•¡ì…˜ ë§ˆìŠ¤í‚¹\n",
        "        self.action_map = env_wrapper.manager_action_map\n",
        "        self.type_to_idx = {v: k for k, v in self.env_wrapper.manager_action_map.items()}\n",
        "        self.idx_to_type = {v: k for k, v in self.type_to_idx.items()}\n",
        "\n",
        "        self.network = ManagerNetwork(config.MANAGER_STATE_DIM, config.MANAGER_ACTION_DIM).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.network.parameters(), lr=config.MANAGER_LR, eps=1e-5)\n",
        "\n",
        "\n",
        "    def _build_action_mask(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        ë‹¬ì„±ëœ goal, í˜¹ì€ ë¬¼ë¦¬ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥í•œ goalì˜ ë¡œì§“ì„ -1e9 ë¡œ ë‚´ë ¤ì„œ\n",
        "        softmax í™•ë¥  0 ì´ ë˜ê²Œ ë§Œë“ ë‹¤.\n",
        "        \"\"\"\n",
        "        mask = torch.zeros(1, self.config.MANAGER_ACTION_DIM, device=self.device)\n",
        "        s_env = self.env_wrapper.ship_env # ship_envì— ë” ì‰½ê²Œ ì ‘ê·¼\n",
        "\n",
        "        # 1. ì´ë¯¸ ë‹¬ì„±ëœ ëª©í‘œ ë§ˆìŠ¤í‚¹\n",
        "        for idx, goal in self.env_wrapper.manager_action_map.items():\n",
        "            if self.env_wrapper._is_goal_achieved(goal):\n",
        "                mask[0, idx] = -1e9\n",
        "\n",
        "        # ğŸ’¡ --- [í•µì‹¬ ì¶”ê°€] --- ğŸ’¡\n",
        "        # 2. 'PROCEED_TO_NEXT_PORT'ê°€ ë¶ˆê°€ëŠ¥í•œ ì¡°ê±´ì¼ ë•Œ ë§ˆìŠ¤í‚¹\n",
        "        #   - í•­êµ¬ì— ë‚´ë ¤ì•¼ í•  ì°¨ê°€ ìˆê±°ë‚˜, ì„ì‹œë¡œ ë‚´ë¦° ì°¨ê°€ ìˆìœ¼ë©´ PROCEED ë¶ˆê°€\n",
        "        waiting_cars = any(c['origin'] == s_env.current_port and c['id'] not in s_env.cars_on_board and c['id'] not in s_env.temporarily_unloaded_cars for c in s_env.cars)\n",
        "        has_temp_unloaded = bool(s_env.temporarily_unloaded_cars)\n",
        "\n",
        "        if waiting_cars or has_temp_unloaded:\n",
        "            proceed_idx = self.type_to_idx['PROCEED_TO_NEXT_PORT']\n",
        "            mask[0, proceed_idx] = -1e9\n",
        "        # ğŸ’¡ --- [ìˆ˜ì • ì™„ë£Œ] --- ğŸ’¡\n",
        "        # ë¡œê·¸ì— action 0-4 ë§ˆìŠ¤í¬ ìƒíƒœê°€ ì°í ê²ƒ. ì „ë¶€ -1e9 (ê¸ˆì§€)ë¡œ ë‚˜ì˜¤ë©´ ë§ˆìŠ¤í‚¹ ì˜¤ë¥˜.\n",
        "        logging.debug(f\"[MASK] allowed={[(i, m.item()==0) for i,m in enumerate(mask[0])]}  temp=# {len(s_env.temporarily_unloaded_cars)}  wait-load? {waiting_cars}\")\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def get_action_and_value(\n",
        "        self,\n",
        "        state: torch.Tensor,\n",
        "        legal_actions: list[tuple[str,int]],\n",
        "        prev_action_idx: torch.LongTensor,\n",
        "        greedy: bool = False\n",
        "    ) -> tuple: # [ìˆ˜ì •] ë°˜í™˜ íƒ€ì… íŠœí”Œë¡œ ëª…ì‹œ\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state: [state_dim] í¬ê¸°ì˜ í…ì„œ\n",
        "            legal_actions: ê°€ëŠ¥í•œ í–‰ë™ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: [(\"FINISH_LOAD\",-1), â€¦])\n",
        "            prev_action_idx: ì´ì „ì— ì„ íƒí•œ ë§¤ë‹ˆì € ì•¡ì…˜ ì¸ë±ìŠ¤ ([1]-shape LongTensor)\n",
        "            greedy: Trueë©´ íƒìš•ì ìœ¼ë¡œ(max) ì„ íƒ\n",
        "\n",
        "        Returns:\n",
        "            [greedy=False] (action_tensor, logp, ent, value, mask)\n",
        "            - action_tensor (Tensor): í•™ìŠµìš© ì•¡ì…˜ í…ì„œ ([1]-shape LongTensor)\n",
        "            - logp (Tensor): ì„ íƒ í™•ë¥ ì˜ ë¡œê·¸ê°’\n",
        "            - ent (Tensor): ì„ íƒ ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼\n",
        "            - value (Tensor): ìƒíƒœ ê°€ì¹˜ ì¶”ì • (ìŠ¤ì¹¼ë¼)\n",
        "            - mask (Tensor): PPO ì—…ë°ì´íŠ¸ì— ì‚¬ìš©í•  ìƒíƒœ ì˜ì¡´ì  ë§ˆìŠ¤í¬\n",
        "\n",
        "            [greedy=True] (action_idx, None, None, None, None)\n",
        "            - action_idx (int): ì„ íƒëœ ë§¤ë‹ˆì € ì•¡ì…˜ ì¸ë±ìŠ¤ (0~4)\n",
        "        \"\"\"\n",
        "        # 1) í‰ê°€ ëª¨ë“œ\n",
        "        self.network.eval()\n",
        "        with torch.no_grad():\n",
        "            # 2) ë°°ì¹˜ ì°¨ì› ì¶”ê°€: [state_dim] â†’ [1, state_dim]\n",
        "            batch_state = state.unsqueeze(0).to(self.device)\n",
        "\n",
        "            # 3) ë‹¬ì„± ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜ ì´ë¯¸ ë‹¬ì„±ëœ ëª©í‘œ ë¡œì§“ ë§ˆìŠ¤í‚¹\n",
        "            mask = self._build_action_mask()[0]          # [action_dim]\n",
        "\n",
        "            # 4) ë„¤íŠ¸ì›Œí¬ í˜¸ì¶œ: logits [1,action_dim], value [1,]\n",
        "            logits, value = self.network(batch_state, prev_action_idx)\n",
        "            logits = logits[0] + mask                    # [action_dim]\n",
        "\n",
        "            # 5) legal_actions ì— ë”°ë¼ ê°€ëŠ¥í•œ ëª©í‘œë§Œ ë‚¨ê¸°ê¸°\n",
        "            allowed = {act for act, _ in legal_actions}\n",
        "            type_mask = torch.full_like(logits, -1e9)\n",
        "            for idx, t_str in self.idx_to_type.items():\n",
        "                if t_str in allowed:\n",
        "                    type_mask[idx] = 0.0\n",
        "            masked_logits = logits + type_mask\n",
        "\n",
        "            dist = Categorical(logits=masked_logits)\n",
        "\n",
        "            # [í•µì‹¬ ìˆ˜ì •] greedy(í‰ê°€) ëª¨ë“œì™€ í•™ìŠµ ëª¨ë“œì˜ ë°˜í™˜ ê°’ì„ ë¶„ë¦¬í•˜ì—¬ ëª…í™•íˆ í•¨\n",
        "            if greedy:\n",
        "                type_idx = torch.argmax(masked_logits)\n",
        "                self.network.train() # ëª¨ë“œ ë³µê·€\n",
        "                return type_idx.item(), None, None, None, None\n",
        "\n",
        "            # í•™ìŠµ ëª¨ë“œ\n",
        "            type_idx = dist.sample() # 0-dim í…ì„œ\n",
        "            logp     = dist.log_prob(type_idx)\n",
        "            ent      = dist.entropy()\n",
        "\n",
        "            # PPO ì €ì¥ì„ ìœ„í•´ (1,) í˜•íƒœë¡œ ë³€í™˜\n",
        "            action_tensor = type_idx.unsqueeze(0)\n",
        "\n",
        "        # 7) í•™ìŠµ ëª¨ë“œ ë³µê·€\n",
        "        self.network.train()\n",
        "\n",
        "        # [í•µì‹¬ ìˆ˜ì •] í•™ìŠµì— í•„ìš”í•œ ëª¨ë“  ê°’ì„ ì˜¬ë°”ë¥¸ ìˆœì„œì™€ íƒ€ì…ìœ¼ë¡œ ë°˜í™˜\n",
        "        return action_tensor, logp, ent, value[0], mask\n",
        "\n",
        "\n",
        "    def update(self, storage: PPOStorage) -> dict:\n",
        "        \"\"\"ì €ì¥ëœ ê²½í—˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ PPO ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\"\"\"\n",
        "        # ë§ˆì§€ë§‰ ìƒíƒœì˜ ê°€ì¹˜ ê³„ì‚°\n",
        "        with torch.no_grad():\n",
        "            # ë§ˆì§€ë§‰ ìƒíƒœëŠ” storage.obsì— ì €ì¥ë˜ì–´ ìˆì§€ë§Œ, ì´ì „ ì•¡ì…˜ì´ í•„ìš”í•¨\n",
        "            # ì´ ë¡œì§ì€ ê°„ë‹¨í•˜ê²Œ ë§ˆì§€ë§‰ obsì™€ ë§ˆì§€ë§‰ actionì„ ê°€ì ¸ì™€ì„œ ì²˜ë¦¬í•´ì•¼ í•¨\n",
        "            last_obs = storage.obs[-1].unsqueeze(0).to(self.device)\n",
        "            last_prev_action = storage.actions[-2] if storage.step > 1 else torch.zeros(1, dtype=torch.long, device=self.device)\n",
        "\n",
        "            _, last_value = self.network(last_obs, last_prev_action)\n",
        "\n",
        "\n",
        "        # GAEì™€ Return ê³„ì‚°\n",
        "        storage.compute_returns_and_advantages(last_value, self.config.MANAGER_GAMMA, self.config.PPO_GAE_LAMBDA)\n",
        "\n",
        "        # í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„\n",
        "        # obsëŠ” PPOStorageì— tensorë¡œ ì €ì¥ë˜ë„ë¡ ìˆ˜ì •ë˜ì—ˆìŒì„ ê°€ì •\n",
        "        b_obs = torch.stack(list(storage.obs)).to(self.device)\n",
        "        b_actions = storage.actions.squeeze(-1).to(self.device)\n",
        "        b_logprobs = storage.logprobs.to(self.device)\n",
        "        b_returns = storage.returns.to(self.device)\n",
        "        b_advantages = storage.advantages.to(self.device)\n",
        "        b_masks = storage.masks.to(self.device)\n",
        "\n",
        "        # ì´ì „ ì•¡ì…˜(b_prev_actions) ë°°ì¹˜ ìƒì„±\n",
        "        b_prev_actions = torch.cat(\n",
        "            (torch.zeros(1, dtype=torch.long, device=self.device), b_actions[:-1]),\n",
        "            dim=0\n",
        "        )\n",
        "\n",
        "        batch_size = storage.step\n",
        "        minibatch_size = max(1, batch_size // self.config.PPO_NUM_MINIBATCHES)\n",
        "\n",
        "        # PPO ì—…ë°ì´íŠ¸ ë£¨í”„\n",
        "        for _ in range(self.config.PPO_UPDATE_EPOCHS):\n",
        "            perm_indices = np.random.permutation(batch_size)\n",
        "            for start in range(0, batch_size, minibatch_size):\n",
        "                end = start + minibatch_size\n",
        "                mb_idx = perm_indices[start:end]\n",
        "\n",
        "                # ë¯¸ë‹ˆë°°ì¹˜ ë°ì´í„° ìƒì„±\n",
        "                mb_states = b_obs[mb_idx]\n",
        "                mb_actions = b_actions[mb_idx]\n",
        "                mb_logprobs = b_logprobs[mb_idx]\n",
        "                mb_returns = b_returns[mb_idx]\n",
        "                mb_advantages = b_advantages[mb_idx]\n",
        "                mb_masks = b_masks[mb_idx]\n",
        "                mb_prev_actions = b_prev_actions[mb_idx] # ì´ì „ ì•¡ì…˜ ë¯¸ë‹ˆë°°ì¹˜\n",
        "\n",
        "                # ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ìƒˆë¡œìš´ ë¡œê·¸í™•ë¥ , ê°€ì¹˜, ì—”íŠ¸ë¡œí”¼ ê³„ì‚°\n",
        "                logits, new_values = self.network(mb_states, mb_prev_actions)\n",
        "\n",
        "                # ì €ì¥ëœ ë§ˆìŠ¤í¬ë¥¼ ì ìš©í•˜ì—¬ í™•ë¥  ë¶„í¬ ì¬ê³„ì‚°\n",
        "                final_logits = logits + mb_masks\n",
        "                dist = Categorical(logits=final_logits)\n",
        "                new_logprobs = dist.log_prob(mb_actions)\n",
        "                entropy = dist.entropy()\n",
        "\n",
        "                # PPO ì†ì‹¤ ê³„ì‚°\n",
        "                log_ratio = new_logprobs - mb_logprobs\n",
        "                ratio = log_ratio.exp()\n",
        "\n",
        "                surr1 = mb_advantages * ratio\n",
        "                surr2 = mb_advantages * torch.clamp(ratio, 1 - self.config.PPO_CLIP_COEF, 1 + self.config.PPO_CLIP_COEF)\n",
        "\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "                value_loss = F.mse_loss(new_values, mb_returns)\n",
        "\n",
        "                loss = (policy_loss +\n",
        "                        self.config.PPO_VALUE_COEF * value_loss -\n",
        "                        self.config.MANAGER_ENTROPY_COEF * entropy.mean())\n",
        "\n",
        "                # ì—­ì „íŒŒ ë° ìµœì í™”\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.network.parameters(), self.config.PPO_MAX_GRAD_NORM)\n",
        "                self.optimizer.step()\n",
        "\n",
        "        return {\n",
        "            \"policy_loss\": policy_loss.item(),\n",
        "            \"value_loss\": value_loss.item(),\n",
        "            \"entropy\": entropy.mean().item()\n",
        "        }\n",
        "\n",
        "class WorkerAgent:\n",
        "    \"\"\"í•˜ìœ„ ë ˆë²¨ì˜ ì„¸ë¶€ í–‰ë™ì„ ê²°ì •í•˜ëŠ” Worker ì—ì´ì „íŠ¸.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        node_feature_size: int,\n",
        "        global_feature_size: int,\n",
        "        max_cars: int,\n",
        "        max_nodes: int,\n",
        "        config: Config,\n",
        "        device: torch.device\n",
        "    ):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        # v11 ì‹œê·¸ë‹ˆì²˜ì— ë§ì¶° Config ê°ì²´ë§Œ ë„˜ê¸°ë„ë¡ ìˆ˜ì •\n",
        "        self.network = WorkerNetwork(\n",
        "            node_feature_size,\n",
        "            global_feature_size,\n",
        "            max_cars,\n",
        "            max_nodes,\n",
        "            config\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.network.parameters(),\n",
        "            lr=config.WORKER_LR,\n",
        "            eps=1e-5\n",
        "        )\n",
        "\n",
        "        # í–‰ë™ íƒ€ì… â†” ì¸ë±ìŠ¤ ë§¤í•‘\n",
        "        self.type_to_idx = {'LOAD': 0, 'UNLOAD': 1, 'RELOCATE_INTERNAL': 2}\n",
        "        self.idx_to_type = {v: k for k, v in self.type_to_idx.items()}\n",
        "\n",
        "\n",
        "    def get_action_and_value(\n",
        "                              self,\n",
        "                              batch_graph: Batch,           # â† Dataê°€ ì•„ë‹ˆë¼ pre-batched ê·¸ë˜í”„ë¥¼ ë°›ìŠµë‹ˆë‹¤\n",
        "                              legal_actions: list,\n",
        "                              goal_embedding: torch.Tensor,\n",
        "                              greedy: bool = False\n",
        "                            ) -> tuple:\n",
        "\n",
        "        \"\"\"í˜„ì¬ ìƒíƒœì™€ ëª©í‘œì— ë”°ë¼ ì„¸ë¶€ í–‰ë™ì„ ê²°ì •í•©ë‹ˆë‹¤.\"\"\"\n",
        "        self.network.eval() # í‰ê°€ ëª¨ë“œ\n",
        "        with torch.no_grad():\n",
        "            type_logits, load_logits, unload_logits, relocate_logits, value, _ = self.network(batch_graph, goal_embedding)\n",
        "            type_logits, load_logits, unload_logits, relocate_logits = type_logits[0], load_logits[0], unload_logits[0], relocate_logits[0]\n",
        "\n",
        "            # --- í•©ë²•ì ì¸ í–‰ë™(Legal Action)ì— ëŒ€í•œ ë§ˆìŠ¤í‚¹ ---\n",
        "            # ê° í–‰ë™ íƒ€ì…ê³¼ ì°¨ëŸ‰ IDì— ëŒ€í•´ ê°€ëŠ¥í•œ í–‰ë™ë§Œ ë‚¨ê¹ë‹ˆë‹¤.\n",
        "            allowed_types_str = {act for act, _ in legal_actions}\n",
        "            type_mask = torch.full_like(type_logits, -1e9)\n",
        "            for idx, t_str in self.idx_to_type.items():\n",
        "                if t_str in allowed_types_str:\n",
        "                    type_mask[idx] = 0.0\n",
        "\n",
        "            allowed_load_ids = {aid for (act, aid) in legal_actions if act == 'LOAD'}\n",
        "            load_mask = torch.full_like(load_logits, -1e9)\n",
        "            for idx in allowed_load_ids: load_mask[idx] = 0.0\n",
        "\n",
        "            allowed_unload_ids = {aid for (act, aid) in legal_actions if act == 'UNLOAD'}\n",
        "            unload_mask = torch.full_like(unload_logits, -1e9)\n",
        "            for idx in allowed_unload_ids: unload_mask[idx] = 0.0\n",
        "\n",
        "            allowed_relocate_ids = {aid for (act, aid) in legal_actions if act == 'RELOCATE_INTERNAL'}\n",
        "            relocate_mask = torch.full_like(relocate_logits, -1e9)\n",
        "            for idx in allowed_relocate_ids: relocate_mask[idx] = 0.0\n",
        "\n",
        "            # ë§ˆìŠ¤í¬ ì ìš©\n",
        "            masked_type_logits = type_logits + type_mask\n",
        "\n",
        "            # --- ê³„ì¸µì  ìƒ˜í”Œë§ (Hierarchical Sampling) ---\n",
        "            # 1. í–‰ë™ íƒ€ì… ê²°ì •\n",
        "            type_dist = Categorical(logits=masked_type_logits)\n",
        "            type_idx = torch.argmax(masked_type_logits) if greedy else type_dist.sample()\n",
        "            type_str = self.idx_to_type[int(type_idx.item())]\n",
        "\n",
        "            # 2. ê²°ì •ëœ íƒ€ì…ì— ë”°ë¼ ì°¨ëŸ‰ ID ê²°ì •\n",
        "            car_idx_tensor = torch.tensor(-1, device=self.device, dtype=torch.long)\n",
        "            car_dist = None\n",
        "            if type_str == 'LOAD':\n",
        "                masked_logits = load_logits + load_mask\n",
        "                car_dist = Categorical(logits=masked_logits)\n",
        "                car_idx_tensor = torch.argmax(masked_logits) if greedy else car_dist.sample()\n",
        "            elif type_str == 'UNLOAD':\n",
        "                masked_logits = unload_logits + unload_mask\n",
        "                car_dist = Categorical(logits=masked_logits)\n",
        "                car_idx_tensor = torch.argmax(masked_logits) if greedy else car_dist.sample()\n",
        "            elif type_str == 'RELOCATE_INTERNAL':\n",
        "                masked_logits = relocate_logits + relocate_mask\n",
        "                car_dist = Categorical(logits=masked_logits)\n",
        "                car_idx_tensor = torch.argmax(masked_logits) if greedy else car_dist.sample()\n",
        "\n",
        "            action_tuple = (type_str, int(car_idx_tensor.item()))\n",
        "\n",
        "            # í•™ìŠµ ëª¨ë“œì¼ ë•Œë§Œ ë¡œê·¸ í™•ë¥ ê³¼ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°\n",
        "            if greedy:\n",
        "                return action_tuple, None, None, None, value.squeeze(0)\n",
        "\n",
        "            action_tensor = torch.tensor([type_idx.item(), car_idx_tensor.item()], device=self.device)\n",
        "            log_prob_type = type_dist.log_prob(type_idx)\n",
        "            ent_type = type_dist.entropy()\n",
        "\n",
        "            log_prob_car = torch.tensor(0.0, device=self.device)\n",
        "            ent_car = torch.tensor(0.0, device=self.device)\n",
        "            if car_dist is not None:\n",
        "                log_prob_car = car_dist.log_prob(car_idx_tensor)\n",
        "                ent_car = car_dist.entropy()\n",
        "\n",
        "            total_log_prob = log_prob_type + log_prob_car\n",
        "            total_entropy = ent_type + ent_car\n",
        "\n",
        "        self.network.train() # í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜\n",
        "        return action_tuple, action_tensor, total_log_prob, total_entropy, value.squeeze(0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _traj_success(traj):\n",
        "        meta = getattr(traj[-1][0], \"meta\", {})\n",
        "        return meta.get(\"delivered\", -1) == meta.get(\"total\", -2)\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_one(path):\n",
        "        with open(path, \"rb\") as f:\n",
        "            t = pickle.load(f)\n",
        "        if WorkerAgent._traj_success(t):\n",
        "            return t\n",
        "        logging.info(f\"  Â· drop FAILED traj  â†’ {os.path.basename(path)}\")\n",
        "        return []\n",
        "\n",
        "    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    #  (WorkerAgent ë©”ì†Œë“œ)  â”€  Expert-pkl ëª¨ë°© í•™ìŠµ ë£¨í‹´\n",
        "    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    def pretrain_with_imitation(\n",
        "        self,\n",
        "        expert_data_paths: list[str],\n",
        "        epochs: int,\n",
        "        lr: float,\n",
        "        batch_size: int,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        ì „ë¬¸ê°€(pkl) ê¶¤ì ì„ ì´ìš©í•´ Worker ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ì „ í•™ìŠµí•œë‹¤.\n",
        "        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        â–¸ expert_data_paths : '*.pkl' íŒŒì¼ ëª©ë¡\n",
        "        â–¸ ê° pkl = [(state : Data, action : (str, int)), â€¦]\n",
        "        \"\"\"\n",
        "        logging.info(\"[Phase 1] Starting Imitation Learning for Worker Agentâ€¦\")\n",
        "\n",
        "        # 1) ê¶¤ì  ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        expert_pairs: list[tuple[Data, tuple[str, int]]] = []\n",
        "        valid_types = set(self.type_to_idx.keys())      # {'LOAD', â€¦}\n",
        "\n",
        "        for path in expert_data_paths:\n",
        "            if not os.path.exists(path):\n",
        "                logging.warning(f\"  Â· Not found â†’ {path}\")\n",
        "                continue\n",
        "            with open(path, \"rb\") as f:\n",
        "                traj = pickle.load(f)\n",
        "            expert_pairs.extend( (s, a) for s, a in traj if a[0] in valid_types )\n",
        "\n",
        "        if not expert_pairs:\n",
        "            logging.warning(\"  Â· No usable expert samples â†’ skip.\")\n",
        "            return\n",
        "\n",
        "        max_n = getattr(self.config, \"MAX_EXPERT_SAMPLES\", None)\n",
        "        if max_n and len(expert_pairs) > max_n:\n",
        "            random.shuffle(expert_pairs)\n",
        "            expert_pairs = expert_pairs[:max_n]\n",
        "        logging.info(f\"  Â· Total Samples: {len(expert_pairs):,}\")\n",
        "\n",
        "        # 2) ì˜µí‹°ë§ˆì´ì € & í•™ìŠµ ë£¨í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        optim_ = optim.Adam(self.network.parameters(), lr=lr)\n",
        "        self.network.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            random.shuffle(expert_pairs)\n",
        "            total_loss, nb = 0.0, 0\n",
        "\n",
        "            for idx in range(0, len(expert_pairs), batch_size):\n",
        "                batch = expert_pairs[idx: idx + batch_size]\n",
        "                if not batch: continue\n",
        "\n",
        "                states, acts = zip(*batch)\n",
        "                g = Batch.from_data_list(states).to(self.device)\n",
        "\n",
        "                # action â†’ í…ì„œ\n",
        "                a_types = torch.as_tensor(\n",
        "                    [self.type_to_idx[a[0]] for a in acts],\n",
        "                    device=self.device, dtype=torch.long)\n",
        "                a_cars  = torch.as_tensor(\n",
        "                    [a[1] for a in acts],\n",
        "                    device=self.device, dtype=torch.long)\n",
        "\n",
        "                dummy_goal = torch.zeros(\n",
        "                    g.num_graphs, self.config.GOAL_EMBED_DIM,\n",
        "                    device=self.device)\n",
        "\n",
        "                # â”€â”€ forward & loss â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "                with torch.set_grad_enabled(True):\n",
        "                    t_logit, l_logit, u_logit, r_logit, _v, _ = \\\n",
        "                        self.network(g, dummy_goal)\n",
        "\n",
        "                    loss_type = F.cross_entropy(t_logit, a_types)\n",
        "\n",
        "                    loss_load   = F.cross_entropy(l_logit, a_cars, reduction=\"none\")\n",
        "                    loss_unload = F.cross_entropy(u_logit, a_cars, reduction=\"none\")\n",
        "                    loss_reloc  = F.cross_entropy(r_logit, a_cars, reduction=\"none\")\n",
        "\n",
        "                    m_load     = (a_types == self.type_to_idx['LOAD'])\n",
        "                    m_unload   = (a_types == self.type_to_idx['UNLOAD'])\n",
        "                    m_reloc    = (a_types == self.type_to_idx['RELOCATE_INTERNAL'])\n",
        "\n",
        "                    loss_car = loss_type.new_zeros(1)  # grad X í…ì„œ\n",
        "                    if m_load.any().item():\n",
        "                        loss_car = loss_car + loss_load[m_load].mean()\n",
        "                    if m_unload.any().item():\n",
        "                        loss_car = loss_car + loss_unload[m_unload].mean()\n",
        "                    if m_reloc.any().item():\n",
        "                        loss_car = loss_car + loss_reloc[m_reloc].mean()\n",
        "\n",
        "                    loss = loss_type + loss_car\n",
        "\n",
        "                    # backward\n",
        "                    optim_.zero_grad()\n",
        "                    loss.backward()\n",
        "                    nn.utils.clip_grad_norm_(self.network.parameters(),\n",
        "                                             self.config.PPO_MAX_GRAD_NORM)\n",
        "                    optim_.step()\n",
        "\n",
        "                total_loss += loss.item(); nb += 1\n",
        "\n",
        "            if nb and (epoch + 1) % 10 == 0:\n",
        "                logging.info(f\"    Epoch {epoch+1:4d}/{epochs}  \"\n",
        "                             f\"avg-loss {total_loss/nb:.4f}\")\n",
        "\n",
        "        logging.info(\"[Phase 1] Imitation Learning Finished.\")\n",
        "\n",
        "\n",
        "    def evaluate_actions(self, states: list[Data], actions: torch.Tensor, goal_embedding: torch.Tensor) -> tuple:\n",
        "      batch_data = Batch.from_data_list(states).to(self.device)\n",
        "\n",
        "      \"\"\"\n",
        "      PPO ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´, ì£¼ì–´ì§„ ìƒíƒœ(states)ì—ì„œ íŠ¹ì • í–‰ë™(actions)ì„ í–ˆì„ ë•Œì˜\n",
        "      ë¡œê·¸ í™•ë¥ (log_prob), ì—”íŠ¸ë¡œí”¼(entropy), ê°€ì¹˜(value)ë¥¼ ë‹¤ì‹œ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "\n",
        "      Args:\n",
        "          states (list[Data]): ìƒíƒœ(ê·¸ë˜í”„) ë°ì´í„°ì˜ ë¦¬ìŠ¤íŠ¸.\n",
        "          actions (torch.Tensor): [ì•¡ì…˜ íƒ€ì…, ì°¨ëŸ‰ ID] í˜•íƒœì˜ í–‰ë™ í…ì„œ.\n",
        "          goal_embedding (torch.Tensor): í˜„ì¬ ëª©í‘œì— ëŒ€í•œ ì„ë² ë”© í…ì„œ.\n",
        "\n",
        "      Returns:\n",
        "          tuple: (ë¡œê·¸ í™•ë¥ , ì—”íŠ¸ë¡œí”¼, ê°€ì¹˜) í…ì„œ.\n",
        "      \"\"\"\n",
        "      type_logits, load_logits, unload_logits, relocate_logits, values, graph_emb = \\\n",
        "          self.network(batch_data, goal_embedding)\n",
        "\n",
        "      action_types = actions[:, 0]\n",
        "      action_cars  = actions[:, 1]\n",
        "\n",
        "      # ê° í–‰ë™ í—¤ë“œì— ëŒ€í•œ í™•ë¥  ë¶„í¬ ìƒì„±\n",
        "      type_dist      = Categorical(logits=type_logits)\n",
        "      load_dist      = Categorical(logits=load_logits)\n",
        "      unload_dist    = Categorical(logits=unload_logits)\n",
        "      relocate_dist  = Categorical(logits=relocate_logits)\n",
        "\n",
        "      # í–‰ë™ íƒ€ì… ë¡œê·¸ í™•ë¥ \n",
        "      log_probs_type = type_dist.log_prob(action_types)\n",
        "\n",
        "      # í•˜ìœ„ í–‰ë™ ë¡œê·¸ í™•ë¥  ë¯¸ë¦¬ ê³„ì‚°\n",
        "      log_probs_load     = load_dist.log_prob(action_cars)\n",
        "      log_probs_unload   = unload_dist.log_prob(action_cars)\n",
        "      log_probs_relocate = relocate_dist.log_prob(action_cars)\n",
        "\n",
        "      # ì‹¤ì œ ì·¨í•œ íƒ€ì…ì— í•´ë‹¹í•˜ëŠ” í•˜ìœ„ í–‰ë™ í™•ë¥ ë§Œ ì„ íƒ\n",
        "      mask_load     = (action_types == self.type_to_idx['LOAD']).float()\n",
        "      mask_unload   = (action_types == self.type_to_idx['UNLOAD']).float()\n",
        "      mask_relocate = (action_types == self.type_to_idx['RELOCATE_INTERNAL']).float()\n",
        "\n",
        "      log_probs = log_probs_type \\\n",
        "                  + log_probs_load     * mask_load \\\n",
        "                  + log_probs_unload   * mask_unload \\\n",
        "                  + log_probs_relocate * mask_relocate\n",
        "\n",
        "      # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°\n",
        "      type_probs = F.softmax(type_logits, dim=-1)\n",
        "      entropy = (\n",
        "          type_dist.entropy()\n",
        "          + type_probs[:, self.type_to_idx['LOAD']]     * load_dist.entropy()\n",
        "          + type_probs[:, self.type_to_idx['UNLOAD']]   * unload_dist.entropy()\n",
        "          + type_probs[:, self.type_to_idx['RELOCATE_INTERNAL']] * relocate_dist.entropy()\n",
        "      )\n",
        "\n",
        "      return log_probs, entropy, values\n",
        "\n",
        "    def update(self, storage: PPOStorage, goal_embedding: torch.Tensor) -> dict:\n",
        "        \"\"\"ì €ì¥ëœ Workerì˜ ê²½í—˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ PPO ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\"\"\"\n",
        "        self.network.train()\n",
        "\n",
        "        # 1) ë§ˆì§€ë§‰ ìƒíƒœë¥¼ ë°°ì¹˜ë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.\n",
        "        last_state = storage.obs[storage.step - 1]\n",
        "        last_state_batch = Batch.from_data_list([last_state]).to(self.device)\n",
        "        # ë§ˆì§€ë§‰ ìƒíƒœì˜ ê°€ì¹˜ ê³„ì‚°\n",
        "        with torch.no_grad():\n",
        "            # network ë°˜í™˜: (type_logits, load_logits, unload_logits, relocate_logits, critic_value, graph_emb)\n",
        "            _, _, _, _, critic_value, _ = self.network(last_state_batch, goal_embedding)\n",
        "            last_value = critic_value.squeeze(0)\n",
        "\n",
        "\n",
        "\n",
        "        # GAEì™€ Return ê³„ì‚°\n",
        "        storage.compute_returns_and_advantages(last_value, self.config.WORKER_GAMMA, self.config.PPO_GAE_LAMBDA)\n",
        "\n",
        "        # í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (ë¦¬ìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ë‘ê³ , í…ì„œëŠ” deviceë¡œ ì´ë™)\n",
        "        b_obs = storage.obs # ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ deviceë¡œ ì˜®ê¸°ì§€ ì•ŠìŒ\n",
        "        b_actions = storage.actions.to(self.device)\n",
        "        b_logprobs = storage.logprobs.to(self.device)\n",
        "        b_returns = storage.returns.to(self.device)\n",
        "        b_advantages = storage.advantages.to(self.device)\n",
        "\n",
        "        batch_size = storage.step\n",
        "        if batch_size == 0: return {}\n",
        "        minibatch_size = max(1, batch_size // self.config.PPO_NUM_MINIBATCHES)\n",
        "\n",
        "        # PPO ì—…ë°ì´íŠ¸ ë£¨í”„\n",
        "        for _ in range(self.config.PPO_UPDATE_EPOCHS):\n",
        "            perm_indices = np.random.permutation(batch_size)\n",
        "            for start in range(0, batch_size, minibatch_size):\n",
        "                end = start + minibatch_size\n",
        "                mb_idx = perm_indices[start:end]\n",
        "\n",
        "                # ë¯¸ë‹ˆë°°ì¹˜ ë°ì´í„° ìƒì„±\n",
        "                mb_states = [b_obs[i] for i in mb_idx]\n",
        "                mb_actions = b_actions[mb_idx]\n",
        "                mb_logprobs = b_logprobs[mb_idx]\n",
        "                mb_returns = b_returns[mb_idx]\n",
        "                mb_advantages = b_advantages[mb_idx]\n",
        "\n",
        "                new_logprobs, entropy, new_values = self.evaluate_actions(mb_states, mb_actions, goal_embedding)\n",
        "\n",
        "                # PPO ì†ì‹¤ ê³„ì‚°\n",
        "                log_ratio = new_logprobs - mb_logprobs\n",
        "                ratio = log_ratio.exp()\n",
        "\n",
        "                surr1 = mb_advantages * ratio\n",
        "                surr2 = mb_advantages * torch.clamp(ratio, 1 - self.config.PPO_CLIP_COEF, 1 + self.config.PPO_CLIP_COEF)\n",
        "\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "                value_loss = F.mse_loss(new_values, mb_returns)\n",
        "\n",
        "                loss = (policy_loss +\n",
        "                        self.config.PPO_VALUE_COEF * value_loss -\n",
        "                        self.config.WORKER_ENTROPY_COEF * entropy.mean())\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.network.parameters(), self.config.PPO_MAX_GRAD_NORM)\n",
        "                self.optimizer.step()\n",
        "\n",
        "        # í•™ìŠµ ë¡œê·¸ë¥¼ ìœ„í•´ ì†ì‹¤ ê°’ ë°˜í™˜\n",
        "        return {\n",
        "            \"policy_loss\": policy_loss.item(),\n",
        "            \"value_loss\": value_loss.item(),\n",
        "            \"entropy\": entropy.mean().item()\n",
        "        }\n",
        "\n",
        "# ==============================================================================\n",
        "# ì„¹ì…˜ 5: í‰ê°€ ë° ë©”ì¸ ë£¨í”„ (ë¦¬íŒ©í† ë§ ë²„ì „)\n",
        "# ==============================================================================\n",
        "import time\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def evaluate_agent(manager_agent: ManagerAgent, worker_agent: WorkerAgent, problems: list[dict], max_num_ports: int, config: Config):\n",
        "    \"\"\"\n",
        "    í˜„ì¬ ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ , ë‹¤ì–‘í•œ ì§€í‘œë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    logging.info(\"=\" * 20 + \" AGENT EVALUATION START \" + \"=\" * 20)\n",
        "    manager_agent.network.eval()\n",
        "    worker_agent.network.eval()\n",
        "\n",
        "    total_success_count = 0\n",
        "    total_costs = []\n",
        "    total_relocations = []\n",
        "    manager_action_counts = collections.defaultdict(int)\n",
        "\n",
        "    original_env_wrapper = manager_agent.env_wrapper\n",
        "    MAX_EVAL_MANAGER_STEPS = 4000\n",
        "\n",
        "    # ğŸ’¡ --- [í•µì‹¬] í™˜ê²½ ê°ì²´ë¥¼ ë£¨í”„ ë°–ì—ì„œ í•œ ë²ˆë§Œ ìƒì„± --- ğŸ’¡\n",
        "    # ì²« ë²ˆì§¸ ë¬¸ì œë¡œ h_envë¥¼ ì´ˆê¸°í™”í•˜ê³ , ì´í›„ì—ëŠ” resetìœ¼ë¡œ ì¬ì‚¬ìš©\n",
        "    prob = random.choice(problems)\n",
        "    h_env = HierarchicalEnvWrapper(prob, max_num_ports, worker_agent, config)\n",
        "    manager_agent.env_wrapper = h_env\n",
        "\n",
        "    for episode_num in range(config.EVAL_EPISODES):\n",
        "        # ğŸ’¡ [ìˆ˜ì •] ë§¤ë²ˆ ìƒˆë¡œ ìƒì„±í•˜ëŠ” ëŒ€ì‹ , reset ë©”ì„œë“œë¡œ ë¬¸ì œë§Œ êµì²´\n",
        "        if episode_num > 0:\n",
        "            prob = random.choice(problems)\n",
        "            # h_env.reset()ì€ manager_stateë¥¼ ë°˜í™˜í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì§ì ‘ ìƒíƒœë¥¼ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤.\n",
        "            h_env.reset(prob_data=prob)\n",
        "            manager_state = h_env._get_manager_state()\n",
        "        else:\n",
        "            # ì²« ì—í”¼ì†Œë“œëŠ” ì´ë¯¸ h_env ìƒì„± ì‹œ ë¦¬ì…‹ë¨\n",
        "            manager_state = h_env._get_manager_state()\n",
        "\n",
        "        if hasattr(manager_agent, 'prev_action_idx'):\n",
        "             manager_agent.prev_action_idx.zero_()\n",
        "\n",
        "        overall_done = False\n",
        "        episode_cost = 0.0\n",
        "        current_episode_steps = 0\n",
        "\n",
        "        while not overall_done:\n",
        "            if current_episode_steps > MAX_EVAL_MANAGER_STEPS:\n",
        "                logging.warning(f\"\\nEval Episode [{episode_num+1}] reached max steps limit. Breaking loop.\")\n",
        "                break\n",
        "\n",
        "            with torch.no_grad():\n",
        "                legal_actions = h_env.ship_env.get_legal_actions(for_worker=False)\n",
        "                prev_action_idx = manager_agent.prev_action_idx if hasattr(manager_agent, 'prev_action_idx') else torch.zeros(1, dtype=torch.long, device=manager_agent.device)\n",
        "\n",
        "                manager_action_idx, _, _, _, _ = manager_agent.get_action_and_value(\n",
        "                    manager_state.to(manager_agent.device), legal_actions, prev_action_idx, greedy=True\n",
        "                )\n",
        "\n",
        "            if hasattr(manager_agent, 'prev_action_idx'):\n",
        "                manager_agent.prev_action_idx = torch.tensor(\n",
        "                    [manager_action_idx], dtype=torch.long, device=manager_agent.device\n",
        "                )\n",
        "\n",
        "            manager_action_counts[h_env.manager_action_map[manager_action_idx]] += 1\n",
        "            current_goal_str = h_env.manager_action_map[manager_action_idx]\n",
        "            print(f\"\\r  Eval Ep[{episode_num+1}/{config.EVAL_EPISODES}] Step[{current_episode_steps+1}]: Trying Goal -> {current_goal_str.ljust(25)}\", end=\"\")\n",
        "\n",
        "            manager_state, _, overall_done, info = h_env.step(\n",
        "                manager_action_idx, greedy_worker=True\n",
        "            )\n",
        "            episode_cost += info.get('cost', 0.0)\n",
        "            current_episode_steps += 1\n",
        "\n",
        "        print()\n",
        "\n",
        "        if len(h_env.ship_env.delivered_cars) == h_env.ship_env.total_cars:\n",
        "            total_success_count += 1\n",
        "        total_costs.append(episode_cost)\n",
        "        total_relocations.append(h_env.ship_env.relocations_this_episode)\n",
        "        logging.info(f\"  Eval Episode [{episode_num+1}/{config.EVAL_EPISODES}] Finished. Success: {len(h_env.ship_env.delivered_cars) == h_env.ship_env.total_cars}\")\n",
        "\n",
        "    # í‰ê°€ê°€ ëë‚˜ë©´ ì›ë˜ í™˜ê²½ìœ¼ë¡œ ë³µì›\n",
        "    manager_agent.env_wrapper = original_env_wrapper\n",
        "    manager_agent.network.train()\n",
        "    worker_agent.network.train()\n",
        "\n",
        "    # ìµœì¢… ê²°ê³¼ ê³„ì‚° ë° ë¡œê¹…\n",
        "    success_rate = total_success_count / config.EVAL_EPISODES\n",
        "    avg_cost = np.mean(total_costs) if total_costs else 0.0\n",
        "    avg_relocations = np.mean(total_relocations) if total_relocations else 0.0\n",
        "\n",
        "    logging.info(\"-\" * 54)\n",
        "    logging.info(f\"[EVAL RESULT] Success Rate: {success_rate*100:.1f}%\")\n",
        "    logging.info(f\"[EVAL RESULT] Average Cost: {avg_cost:.2f}\")\n",
        "    logging.info(f\"[EVAL RESULT] Average Relocations: {avg_relocations:.2f}\")\n",
        "\n",
        "    total_actions = sum(manager_action_counts.values())\n",
        "    if total_actions > 0:\n",
        "        logging.info(\"[EVAL RESULT] Manager Action Distribution:\")\n",
        "        for action_idx in sorted(h_env.manager_action_map.keys()):\n",
        "            count = manager_action_counts.get(h_env.manager_action_map[action_idx], 0)\n",
        "            action_str = h_env.manager_action_map[action_idx]\n",
        "            percentage = (count / total_actions) * 100\n",
        "            logging.info(f\"  - {action_str:<25s}: {count} times ({percentage:.1f}%)\")\n",
        "\n",
        "    logging.info(\"=\" * 22 + \" EVALUATION END \" + \"=\" * 22 + \"\\n\")\n",
        "\n",
        "    return {\n",
        "        \"success_rate\": success_rate,\n",
        "        \"avg_cost\": avg_cost,\n",
        "        \"avg_relocations\": avg_relocations\n",
        "    }\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    mp.set_start_method(\"spawn\", force=True) # ë©€í‹° í”„ë¡œì„¸\n",
        "    # --- 1. ì´ˆê¸° ì„¤ì • ë° í™˜ê²½ êµ¬ì„± ---\n",
        "    setup_logger()\n",
        "    config = Config()\n",
        "    writer = SummaryWriter(log_dir=config.LOG_DIR)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    logging.info(f\"Using device: {device}\")\n",
        "\n",
        "    assert torch.cuda.is_available(), \"CUDAê°€ í™œì„±í™”ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤!\"\n",
        "    print(\"âœ… CUDA í™œì„±í™” í™•ì¸\")\n",
        "\n",
        "    all_problem_files = [os.path.join(config.PROBLEM_DIR, f) for f in os.listdir(config.PROBLEM_DIR) if f.endswith('.json')]\n",
        "    easy_problem_files = [f for f in all_problem_files if any(name in f for name in ['prob1', 'prob2', 'prob4'])]\n",
        "    easy_problems = [json.load(open(f)) for f in easy_problem_files if os.path.exists(f)]\n",
        "    all_problems = [json.load(open(f)) for f in all_problem_files if os.path.exists(f)]\n",
        "    if not all_problems:\n",
        "        logging.error(f\"No problem files found in {config.PROBLEM_DIR}. Exiting.\")\n",
        "        exit()\n",
        "    if not easy_problems:\n",
        "        logging.warning(f\"No easy problems found. Starting with all problems.\")\n",
        "        easy_problems = all_problems\n",
        "\n",
        "    max_cars = max(sum(q for _, q in p.get('K', [])) for p in all_problems)\n",
        "    max_num_ports = max(p.get('P', 1) for p in all_problems)\n",
        "    max_nodes = max(p.get('N', 1) for p in all_problems)\n",
        "    node_feature_size = 4\n",
        "    global_feature_size = 3 + max_num_ports\n",
        "\n",
        "    # --- 2. ì—ì´ì „íŠ¸ ë° í™˜ê²½ ì´ˆê¸°í™” ---\n",
        "    worker_agent = WorkerAgent(node_feature_size, global_feature_size, max_cars, max_nodes, config, device)\n",
        "    current_prob = random.choice(easy_problems)\n",
        "    h_env_raw = HierarchicalEnvWrapper(current_prob, max_num_ports, worker_agent, config)\n",
        "\n",
        "    # â†“â†“â†“  ì¶”ê°€ â†“â†“â†“\n",
        "    h_env = EpisodeTracker(h_env_raw, log_dir=config.LOG_DIR)\n",
        "    # â†‘â†‘â†‘  ì¶”ê°€ â†‘â†‘â†‘\n",
        "\n",
        "    manager_agent = ManagerAgent(config, device, h_env)\n",
        "    manager_agent.prev_action_idx = torch.zeros(1, dtype=torch.long, device=device)\n",
        "\n",
        "    # --- 3. Worker ì‚¬ì „ í›ˆë ¨ (ëª¨ë°© í•™ìŠµ) ---\n",
        "    print(\"\\nDEBUG: >>>>>>>>>> STEP 1: Calling pretrain_with_imitation NOW...\")\n",
        "    worker_agent.pretrain_with_imitation(\n",
        "        config.EXPERT_DATA_PATHS, config.IMITATION_LEARNING_EPOCHS, config.IMITATION_LR, config.IMITATION_BATCH_SIZE\n",
        "    )\n",
        "    print(\"DEBUG: <<<<<<<<<< STEP 1: pretrain_with_imitation FINISHED.\\n\")\n",
        "\n",
        "    # --- 4. ê³„ì¸µì  ê°•í™”í•™ìŠµ ë©”ì¸ ë£¨í”„ ---\n",
        "    logging.info(\"\\n[Phase 2] Starting Hierarchical Reinforcement Learning...\")\n",
        "\n",
        "    manager_state = h_env.reset(prob_data=current_prob)\n",
        "    manager_storage = PPOStorage(config.MANAGER_NUM_STEPS_PER_UPDATE, (1,), device,\n",
        "                                 state_shape=(config.MANAGER_STATE_DIM,),\n",
        "                                 manager_action_dim=config.MANAGER_ACTION_DIM)\n",
        "\n",
        "    episode_rewards, episode_costs, episode_manager_steps = 0, 0, 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Interval í†µê³„ë¥¼ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”\n",
        "    interval_rewards = 0.0\n",
        "    interval_goals = defaultdict(int)\n",
        "    interval_successes = 0\n",
        "    interval_costs = 0.0\n",
        "    interval_worker_rewards = 0.0\n",
        "    interval_worker_steps = 0\n",
        "\n",
        "    for manager_step in range(1, config.TOTAL_MANAGER_STEPS + 1):\n",
        "        is_curriculum_phase = manager_step < config.CURRICULUM_STEPS\n",
        "\n",
        "        if is_curriculum_phase:\n",
        "            possible_goals = []\n",
        "            if not h_env._is_goal_achieved('FINISH_UNLOAD'): possible_goals.append(1)\n",
        "            if not h_env._is_goal_achieved('CLEAR_TEMP'): possible_goals.append(3)\n",
        "            if not h_env._is_goal_achieved('FINISH_LOAD'): possible_goals.append(2)\n",
        "            if possible_goals: manager_action_idx = random.choice(possible_goals)\n",
        "            else: manager_action_idx = 4\n",
        "            manager_action_tensor = torch.tensor([manager_action_idx], device=device)\n",
        "            m_log_prob, m_value, m_mask = torch.tensor(0.0), torch.tensor(0.0), None\n",
        "            manager_agent.prev_action_idx = manager_action_tensor.clone()\n",
        "        else:\n",
        "            legal_actions = h_env.ship_env.get_legal_actions(for_worker=False)\n",
        "            prev_idx = manager_agent.prev_action_idx\n",
        "            with torch.no_grad():\n",
        "                manager_action_tensor, m_log_prob, _, m_value, m_mask = manager_agent.get_action_and_value(\n",
        "                    manager_state, legal_actions, prev_idx, greedy=False\n",
        "                )\n",
        "            manager_action_idx = manager_action_tensor.item()\n",
        "            manager_agent.prev_action_idx = manager_action_tensor.clone()\n",
        "\n",
        "        next_manager_state, manager_reward, overall_done, worker_info = h_env.step(manager_action_idx)\n",
        "\n",
        "        episode_rewards += manager_reward\n",
        "        episode_costs += worker_info.get('cost', 0.0)\n",
        "        episode_manager_steps += 1\n",
        "\n",
        "        # ë§¤ ìŠ¤í…ë§ˆë‹¤ Interval í†µê³„ ëˆ„ì \n",
        "        interval_rewards += manager_reward\n",
        "        interval_goals[worker_info.get('goal', 'N/A')] += 1\n",
        "        if worker_info.get('success', False):\n",
        "            interval_successes += 1\n",
        "        interval_costs += worker_info.get('cost', 0.0)\n",
        "        interval_worker_rewards += worker_info.get('worker_total_reward', 0.0) # h_env.stepì—ì„œ ì´ ê°’ì„ ë°˜í™˜í•´ì•¼ í•¨\n",
        "        interval_worker_steps += worker_info.get('steps', 0)\n",
        "\n",
        "        if not is_curriculum_phase:\n",
        "            manager_storage.add(manager_state, manager_action_tensor, m_log_prob, manager_reward, overall_done, m_value, m_mask)\n",
        "\n",
        "        manager_state = next_manager_state\n",
        "\n",
        "        if overall_done:\n",
        "            s = h_env.ship_env\n",
        "            success_ratio = len(s.delivered_cars) / s.total_cars if s.total_cars > 0 else 0\n",
        "            logging.info(f\"EPISODE DONE (M-Step: {manager_step}) | Success: {success_ratio*100:.1f}% | Total Reward: {episode_rewards:.2f} | Total Cost: {episode_costs:.2f} | Length: {episode_manager_steps} steps\")\n",
        "            writer.add_scalar(\"Episode/TotalReward\", episode_rewards, manager_step)\n",
        "            writer.add_scalar(\"Episode/TotalCost\", episode_costs, manager_step)\n",
        "            writer.add_scalar(\"Episode/SuccessRatio\", success_ratio, manager_step)\n",
        "            writer.add_scalar(\"Episode/Length\", episode_manager_steps, manager_step)\n",
        "\n",
        "            if manager_step < config.CURRICULUM_TRANSITION_STEP:\n",
        "                current_prob = random.choice(easy_problems)\n",
        "            else:\n",
        "                if manager_step - episode_manager_steps < config.CURRICULUM_TRANSITION_STEP:\n",
        "                     logging.info(\"=\"*20 + \" SWITCHING TO FULL PROBLEM SET \" + \"=\"*20)\n",
        "                current_prob = random.choice(all_problems)\n",
        "\n",
        "            manager_state = h_env.reset(prob_data=current_prob)\n",
        "            manager_agent.prev_action_idx.zero_()\n",
        "            episode_rewards, episode_costs, episode_manager_steps = 0, 0, 0\n",
        "\n",
        "        if not is_curriculum_phase and manager_storage.is_full():\n",
        "            loss_info = manager_agent.update(manager_storage)\n",
        "            if loss_info:\n",
        "                writer.add_scalar(\"Train/Manager_PolicyLoss\", loss_info[\"policy_loss\"], manager_step)\n",
        "                writer.add_scalar(\"Train/Manager_ValueLoss\", loss_info[\"value_loss\"], manager_step)\n",
        "                writer.add_scalar(\"Train/Manager_Entropy\", loss_info[\"entropy\"], manager_step)\n",
        "            manager_storage.reset()\n",
        "\n",
        "        # ğŸ’¡ --- [ìˆ˜ì •] ì£¼ê¸°ì  ë¡œê·¸ ì¶œë ¥ (ì¤‘ë³µ ì œê±° ë° ìµœì¢… ë²„ì „) --- ğŸ’¡\n",
        "        if manager_step % config.PRINT_INTERVAL_MANAGER_STEPS == 0:\n",
        "            elapsed_time = time.time() - start_time\n",
        "            steps_per_sec = config.PRINT_INTERVAL_MANAGER_STEPS / elapsed_time if elapsed_time > 0 else 0\n",
        "\n",
        "            avg_reward = interval_rewards / config.PRINT_INTERVAL_MANAGER_STEPS\n",
        "            interval_success_rate = (interval_successes / config.PRINT_INTERVAL_MANAGER_STEPS) * 100\n",
        "            avg_cost_per_step = interval_costs / config.PRINT_INTERVAL_MANAGER_STEPS\n",
        "            # ğŸ’¡ [ì¶”ê°€] Worker í‰ê·  ë³´ìƒ ê³„ì‚°\n",
        "            avg_worker_rew = interval_worker_rewards / interval_worker_steps if interval_worker_steps > 0 else 0\n",
        "\n",
        "            goal_dist_str = \", \".join([f\"{k.split('_')[-1]}:{v}\" for k, v in sorted(interval_goals.items())])\n",
        "\n",
        "            logging.info(\n",
        "                f\"M-Step {manager_step:6d} | Avg M-Rew: {avg_reward:7.2f} | \"\n",
        "                f\"W-Rew: {avg_worker_rew:6.3f} | \" # Worker í‰ê·  ë³´ìƒ ì¶œë ¥\n",
        "                f\"Success: {interval_success_rate:3.0f}% | Avg Cost: {avg_cost_per_step:8.1f} | \"\n",
        "                f\"Goals: [{goal_dist_str}] | SPS: {steps_per_sec:.2f}\"\n",
        "            )\n",
        "            goal_dist_str = \", \".join([f\"{k.split('_')[-1]}:{v}\" for k, v in sorted(interval_goals.items())])\n",
        "\n",
        "            logging.info(\n",
        "                f\"M-Step {manager_step:6d} | Avg Rew: {avg_reward:7.2f} | \"\n",
        "                f\"Success: {interval_success_rate:3.0f}% | Avg Cost: {avg_cost_per_step:8.1f} | \"\n",
        "                f\"Goals: [{goal_dist_str}] | SPS: {steps_per_sec:.2f}\"\n",
        "            )\n",
        "\n",
        "            # ë‹¤ìŒ Intervalì„ ìœ„í•´ í†µê³„ ë³€ìˆ˜ ì´ˆê¸°í™”\n",
        "            interval_worker_rewards = 0.0\n",
        "            interval_worker_steps = 0\n",
        "            interval_rewards = 0.0\n",
        "            interval_goals.clear()\n",
        "            interval_successes = 0\n",
        "            interval_costs = 0.0\n",
        "            start_time = time.time()\n",
        "\n",
        "        # ì£¼ê¸°ì  í‰ê°€ ë° ëª¨ë¸ ì €ì¥\n",
        "        if manager_step > 0 and manager_step % config.EVAL_INTERVAL_MANAGER_STEPS == 0:\n",
        "            if manager_step == config.CURRICULUM_STEPS:\n",
        "                logging.info(\"=\"*20 + \" CURRICULUM FINISHED \" + \"=\"*20)\n",
        "\n",
        "            eval_results = evaluate_agent(manager_agent, worker_agent, all_problems, max_num_ports, config)\n",
        "            writer.add_scalar(\"Eval/SuccessRate\", eval_results[\"success_rate\"] * 100.0, manager_step)\n",
        "            writer.add_scalar(\"Eval/AvgCost\", eval_results[\"avg_cost\"], manager_step)\n",
        "            writer.add_scalar(\"Eval/AvgRelocations\", eval_results[\"avg_relocations\"], manager_step)\n",
        "\n",
        "            torch.save(worker_agent.network.state_dict(), os.path.join(config.MODEL_DIR, f\"worker_model_step_{manager_step}.pth\"))\n",
        "            torch.save(manager_agent.network.state_dict(), os.path.join(config.MODEL_DIR, f\"manager_model_step_{manager_step}.pth\"))\n",
        "\n",
        "    writer.close()\n",
        "    logging.info(\"--- V12 Refactored Training Completed ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KWwYc4yKt_-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Wq6CxevywILM"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPjWCPGnImRflQZyFmWHN5k",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10b3083f07594018a2d7083f320c2a1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3fbc29bb1fb40f4b8401d67163debcd",
              "IPY_MODEL_3b18db1c78b74f138500c1cd8a6c64eb",
              "IPY_MODEL_0b8243e20c2f4c19b6a0b6ea9f3ed3bb"
            ],
            "layout": "IPY_MODEL_3984f7b3222d4e459531ec76993d6564"
          }
        },
        "f3fbc29bb1fb40f4b8401d67163debcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_098bddb2790a43eb8130699124e3f0cd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2cece6120ff34048b93a960adcca2f2d",
            "value": "BFS-cache:â€‡100%"
          }
        },
        "3b18db1c78b74f138500c1cd8a6c64eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10af330243c24df6a0f066e2763eb05a",
            "max": 131769,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9da3066d65744e69b34160f904ac73e9",
            "value": 131769
          }
        },
        "0b8243e20c2f4c19b6a0b6ea9f3ed3bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ade220bad3a64b13a525cdaff6852a38",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_140fcfcac8054861b811b901ce6286db",
            "value": "â€‡131769/131769â€‡[02:24&lt;00:00,â€‡907.63pair/s]"
          }
        },
        "3984f7b3222d4e459531ec76993d6564": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "098bddb2790a43eb8130699124e3f0cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cece6120ff34048b93a960adcca2f2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10af330243c24df6a0f066e2763eb05a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9da3066d65744e69b34160f904ac73e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ade220bad3a64b13a525cdaff6852a38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "140fcfcac8054861b811b901ce6286db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70982f5ede254274836c763a2b18fa85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d908a324add4e198054b0e8ec3a8d37",
              "IPY_MODEL_7b90f8811e6b4fcba6ad9a081ae4d776",
              "IPY_MODEL_19acf994b0da47a783444480a9536a62"
            ],
            "layout": "IPY_MODEL_ec8dabdb014746c7b62526b22075a208"
          }
        },
        "6d908a324add4e198054b0e8ec3a8d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9467fb546f794957a628bd9156e1ca9b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_dc984cd885d44bf1a3cdc0a5bb64bf98",
            "value": "Ports:â€‡100%"
          }
        },
        "7b90f8811e6b4fcba6ad9a081ae4d776": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b30db2159c64495bb46eec3e9d73e8d3",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e17892e2a78c4246a042c81235e96160",
            "value": 15
          }
        },
        "19acf994b0da47a783444480a9536a62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49b92639664040c1b0a1099460bbdb91",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e76fe8e4eca94d34a4552d054837e8da",
            "value": "â€‡15/15â€‡[00:00&lt;00:00,â€‡1044.03port/s]"
          }
        },
        "ec8dabdb014746c7b62526b22075a208": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9467fb546f794957a628bd9156e1ca9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc984cd885d44bf1a3cdc0a5bb64bf98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b30db2159c64495bb46eec3e9d73e8d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e17892e2a78c4246a042c81235e96160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49b92639664040c1b0a1099460bbdb91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e76fe8e4eca94d34a4552d054837e8da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}