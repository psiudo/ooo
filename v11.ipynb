{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psiudo/ooo/blob/main/v11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_odh6XsYqQG",
        "outputId": "38962e49-cf7c-4423-96ab-35666904ba7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Google Drive가 성공적으로 마운트되었습니다.\n"
          ]
        }
      ],
      "source": [
        "# ==================================\n",
        "# 셀 1: Google Drive 마운트\n",
        "# ==================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"✅ Google Drive가 성공적으로 마운트되었습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUAVUj5OZBE-",
        "outputId": "d5e122f5-526a-4079-8e8e-c1306627e071",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.6.15)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba) (0.43.0)\n",
            "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.11/dist-packages (from numba) (2.0.2)\n",
            "✅ 필수 라이브러리가 준비되었습니다.\n"
          ]
        }
      ],
      "source": [
        "# ==================================\n",
        "# 셀 2: 필수 라이브러리 설치\n",
        "# ==================================\n",
        "!pip install --upgrade torch-geometric\n",
        "!pip install numba\n",
        "print(\"✅ 필수 라이브러리가 준비되었습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "############################## BFS 캐시 & 휴리스틱이 모든 화물을 인도하는지 검증 ##############################\n",
        "\n",
        "\n",
        "#────────────────── 0) 기본 준비 ──────────────────\n",
        "import json, time, random, numba, numpy as np\n",
        "from numba.typed import List\n",
        "from tqdm.auto   import tqdm\n",
        "\n",
        "DRIVE_ROOT   = \"/content/drive/MyDrive/OptiChallenge\"   # ← 개인 경로\n",
        "prob = json.load(open(f\"{DRIVE_ROOT}/Exercise_Problems/prob10.json\"))\n",
        "\n",
        "#────────────────── 1) numba-BFS ──────────────────\n",
        "@numba.njit\n",
        "def _bfs(adj, s, t, n):\n",
        "    if s == t:\n",
        "        L = List.empty_list(numba.int64); L.append(s); return L\n",
        "    par = np.full(n, -1, np.int64)\n",
        "    q   = List.empty_list(numba.int64); q.append(s); par[s] = s\n",
        "    hd = 0\n",
        "    while hd < len(q):\n",
        "        cur = q[hd]; hd += 1\n",
        "        for nb in adj[cur]:\n",
        "            if par[nb] == -1:\n",
        "                par[nb] = cur\n",
        "                if nb == t:\n",
        "                    out = List.empty_list(numba.int64)\n",
        "                    x = t\n",
        "                    while True:\n",
        "                        out.append(x)\n",
        "                        if x == s: break\n",
        "                        x = par[x]\n",
        "                    rev = List.empty_list(numba.int64)\n",
        "                    for i in range(len(out)-1, -1, -1): rev.append(out[i])\n",
        "                    return rev\n",
        "                q.append(nb)\n",
        "    return List.empty_list(numba.int64)\n",
        "\n",
        "#────────────────── 2) ShipEnv (RELOC 제거) ──────────────────\n",
        "class ShipEnv:\n",
        "    def __init__(self, pb):\n",
        "        self.N, self.P = pb[\"N\"], pb[\"P\"]; self.F = float(pb[\"F\"])\n",
        "        adj = [List.empty_list(numba.int64) for _ in range(self.N)]\n",
        "        for u, v in pb[\"E\"]: adj[u].append(v); adj[v].append(u)\n",
        "        self.adj = adj\n",
        "\n",
        "        self.cars, cid = [], 0\n",
        "        for (o, d), q in pb[\"K\"]:\n",
        "            for _ in range(q):\n",
        "                self.cars.append({\"id\": cid, \"origin\": o, \"dest\": d}); cid += 1\n",
        "        self.total = len(self.cars)\n",
        "\n",
        "        self.sp = {}\n",
        "        bar = tqdm(total=self.N**2, desc=\"BFS-cache\", unit=\"pair\")\n",
        "        for i in range(self.N):\n",
        "            row = {}\n",
        "            for j in range(self.N):\n",
        "                row[j] = list(_bfs(adj, i, j, self.N)); bar.update()\n",
        "            self.sp[i] = row\n",
        "        bar.close()\n",
        "        self.reset()\n",
        "\n",
        "    # helpers\n",
        "    def _path (self,s,t): return self.sp[s][t]\n",
        "    def _depth(self,i ):   return 0 if i==0 else len(self._path(0,i))-1\n",
        "    #-------------------------------------------------------------\n",
        "    def reset(self):\n",
        "        self.port = 0\n",
        "        self.node = [-1]*self.N\n",
        "        self.loc  = {}\n",
        "        self.on, self.tmp, self.done = set(), set(), set()\n",
        "\n",
        "    #-------------------------------------------------------------\n",
        "    def _nearest_empty(self):\n",
        "        for i in range(1, self.N):\n",
        "            if self.node[i] == -1: return i\n",
        "        return -1                       # 선내 만차 (연습용 문제에선 안 나옴)\n",
        "\n",
        "    #-------------------------------------------------------------\n",
        "    def load(self, cid):\n",
        "        tgt = self._nearest_empty()\n",
        "        if tgt == -1: return\n",
        "        self.node[tgt] = cid\n",
        "        self.loc[cid]  = tgt\n",
        "        self.on.add(cid)\n",
        "        self.tmp.discard(cid)\n",
        "\n",
        "    def unload(self, cid):\n",
        "        idx = self.loc[cid]\n",
        "        self.node[idx] = -1\n",
        "        self.on.remove(cid)\n",
        "        self.loc.pop(cid)\n",
        "        (self.done if self.cars[cid][\"dest\"] == self.port else self.tmp).add(cid)\n",
        "\n",
        "#────────────────── 3) Port-단위 매니저 ──────────────────\n",
        "def process_port(env: ShipEnv):\n",
        "    # 1) 목적지 = 현재 항구인 차들 먼저 하역\n",
        "    changed = True\n",
        "    while changed:\n",
        "        changed = False\n",
        "        for cid in list(env.on):\n",
        "            if env.cars[cid][\"dest\"] == env.port:\n",
        "                env.unload(cid); changed = True\n",
        "\n",
        "    # 2) 대기 + 임시 하역 →  도착港 오름차순으로 다시 선적\n",
        "    waiting = [c[\"id\"] for c in env.cars\n",
        "               if (c[\"origin\"] == env.port and\n",
        "                   c[\"id\"] not in env.on and\n",
        "                   c[\"id\"] not in env.tmp and\n",
        "                   c[\"id\"] not in env.done)]\n",
        "    waiting += list(env.tmp)            # 임시 하역 차도 함께\n",
        "    waiting.sort(key=lambda cid: env.cars[cid][\"dest\"])\n",
        "\n",
        "    for cid in waiting:\n",
        "        env.load(cid)\n",
        "\n",
        "#────────────────── 4) 전체 시뮬레이션 ──────────────────\n",
        "t0 = time.perf_counter()\n",
        "env = ShipEnv(prob)         # BFS 캐시 + JIT\n",
        "t1 = time.perf_counter()\n",
        "\n",
        "for _ in tqdm(range(env.P), desc=\"Ports\", unit=\"port\"):\n",
        "    process_port(env)\n",
        "    env.port += 1\n",
        "t2 = time.perf_counter()\n",
        "\n",
        "#────────────────── 5) 결과 ──────────────────\n",
        "print(f\"\\nDelivered {len(env.done)} / {env.total}\")\n",
        "print(f\"· init   : {t1 - t0:5.2f}s\")\n",
        "print(f\"· rollout: {t2 - t1:5.2f}s\")\n",
        "print(f\"· total  : {t2 - t0:5.2f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174,
          "referenced_widgets": [
            "10b3083f07594018a2d7083f320c2a1f",
            "f3fbc29bb1fb40f4b8401d67163debcd",
            "3b18db1c78b74f138500c1cd8a6c64eb",
            "0b8243e20c2f4c19b6a0b6ea9f3ed3bb",
            "3984f7b3222d4e459531ec76993d6564",
            "098bddb2790a43eb8130699124e3f0cd",
            "2cece6120ff34048b93a960adcca2f2d",
            "10af330243c24df6a0f066e2763eb05a",
            "9da3066d65744e69b34160f904ac73e9",
            "ade220bad3a64b13a525cdaff6852a38",
            "140fcfcac8054861b811b901ce6286db",
            "70982f5ede254274836c763a2b18fa85",
            "6d908a324add4e198054b0e8ec3a8d37",
            "7b90f8811e6b4fcba6ad9a081ae4d776",
            "19acf994b0da47a783444480a9536a62",
            "ec8dabdb014746c7b62526b22075a208",
            "9467fb546f794957a628bd9156e1ca9b",
            "dc984cd885d44bf1a3cdc0a5bb64bf98",
            "b30db2159c64495bb46eec3e9d73e8d3",
            "e17892e2a78c4246a042c81235e96160",
            "49b92639664040c1b0a1099460bbdb91",
            "e76fe8e4eca94d34a4552d054837e8da"
          ]
        },
        "id": "91raDutGM0ac",
        "outputId": "08b945c7-adae-42cd-e0fb-349e3c06fb94"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "BFS-cache:   0%|          | 0/131769 [00:00<?, ?pair/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10b3083f07594018a2d7083f320c2a1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Ports:   0%|          | 0/15 [00:00<?, ?port/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70982f5ede254274836c763a2b18fa85"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Delivered 812 / 812\n",
            "· init   : 144.25s\n",
            "· rollout:  0.02s\n",
            "· total  : 144.27s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mIPxSNJZeo6",
        "outputId": "1d9cf005-73aa-45d2-8b54-a22f128636c0"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-07-04 04:09:18 - INFO - Using device: cuda\n",
            "✅ CUDA 활성화 확인\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEBUG: >>>>>>>>>> STEP 1: Calling pretrain_with_imitation NOW...\n",
            "2025-07-04 04:09:19 - INFO - [Phase 1] Starting Imitation Learning for Worker Agent…\n",
            "2025-07-04 04:09:21 - INFO -   · Total Samples: 5,420\n",
            "2025-07-04 04:10:51 - INFO -     Epoch   10/50  avg-loss 11.2757\n",
            "2025-07-04 04:12:21 - INFO -     Epoch   20/50  avg-loss 9.8394\n",
            "2025-07-04 04:13:50 - INFO -     Epoch   30/50  avg-loss 8.8763\n",
            "2025-07-04 04:15:19 - INFO -     Epoch   40/50  avg-loss 8.1849\n",
            "2025-07-04 04:16:48 - INFO -     Epoch   50/50  avg-loss 7.7122\n",
            "2025-07-04 04:16:48 - INFO - [Phase 1] Imitation Learning Finished.\n",
            "DEBUG: <<<<<<<<<< STEP 1: pretrain_with_imitation FINISHED.\n",
            "\n",
            "2025-07-04 04:16:48 - INFO - \n",
            "[Phase 2] Starting Hierarchical Reinforcement Learning...\n",
            "2025-07-04 04:17:23 - INFO - M-Step     20 | Avg M-Rew:   -8.50 | W-Rew: -101.219 | Success:  65% | Avg Cost:  13640.0 | Goals: [TEMP:7, LOAD:13] | SPS: 0.58\n",
            "2025-07-04 04:17:23 - INFO - M-Step     20 | Avg Rew:   -8.50 | Success:  65% | Avg Cost:  13640.0 | Goals: [TEMP:7, LOAD:13] | SPS: 0.58\n",
            "2025-07-04 04:18:02 - INFO - M-Step     40 | Avg M-Rew:  -26.35 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.50\n",
            "2025-07-04 04:18:02 - INFO - M-Step     40 | Avg Rew:  -26.35 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.50\n",
            "2025-07-04 04:18:41 - INFO - M-Step     60 | Avg M-Rew:  -22.30 | W-Rew: -103.034 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.52\n",
            "2025-07-04 04:18:41 - INFO - M-Step     60 | Avg Rew:  -22.30 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.52\n",
            "2025-07-04 04:19:21 - INFO - M-Step     80 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.50\n",
            "2025-07-04 04:19:21 - INFO - M-Step     80 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.50\n",
            "2025-07-04 04:20:02 - INFO - M-Step    100 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.48\n",
            "2025-07-04 04:20:02 - INFO - M-Step    100 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.48\n",
            "2025-07-04 04:20:36 - INFO - M-Step    120 | Avg M-Rew:  -21.83 | W-Rew: -102.911 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.58\n",
            "2025-07-04 04:20:36 - INFO - M-Step    120 | Avg Rew:  -21.83 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.58\n",
            "2025-07-04 04:21:11 - INFO - M-Step    140 | Avg M-Rew:  -21.83 | W-Rew: -102.911 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.58\n",
            "2025-07-04 04:21:11 - INFO - M-Step    140 | Avg Rew:  -21.83 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.58\n",
            "2025-07-04 04:21:49 - INFO - M-Step    160 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:21:49 - INFO - M-Step    160 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:22:30 - INFO - M-Step    180 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 04:22:30 - INFO - M-Step    180 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 04:23:08 - INFO - M-Step    200 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:23:08 - INFO - M-Step    200 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:23:53 - INFO - M-Step    220 | Avg M-Rew:  -31.88 | W-Rew: -103.196 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:23:53 - INFO - M-Step    220 | Avg Rew:  -31.88 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:24:36 - INFO - M-Step    240 | Avg M-Rew:  -30.20 | W-Rew: -103.164 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.46\n",
            "2025-07-04 04:24:36 - INFO - M-Step    240 | Avg Rew:  -30.20 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.46\n",
            "2025-07-04 04:25:17 - INFO - M-Step    260 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.48\n",
            "2025-07-04 04:25:17 - INFO - M-Step    260 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.48\n",
            "2025-07-04 04:26:03 - INFO - M-Step    280 | Avg M-Rew:  -33.55 | W-Rew: -103.226 | Success:  30% | Avg Cost:  22725.0 | Goals: [TEMP:14, LOAD:6] | SPS: 0.43\n",
            "2025-07-04 04:26:03 - INFO - M-Step    280 | Avg Rew:  -33.55 | Success:  30% | Avg Cost:  22725.0 | Goals: [TEMP:14, LOAD:6] | SPS: 0.43\n",
            "2025-07-04 04:26:34 - INFO - M-Step    300 | Avg M-Rew:  -18.48 | W-Rew: -102.732 | Success:  75% | Avg Cost:  11362.5 | Goals: [TEMP:5, LOAD:15] | SPS: 0.66\n",
            "2025-07-04 04:26:34 - INFO - M-Step    300 | Avg Rew:  -18.48 | Success:  75% | Avg Cost:  11362.5 | Goals: [TEMP:5, LOAD:15] | SPS: 0.66\n",
            "2025-07-04 04:27:17 - INFO - M-Step    320 | Avg M-Rew:  -30.20 | W-Rew: -103.164 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:27:17 - INFO - M-Step    320 | Avg Rew:  -30.20 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:27:54 - INFO - M-Step    340 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:27:54 - INFO - M-Step    340 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:28:32 - INFO - M-Step    360 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:28:32 - INFO - M-Step    360 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:29:22 - INFO - M-Step    380 | Avg M-Rew:  -36.90 | W-Rew: -103.275 | Success:  20% | Avg Cost:  25250.0 | Goals: [TEMP:16, LOAD:4] | SPS: 0.40\n",
            "2025-07-04 04:29:22 - INFO - M-Step    380 | Avg Rew:  -36.90 | Success:  20% | Avg Cost:  25250.0 | Goals: [TEMP:16, LOAD:4] | SPS: 0.40\n",
            "2025-07-04 04:29:58 - INFO - M-Step    400 | Avg M-Rew:  -23.50 | W-Rew: -102.978 | Success:  60% | Avg Cost:  15150.0 | Goals: [TEMP:8, LOAD:12] | SPS: 0.56\n",
            "2025-07-04 04:29:58 - INFO - M-Step    400 | Avg Rew:  -23.50 | Success:  60% | Avg Cost:  15150.0 | Goals: [TEMP:8, LOAD:12] | SPS: 0.56\n",
            "2025-07-04 04:30:36 - INFO - M-Step    420 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:30:36 - INFO - M-Step    420 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:31:15 - INFO - M-Step    440 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:31:15 - INFO - M-Step    440 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:31:53 - INFO - M-Step    460 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:31:53 - INFO - M-Step    460 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:32:27 - INFO - M-Step    480 | Avg M-Rew:  -21.83 | W-Rew: -102.911 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.58\n",
            "2025-07-04 04:32:27 - INFO - M-Step    480 | Avg Rew:  -21.83 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.58\n",
            "2025-07-04 04:33:01 - INFO - M-Step    500 | Avg M-Rew:  -21.83 | W-Rew: -102.911 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.59\n",
            "2025-07-04 04:33:01 - INFO - M-Step    500 | Avg Rew:  -21.83 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.59\n",
            "2025-07-04 04:33:41 - INFO - M-Step    520 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:33:41 - INFO - M-Step    520 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:34:18 - INFO - M-Step    540 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:34:18 - INFO - M-Step    540 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:35:00 - INFO - M-Step    560 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.48\n",
            "2025-07-04 04:35:00 - INFO - M-Step    560 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.48\n",
            "2025-07-04 04:35:41 - INFO - M-Step    580 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 04:35:41 - INFO - M-Step    580 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 04:36:15 - INFO - M-Step    600 | Avg M-Rew:  -21.83 | W-Rew: -102.911 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.59\n",
            "2025-07-04 04:36:15 - INFO - M-Step    600 | Avg Rew:  -21.83 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.59\n",
            "2025-07-04 04:37:00 - INFO - M-Step    620 | Avg M-Rew:  -31.88 | W-Rew: -103.196 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:37:00 - INFO - M-Step    620 | Avg Rew:  -31.88 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:37:39 - INFO - M-Step    640 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:37:39 - INFO - M-Step    640 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:38:24 - INFO - M-Step    660 | Avg M-Rew:  -31.88 | W-Rew: -103.196 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:38:24 - INFO - M-Step    660 | Avg Rew:  -31.88 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:39:00 - INFO - M-Step    680 | Avg M-Rew:  -23.50 | W-Rew: -102.978 | Success:  60% | Avg Cost:  15150.0 | Goals: [TEMP:8, LOAD:12] | SPS: 0.55\n",
            "2025-07-04 04:39:00 - INFO - M-Step    680 | Avg Rew:  -23.50 | Success:  60% | Avg Cost:  15150.0 | Goals: [TEMP:8, LOAD:12] | SPS: 0.55\n",
            "2025-07-04 04:39:34 - INFO - M-Step    700 | Avg M-Rew:  -21.83 | W-Rew: -102.911 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.59\n",
            "2025-07-04 04:39:34 - INFO - M-Step    700 | Avg Rew:  -21.83 | Success:  65% | Avg Cost:  13887.5 | Goals: [TEMP:7, LOAD:13] | SPS: 0.59\n",
            "2025-07-04 04:40:12 - INFO - M-Step    720 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:40:12 - INFO - M-Step    720 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:40:56 - INFO - M-Step    740 | Avg M-Rew:  -31.88 | W-Rew: -103.196 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:40:56 - INFO - M-Step    740 | Avg Rew:  -31.88 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:41:34 - INFO - M-Step    760 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:41:34 - INFO - M-Step    760 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:42:13 - INFO - M-Step    780 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:42:13 - INFO - M-Step    780 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:42:52 - INFO - M-Step    800 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:42:52 - INFO - M-Step    800 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:43:32 - INFO - M-Step    820 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:43:32 - INFO - M-Step    820 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:44:17 - INFO - M-Step    840 | Avg M-Rew:  -31.88 | W-Rew: -103.196 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:44:17 - INFO - M-Step    840 | Avg Rew:  -31.88 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:44:55 - INFO - M-Step    860 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:44:55 - INFO - M-Step    860 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:45:32 - INFO - M-Step    880 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:45:32 - INFO - M-Step    880 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:46:10 - INFO - M-Step    900 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:46:10 - INFO - M-Step    900 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:46:51 - INFO - M-Step    920 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 04:46:51 - INFO - M-Step    920 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 04:47:32 - INFO - M-Step    940 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.48\n",
            "2025-07-04 04:47:32 - INFO - M-Step    940 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.48\n",
            "2025-07-04 04:48:12 - INFO - M-Step    960 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:48:12 - INFO - M-Step    960 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:48:57 - INFO - M-Step    980 | Avg M-Rew:  -31.88 | W-Rew: -103.196 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.44\n",
            "2025-07-04 04:48:57 - INFO - M-Step    980 | Avg Rew:  -31.88 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.44\n",
            "2025-07-04 04:49:38 - INFO - M-Step   1000 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 04:49:38 - INFO - M-Step   1000 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 04:50:21 - INFO - M-Step   1020 | Avg M-Rew:  -30.20 | W-Rew: -103.164 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:50:21 - INFO - M-Step   1020 | Avg Rew:  -30.20 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:51:06 - INFO - M-Step   1040 | Avg M-Rew:  -31.88 | W-Rew: -103.196 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:51:06 - INFO - M-Step   1040 | Avg Rew:  -31.88 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 04:51:44 - INFO - M-Step   1060 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:51:44 - INFO - M-Step   1060 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:52:27 - INFO - M-Step   1080 | Avg M-Rew:  -30.20 | W-Rew: -103.164 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.46\n",
            "2025-07-04 04:52:27 - INFO - M-Step   1080 | Avg Rew:  -30.20 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.46\n",
            "2025-07-04 04:53:09 - INFO - M-Step   1100 | Avg M-Rew:  -30.20 | W-Rew: -103.164 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:53:09 - INFO - M-Step   1100 | Avg Rew:  -30.20 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:53:49 - INFO - M-Step   1120 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:53:49 - INFO - M-Step   1120 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:54:32 - INFO - M-Step   1140 | Avg M-Rew:  -30.20 | W-Rew: -103.164 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:54:32 - INFO - M-Step   1140 | Avg Rew:  -30.20 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:55:18 - INFO - M-Step   1160 | Avg M-Rew:  -33.55 | W-Rew: -103.226 | Success:  30% | Avg Cost:  22725.0 | Goals: [TEMP:14, LOAD:6] | SPS: 0.43\n",
            "2025-07-04 04:55:18 - INFO - M-Step   1160 | Avg Rew:  -33.55 | Success:  30% | Avg Cost:  22725.0 | Goals: [TEMP:14, LOAD:6] | SPS: 0.43\n",
            "2025-07-04 04:55:56 - INFO - M-Step   1180 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:55:56 - INFO - M-Step   1180 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:56:33 - INFO - M-Step   1200 | Avg M-Rew:  -25.18 | W-Rew: -103.035 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:56:33 - INFO - M-Step   1200 | Avg Rew:  -25.18 | Success:  55% | Avg Cost:  16412.5 | Goals: [TEMP:9, LOAD:11] | SPS: 0.53\n",
            "2025-07-04 04:57:16 - INFO - M-Step   1220 | Avg M-Rew:  -30.20 | W-Rew: -103.164 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:57:16 - INFO - M-Step   1220 | Avg Rew:  -30.20 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:57:55 - INFO - M-Step   1240 | Avg M-Rew:  -26.85 | W-Rew: -103.084 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:57:55 - INFO - M-Step   1240 | Avg Rew:  -26.85 | Success:  50% | Avg Cost:  17675.0 | Goals: [TEMP:10, LOAD:10] | SPS: 0.51\n",
            "2025-07-04 04:58:31 - INFO - M-Step   1260 | Avg M-Rew:  -23.50 | W-Rew: -102.978 | Success:  60% | Avg Cost:  15150.0 | Goals: [TEMP:8, LOAD:12] | SPS: 0.56\n",
            "2025-07-04 04:58:31 - INFO - M-Step   1260 | Avg Rew:  -23.50 | Success:  60% | Avg Cost:  15150.0 | Goals: [TEMP:8, LOAD:12] | SPS: 0.56\n",
            "2025-07-04 04:58:56 - INFO - M-Step   1280 | Avg M-Rew:  -13.45 | W-Rew: -102.243 | Success:  90% | Avg Cost:   7575.0 | Goals: [TEMP:2, LOAD:18] | SPS: 0.80\n",
            "2025-07-04 04:58:56 - INFO - M-Step   1280 | Avg Rew:  -13.45 | Success:  90% | Avg Cost:   7575.0 | Goals: [TEMP:2, LOAD:18] | SPS: 0.80\n",
            "2025-07-04 04:59:39 - INFO - M-Step   1300 | Avg M-Rew:  -30.20 | W-Rew: -103.164 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 04:59:39 - INFO - M-Step   1300 | Avg Rew:  -30.20 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 05:00:20 - INFO - M-Step   1320 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 05:00:20 - INFO - M-Step   1320 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 05:01:07 - INFO - M-Step   1340 | Avg M-Rew:  -33.55 | W-Rew: -103.226 | Success:  30% | Avg Cost:  22725.0 | Goals: [TEMP:14, LOAD:6] | SPS: 0.43\n",
            "2025-07-04 05:01:07 - INFO - M-Step   1340 | Avg Rew:  -33.55 | Success:  30% | Avg Cost:  22725.0 | Goals: [TEMP:14, LOAD:6] | SPS: 0.43\n",
            "2025-07-04 05:01:52 - INFO - M-Step   1360 | Avg M-Rew:  -31.88 | W-Rew: -103.196 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 05:01:52 - INFO - M-Step   1360 | Avg Rew:  -31.88 | Success:  35% | Avg Cost:  21462.5 | Goals: [TEMP:13, LOAD:7] | SPS: 0.45\n",
            "2025-07-04 05:02:33 - INFO - M-Step   1380 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 05:02:33 - INFO - M-Step   1380 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 05:03:15 - INFO - M-Step   1400 | Avg M-Rew:  -30.20 | W-Rew: -103.164 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 05:03:15 - INFO - M-Step   1400 | Avg Rew:  -30.20 | Success:  40% | Avg Cost:  20200.0 | Goals: [TEMP:12, LOAD:8] | SPS: 0.47\n",
            "2025-07-04 05:03:56 - INFO - M-Step   1420 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 05:03:56 - INFO - M-Step   1420 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 05:04:38 - INFO - M-Step   1440 | Avg M-Rew:  -28.53 | W-Rew: -103.126 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n",
            "2025-07-04 05:04:38 - INFO - M-Step   1440 | Avg Rew:  -28.53 | Success:  45% | Avg Cost:  18937.5 | Goals: [TEMP:11, LOAD:9] | SPS: 0.49\n"
          ]
        }
      ],
      "source": [
        "########## v11 : HRL 트랜스포머 GNN 보상함수의 unambiguity ##########\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 섹션 1: 설정 및 기본 유틸리티 (리팩토링 버전)\n",
        "# ==============================================================================\n",
        "import json\n",
        "import torch.optim as optim\n",
        "import collections\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "import logging\n",
        "from torch_geometric.data import Data\n",
        "import pickle\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "import logging\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from collections import defaultdict, deque\n",
        "import multiprocessing as mp\n",
        "import random\n",
        "import time, collections, torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numba\n",
        "from numba.core import types\n",
        "from numba.typed import List\n",
        "import time, collections\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time, collections\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# =========================================\n",
        "# ⚡ 셀: EpisodeTracker – 충돌 수정본\n",
        "# =========================================\n",
        "\n",
        "\n",
        "class EpisodeTracker:\n",
        "    def __init__(self, h_env, log_dir, max_same_streak=15):\n",
        "        self.env   = h_env\n",
        "        ts         = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.tb    = SummaryWriter(f\"{log_dir}/tracker_{ts}\")\n",
        "        self.eps   = 0          # 에피소드 번호\n",
        "        self.t     = 0          # ✅ 스텝 카운터 (이전 self.step → self.t 로 변경)\n",
        "        self.buf   = collections.defaultdict(list)\n",
        "        self.max_same_streak = max_same_streak\n",
        "\n",
        "    # 내부 env 속성 투명 패스-스루\n",
        "    def __getattr__(self, name):\n",
        "        return getattr(self.env, name)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Manager 레벨에서 reset 호출\n",
        "    # -------------------------------\n",
        "    def reset(self, *a, **kw):\n",
        "        self._flush()\n",
        "        self.eps += 1\n",
        "        self.t   = 0\n",
        "        return self.env.reset(*a, **kw)\n",
        "\n",
        "    # -------------------------------\n",
        "    # Manager 레벨에서 step 호출\n",
        "    # -------------------------------\n",
        "    def step(self, manager_action_idx, *, greedy_worker=False):\n",
        "        self.t += 1\n",
        "        s_next, m_rew, done, info = \\\n",
        "            self.env.step(manager_action_idx, greedy_worker=greedy_worker)\n",
        "\n",
        "        # ── 추가 통계 수집 예시 ───────────────────────────\n",
        "        s_env  = self.env.ship_env\n",
        "        legal  = s_env.get_legal_actions(for_worker=True)\n",
        "        valid  = [a for a in legal\n",
        "                  if not (a[0]=='RELOCATE_INTERNAL' and\n",
        "                          s_env._find_best_internal_spot(a[1])[0] == -1)]\n",
        "\n",
        "        self.buf['legal_cnt'].append(len(legal))\n",
        "        self.buf['valid_cnt'].append(len(valid))\n",
        "        self.buf['manager_r'].append(m_rew)\n",
        "        self.buf['worker_r'].append(info.get('worker_total_reward', 0.0))\n",
        "        self.buf['cost'].append(info.get('cost', 0.0))\n",
        "        self.buf['same_streak_break'].append(\n",
        "            1 if info.get('same_action_streak', 0) >= self.max_same_streak else 0)\n",
        "\n",
        "        if done:\n",
        "            self._flush()\n",
        "        return s_next, m_rew, done, info\n",
        "    # -------------------------------\n",
        "\n",
        "    def _flush(self):\n",
        "        if not self.buf:\n",
        "            return\n",
        "        avg = lambda k: sum(self.buf[k]) / max(1, len(self.buf[k]))\n",
        "        e   = self.eps\n",
        "        self.tb.add_scalar(\"Episode/avg_legal_actions\",  avg('legal_cnt'),     e)\n",
        "        self.tb.add_scalar(\"Episode/avg_valid_actions\",  avg('valid_cnt'),     e)\n",
        "        self.tb.add_scalar(\"Episode/avg_manager_reward\", avg('manager_r'),     e)\n",
        "        self.tb.add_scalar(\"Episode/avg_worker_reward\",  avg('worker_r'),      e)\n",
        "        self.tb.add_scalar(\"Episode/avg_cost\",           avg('cost'),          e)\n",
        "        self.tb.add_scalar(\"Episode/same_streak_breaks\", sum(self.buf['same_streak_break']), e)\n",
        "        self.tb.flush()\n",
        "        self.buf.clear()\n",
        "\n",
        "\n",
        "def setup_logger():\n",
        "    \"\"\"\n",
        "    다른 라이브러리의 로깅 설정을 모두 초기화하고,\n",
        "    우리가 원하는 설정으로 강제 적용하는 함수.\n",
        "    \"\"\"\n",
        "    # 루트 로거를 가져옵니다.\n",
        "    root_logger = logging.getLogger()\n",
        "    root_logger.setLevel(logging.INFO) # 로그 레벨 설정\n",
        "\n",
        "    # 루트 로거에 연결된 모든 기존 핸들러(Handler)를 제거합니다. (가장 중요)\n",
        "    for handler in root_logger.handlers[:]:\n",
        "        root_logger.removeHandler(handler)\n",
        "\n",
        "    # 우리가 원하는 새로운 핸들러를 생성하여 추가합니다.\n",
        "    handler = logging.StreamHandler(sys.stdout) # 로그를 콘솔에 출력\n",
        "    formatter = logging.Formatter(\n",
        "        '%(asctime)s - %(levelname)s - %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S'\n",
        "    )\n",
        "    handler.setFormatter(formatter)\n",
        "    root_logger.addHandler(handler)\n",
        "\n",
        "\n",
        "class Config:\n",
        "    \"\"\"학습과 관련된 모든 하이퍼파라미터와 설정을 관리하는 클래스\"\"\"\n",
        "\n",
        "    # --- 경로 설정 ---\n",
        "    DRIVE_PROJECT_ROOT = '/content/drive/MyDrive/OptiChallenge'\n",
        "    PROBLEM_DIR  = os.path.join(DRIVE_PROJECT_ROOT, 'Exercise_Problems')\n",
        "    LOG_DIR      = os.path.join(DRIVE_PROJECT_ROOT, 'v12_logs_refactored')\n",
        "    MODEL_DIR    = os.path.join(DRIVE_PROJECT_ROOT, 'v12_models_refactored')\n",
        "\n",
        "    # 💡 새 expert pkl  ‘expert_probX_TYY.pkl’ 형식으로 있다고 가정\n",
        "    EXPERT_DIR   = os.path.join(DRIVE_PROJECT_ROOT)\n",
        "    EXPERT_GLOB  = \"expert_prob[1248]_T*.pkl\"\n",
        "    MAX_EXPERT_SAMPLES = 40_000\n",
        "\n",
        "    @property\n",
        "    def EXPERT_DATA_PATHS(self):\n",
        "        import glob\n",
        "        paths = sorted(glob.glob(os.path.join(self.EXPERT_DIR, self.EXPERT_GLOB)))\n",
        "        if not paths:\n",
        "            logging.warning(f\"[Config] No expert pkl found under {self.EXPERT_DIR}\")\n",
        "        return paths\n",
        "\n",
        "    # --- 학습 제어 ---\n",
        "    TOTAL_MANAGER_STEPS = 500_000       # Manager 에이전트의 총 학습 스텝\n",
        "    CURRICULUM_STEPS = 2000            # 전문가 정책을 모방하는 커리큘럼 학습 스텝 수\n",
        "    PRINT_INTERVAL_MANAGER_STEPS = 20  # 학습 중 로그 출력 간격 (Manager 스텝 기준)\n",
        "    EVAL_INTERVAL_MANAGER_STEPS = 2000 # 모델 평가 및 저장 간격 (Manager 스텝 기준)\n",
        "    EVAL_EPISODES = 10                  # 평가 시 실행할 에피소드 수\n",
        "    CURRICULUM_TRANSITION_STEP = 20000 # 💡 [추가] 문제 난이도 커리큘럼 전환 시점\n",
        "    FIXED_FEE = 100.0\n",
        "    ALPHA_MOVE_DIST  = 1.0     # 거리 1칸당 변동비\n",
        "    BETA_RELOC_COST  = 50.0    # 재배치(route 2개) 고정 패널티\n",
        "    SHAPING_REWARD_WEIGHT = FIXED_FEE\n",
        "\n",
        "    # --- 모방 학습 (Worker) ---\n",
        "    IMITATION_LEARNING_EPOCHS = 50      # Worker 모방 학습 에폭 수\n",
        "    IMITATION_LR = 1e-4                 # Worker 모방 학습 Learning Rate\n",
        "    IMITATION_BATCH_SIZE = 512          # Worker 모방 학습 배치 크기\n",
        "\n",
        "    # --- Manager 에이전트 설정 ---\n",
        "    MANAGER_STATE_DIM = 6               # Manager 상태 벡터의 차원\n",
        "    MANAGER_ACTION_DIM = 5              # Manager 행동의 가짓수\n",
        "    MANAGER_LR = 3e-4                   # Manager Learning Rate\n",
        "    MANAGER_GAMMA = 0.99                # Manager 할인율 (Gamma)\n",
        "    MANAGER_ENTROPY_COEF = 0.05         # Manager 엔트로피 보너스 계수 (탐험 장려)\n",
        "    MANAGER_NUM_STEPS_PER_UPDATE = 512  # Manager 업데이트를 위한 데이터 수집 스텝\n",
        "\n",
        "    # [신규] 보상 체계 하이퍼파라미터\n",
        "    REPEAT_PENALTY = -1             # 같은 행동 반복 페널티\n",
        "    STEP_PENALTY_WEIGHT = 0.001       # Worker 스텝당 시간 페널티 가중치\n",
        "    NO_PROGRESS_PENALTY = -2.0        # Worker가 진척 없이 종료 시 페널티\n",
        "    NO_PROGRESS_LIMIT = 50            # Worker 진척 판정 한도 (기존 50 하드코딩 값 대체)\\\n",
        "    TIMEOUT_PENALTY = -10.0\n",
        "\n",
        "    # --- Worker 에이전트 설정 ---\n",
        "    WORKER_LR = 3e-4                    # Worker Learning Rate\n",
        "    WORKER_GAMMA = 0.95                 # Worker 할인율 (Gamma)\n",
        "    WORKER_ENTROPY_COEF = 0.02         # Worker 엔트로피 보너스 계수\n",
        "    WORKER_MAX_STEPS_PER_GOAL = 300     # Manager의 목표 하나당 Worker가 수행할 최대 스텝\n",
        "    WORKER_NUM_STEPS_PER_UPDATE = 1024\n",
        "\n",
        "    # --- PPO 알고리즘 공통 설정 ---\n",
        "    PPO_UPDATE_EPOCHS = 4               # 한 번의 업데이트 시 에폭 수\n",
        "    PPO_NUM_MINIBATCHES = 8             # 미니배치 개수\n",
        "    PPO_CLIP_COEF = 0.2                 # PPO 클리핑 계수\n",
        "    PPO_GAE_LAMBDA = 0.95               # GAE(Generalized Advantage Estimation) 람다값\n",
        "    PPO_VALUE_COEF = 1.0                # 가치 함수 손실(Value Loss) 계수\n",
        "    PPO_MAX_GRAD_NORM = 0.5             # Gradient Clipping 최대 L2 Norm\n",
        "\n",
        "    # --- 네트워크 구조 설정 ---\n",
        "    NODE_FEATURE_DIM = 4      # [is_occupied, dest_diff, blocking_count, is_relocatable]\n",
        "    GNN_EMBED_DIM = 128       # GNN의 기본 임베딩 차원\n",
        "    GOAL_EMBED_DIM = 16       # 목표 임베딩 벡터 차원\n",
        "    # Worker의 GNN 출력을 Manager 상태로 사용 (mean_pool + att_pool)\n",
        "    # MANAGER_STATE_DIM = GNN_EMBED_DIM * 2\n",
        "\n",
        "# --- 경로 생성 ---\n",
        "# 학습 로그와 모델 가중치를 저장할 디렉토리를 생성합니다.\n",
        "os.makedirs(Config.LOG_DIR, exist_ok=True)\n",
        "os.makedirs(Config.MODEL_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "# 💡 --- 이 함수로 기존의 모든 get_shortest_path 관련 함수를 교체 --- 💡\n",
        "@numba.jit(nopython=True)\n",
        "def get_shortest_path(adj_list, start, end, num_nodes):\n",
        "    \"\"\"\n",
        "    Numba에 완벽히 호환되는 가장 표준적이고 안정적인 BFS 함수.\n",
        "    - 부모 노드 추적 방식을 사용\n",
        "    - NumPy 배열과 기본 리스트만 사용\n",
        "    \"\"\"\n",
        "    if start == end:\n",
        "        # Numba를 위해 타입을 명시적으로 리스트로 생성\n",
        "        path = numba.typed.List()\n",
        "        path.append(start)\n",
        "        return path\n",
        "\n",
        "    # 부모 노드를 기록할 NumPy 배열 (-1로 초기화)\n",
        "    parents = np.full(num_nodes, -1, dtype=np.int64)\n",
        "\n",
        "    # 방문 기록을 위한 boolean NumPy 배열\n",
        "    visited = np.zeros(num_nodes, dtype=np.bool_)\n",
        "\n",
        "    # 큐로 사용할 단순 리스트\n",
        "    queue = numba.typed.List()\n",
        "\n",
        "    queue.append(start)\n",
        "    visited[start] = True\n",
        "    head = 0 # 큐의 맨 앞을 가리키는 포인터\n",
        "\n",
        "    path_found = False\n",
        "    while head < len(queue):\n",
        "        current = queue[head]\n",
        "        head += 1\n",
        "\n",
        "        if current == end:\n",
        "            path_found = True\n",
        "            break\n",
        "\n",
        "        for neighbor in adj_list[current]:\n",
        "            if not visited[neighbor]:\n",
        "                visited[neighbor] = True\n",
        "                parents[neighbor] = current\n",
        "                queue.append(neighbor)\n",
        "\n",
        "    # 경로 역추적\n",
        "    if path_found:\n",
        "        path = numba.typed.List()\n",
        "        curr = end\n",
        "        while curr != -1:\n",
        "            path.append(curr)\n",
        "            curr = parents[curr]\n",
        "        return path[::-1] # 역순이므로 뒤집어서 반환\n",
        "\n",
        "    return numba.typed.List.empty_list(numba.int64)\n",
        "\n",
        "# 💡 --- 교체 완료 --- 💡\n",
        "\n",
        "class ShipEnv:\n",
        "    \"\"\"화물선의 상태와 행동을 시뮬레이션하는 환경 클래스 (Numba 최종 최적화 적용)\"\"\"\n",
        "    def __init__(self, problem_data: dict, max_num_ports: int):\n",
        "        self.num_nodes = problem_data.get('N', 1)\n",
        "        self.num_ports = problem_data.get('P', 1)\n",
        "        # self.fixed_cost = float(problem_data.get('F', 100))\n",
        "        self.fixed_cost = float(problem_data['F'])     # json 에 반드시 있음\n",
        "        self.max_num_ports = max_num_ports\n",
        "        self.load_fail_streak: int = 0\n",
        "\n",
        "        # 💡 --- [핵심] Numba 호환을 위한 그래프 데이터 구조화 --- 💡\n",
        "        adj_list = [numba.typed.List.empty_list(numba.int64) for _ in range(self.num_nodes)]\n",
        "        edge_list_for_tensor = []\n",
        "        for u, v in problem_data.get('E', []):\n",
        "            adj_list[u].append(v)\n",
        "            adj_list[v].append(u)\n",
        "            edge_list_for_tensor.extend([[u, v], [v, u]])\n",
        "        self.adj_list = adj_list # Numba 함수에 넘겨주기 위해 저장\n",
        "        # 💡 --- 수정 완료 --- 💡\n",
        "\n",
        "        self.edge_index_tensor = torch.tensor(edge_list_for_tensor, dtype=torch.long).t().contiguous()\n",
        "\n",
        "        # 모든 노드 쌍 간의 최단 경로를 미리 계산하여 캐싱\n",
        "        self.shortest_paths = {}\n",
        "        for i in range(self.num_nodes):\n",
        "            self.shortest_paths[i] = {}\n",
        "            for j in range(self.num_nodes):\n",
        "                # 💡 [핵심] 수정된 get_shortest_path 함수 호출\n",
        "                path_result = get_shortest_path(self.adj_list, i, j, self.num_nodes)\n",
        "                self.shortest_paths[i][j] = list(path_result) # 결과를 일반 리스트로 저장\n",
        "\n",
        "        self.cars = []\n",
        "        car_id_counter = 0\n",
        "        for demand_idx, (demand, quantity) in enumerate(problem_data.get('K', [])):\n",
        "            origin, dest = demand\n",
        "            for _ in range(quantity):\n",
        "                self.cars.append({'id': car_id_counter, 'demand_id': demand_idx, 'origin': origin, 'dest': dest})\n",
        "                car_id_counter += 1\n",
        "        self.total_cars = len(self.cars)\n",
        "        self.reset()\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _move_cost(dist: int, *, is_reloc: bool) -> float:\n",
        "      \"\"\"\n",
        "      모든 이동 경로의 총 비용을 계산한다.\n",
        "        • 선적 / 하역 / 위치변경 / 임시하역-재적재 모두 동일 공식\n",
        "        • is_reloc=True 이면 β(재배치 패널티)만 한 번 더 더한다\n",
        "      \"\"\"\n",
        "      base = Config.FIXED_FEE + Config.ALPHA_MOVE_DIST * dist\n",
        "      return base + (Config.BETA_RELOC_COST if is_reloc else 0.0)\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # 💡  Gate-depth  (게이트에서 몇 칸 안쪽인가?)\n",
        "    # ------------------------------------------------------------\n",
        "    def _gate_depth(self, node_idx: int) -> int:\n",
        "        \"\"\"\n",
        "        노드가 게이트(0)에서 떨어진 깊이(=edge 수)를 빠르게 계산.\n",
        "        • 캐싱된 최단경로를 쓰므로 O(1)\n",
        "        • gate(0) 자체는 depth 0\n",
        "        \"\"\"\n",
        "        if node_idx == 0:\n",
        "            return 0\n",
        "        path = self._get_or_compute_path(0, node_idx)\n",
        "        return (len(path) - 1) if path else 1_000_000   # unreachable guard\n",
        "\n",
        "\n",
        "    # ===================================================================\n",
        "    # ➊  ShipEnv  ── 초경량 인접­확인 헬퍼\n",
        "    #     (adj_list 만으로 O(1) 체크, path 계산 호출 ↓)\n",
        "    # ===================================================================\n",
        "    def _is_adjacent(self, n1: int, n2: int) -> bool:\n",
        "        \"\"\"\n",
        "        두 노드가 바로 연결돼 있는지 빠르게 확인한다.\n",
        "        ▸ _get_or_compute_path() 호출을 피하면서\n",
        "          “바로 옆칸” 여부만 알고 싶을 때 사용.\n",
        "        \"\"\"\n",
        "        return n2 in self.adj_list[n1]\n",
        "\n",
        "\n",
        "    def _get_or_compute_path(self, start: int, end: int) -> list | None:\n",
        "        \"\"\"미리 계산된 경로를 캐시에서 조회합니다.\"\"\"\n",
        "        return self.shortest_paths.get(start, {}).get(end, None)\n",
        "\n",
        "\n",
        "    def reset(self) -> Data:\n",
        "        self.current_port: int = 0\n",
        "        self.node_status: list[int] = [-1] * self.num_nodes\n",
        "        self.car_locations: dict[int, int] = {}\n",
        "        self.cars_on_board: set[int] = set()\n",
        "        self.temporarily_unloaded_cars: set[int] = set()\n",
        "        self.delivered_cars: set[int] = set()\n",
        "        self.relocations_this_episode: int = 0\n",
        "        self.last_car_action = {}\n",
        "        # [NEW] 에피소드가 바뀌면 실패 누적도 초기화\n",
        "        self.load_fail_streak: int = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self) -> Data:\n",
        "      node_features = []\n",
        "      for i in range(self.num_nodes):\n",
        "          if i == 0:\n",
        "              node_features.append([0.0, 0.0, 0.0, 0.0]); continue\n",
        "          car_id = self.node_status[i]\n",
        "          if car_id == -1:\n",
        "              node_features.append([0.0, 0.0, 0.0, 0.0])\n",
        "          else:\n",
        "              car = self.cars[car_id]\n",
        "              dest_diff = float(car['dest'] - self.current_port)\n",
        "              path_to_gate = self._get_or_compute_path(i, 0)\n",
        "\n",
        "              # 💡 --- [핵심 최적화] 경로 리스트를 set으로 변환하여 확인 속도 향상 --- 💡\n",
        "              if path_to_gate:\n",
        "                  path_to_gate_set = set(path_to_gate[1:])\n",
        "                  blocking_count = sum(1 for node_idx, status in enumerate(self.node_status)\n",
        "                                      if status != -1 and node_idx in path_to_gate_set)\n",
        "              else:\n",
        "                  blocking_count = 0\n",
        "              # 💡 --- 최적화 완료 --- 💡\n",
        "\n",
        "              is_relocatable = 1.0 if car['dest'] != self.current_port else 0.0\n",
        "              node_features.append([1.0, dest_diff, float(blocking_count), is_relocatable])\n",
        "\n",
        "\n",
        "      waiting_cars = [c for c in self.cars if c['origin'] == self.current_port and c['id'] not in self.cars_on_board and c['id'] not in self.temporarily_unloaded_cars]\n",
        "      waiting_dest_counts = [0.0] * self.max_num_ports\n",
        "      for car in waiting_cars:\n",
        "          if car['dest'] < self.max_num_ports: waiting_dest_counts[car['dest']] += 1.0\n",
        "      global_features = [float(self.current_port), float(len(waiting_cars)), float(len(self.temporarily_unloaded_cars))] + waiting_dest_counts\n",
        "\n",
        "      return Data(x=torch.tensor(node_features, dtype=torch.float),\n",
        "                  edge_index=self.edge_index_tensor,\n",
        "                  global_features=torch.tensor([global_features], dtype=torch.float))\n",
        "\n",
        "    # ===================================================================\n",
        "    # ➋  ShipEnv.get_legal_actions  ―  완전 교체본\n",
        "    #     - RELOCATE_INTERNAL 후보는 ‘실제로 빈자리로 이동 가능한 차’만\n",
        "    #     - LOAD / UNLOAD 가능 조건을 좀 더 날카롭게 필터링\n",
        "    # ===================================================================\n",
        "    def get_legal_actions(self, *, for_worker: bool = False) -> list[tuple[str, int]]:\n",
        "        \"\"\"\n",
        "        환경 상태에서 취할 수 있는 합법-행동 목록을 반환한다.\n",
        "          · for_worker=False  →  Manager 레벨(=PROCEED 포함)\n",
        "          · for_worker=True   →  Worker 레벨\n",
        "        \"\"\"\n",
        "        actions: list[tuple[str, int]] = []\n",
        "\n",
        "        # ─────────────────── UNLOAD ───────────────────\n",
        "        # ① 현재 항구가 목적지인 차            → 무조건 허용\n",
        "        # ② 게이트를 막고 있는 차              → 일단 허용\n",
        "        for cid in list(self.cars_on_board):\n",
        "            car = self.cars[cid]\n",
        "            if car['dest'] == self.current_port:\n",
        "                actions.append(('UNLOAD', cid))\n",
        "            else:\n",
        "                # 게이트 통로를 막고 있으면 UNLOAD 후보\n",
        "                node_idx = self.car_locations.get(cid, -1)\n",
        "                if node_idx != -1:\n",
        "                    path = self._get_or_compute_path(node_idx, 0)\n",
        "                    # 자기 위치 제외, 경로 중간에 차가 없으면 곧바로 뺄 수 있음\n",
        "                    if path and all(self.node_status[n] == -1 for n in path[1:-1]):\n",
        "                        actions.append(('UNLOAD', cid))\n",
        "\n",
        "        # ─────────────────── LOAD ────────────────────\n",
        "        any_empty_spot = any(status == -1 for status in self.node_status[1:])\n",
        "        if any_empty_spot:\n",
        "            # 선적 대기 + 임시 하역 차량 모두 LOAD 후보\n",
        "            for c in self.cars:\n",
        "                cid = c['id']\n",
        "                if (c['origin'] == self.current_port and\n",
        "                    cid not in self.cars_on_board and\n",
        "                    cid not in self.temporarily_unloaded_cars):\n",
        "                    actions.append(('LOAD', cid))\n",
        "            for cid in self.temporarily_unloaded_cars:\n",
        "                actions.append(('LOAD', cid))\n",
        "\n",
        "        # ──────────────── RELOCATE_INTERNAL ───────────────\n",
        "        # “다른 빈 칸으로 실제로 옮길 자리”가 있는 차만 후보로 올린다\n",
        "        for cid in list(self.cars_on_board):\n",
        "            tgt, _ = self._find_best_internal_spot(cid)\n",
        "            if tgt != -1:\n",
        "                actions.append(('RELOCATE_INTERNAL', cid))\n",
        "\n",
        "        # ──────────────── Manager 전용 ───────────────\n",
        "        if not for_worker:\n",
        "            actions.append(('PROCEED_TO_NEXT_PORT', -1))\n",
        "\n",
        "        return actions\n",
        "\n",
        "\n",
        "    # ShipEnv 내 (기존 메소드들 바로 위/아래 아무 곳)\n",
        "    def _is_hard_blocker(self, node_idx: int) -> bool:\n",
        "        \"\"\"\n",
        "        게이트에서 가깝고 곧 빠질 차는 ‘소프트 블로커’로 보고\n",
        "        정말 치워야만 통과 가능한 차만 True.\n",
        "        \"\"\"\n",
        "        cid = self.node_status[node_idx]\n",
        "        if cid == -1:\n",
        "            return False\n",
        "\n",
        "        car = self.cars[cid]\n",
        "\n",
        "        # (1) 지금 하역 항구면 금방 내린다 → False\n",
        "        if car['dest'] == self.current_port:\n",
        "            return False\n",
        "        # (2) 게이트에서 두-세 칸 이내(manhattan depth <3)면 통과 대기열 → False\n",
        "        gate_depth = len(self._get_or_compute_path(node_idx, 0)) - 1\n",
        "        return gate_depth >= 3\n",
        "\n",
        "    # ==================================================================\n",
        "    #  ShipEnv._find_best_spot  ─  LOAD 시 새 자리 선정 (간단 버전)\n",
        "    # ==================================================================\n",
        "    def _find_best_spot(self, car_id: int) -> tuple[int, list]:\n",
        "        \"\"\"\n",
        "        선적할 차량을 깊이-단조 규칙과 통로 청정도에 따라 가장\n",
        "        좋은 빈 칸에 배치한다.  실패 시 (-1, []) 반환.\n",
        "        \"\"\"\n",
        "        best = None\n",
        "        car_dest = self.cars[car_id]['dest']\n",
        "\n",
        "        earlier_depths = [\n",
        "            self._gate_depth(self.car_locations[c])\n",
        "            for c in self.cars_on_board\n",
        "            if self.cars[c]['dest'] < car_dest\n",
        "        ]\n",
        "        max_earlier = max(earlier_depths) if earlier_depths else -1\n",
        "\n",
        "        later_depths = [\n",
        "            self._gate_depth(self.car_locations[c])\n",
        "            for c in self.cars_on_board\n",
        "            if self.cars[c]['dest'] > car_dest\n",
        "        ]\n",
        "        min_later = min(later_depths) if later_depths else 10**6\n",
        "\n",
        "        for spot in range(1, self.num_nodes):\n",
        "            if self.node_status[spot] != -1:\n",
        "                continue\n",
        "            depth = self._gate_depth(spot)\n",
        "            if not (max_earlier < depth <= min_later):\n",
        "                continue      # 깊이 단조 위반\n",
        "\n",
        "            path = self._get_or_compute_path(0, spot)\n",
        "            if not path:\n",
        "                continue\n",
        "            inner = path[1:]\n",
        "\n",
        "            # 통로에 다른 차가 있으면 건너뜀\n",
        "            if any(self.node_status[n] != -1 for n in inner):\n",
        "                continue\n",
        "\n",
        "            score = depth + random.random() * 1e-3  # 가장 얕은 거리 우선\n",
        "            if best is None or score < best[0]:\n",
        "                best = (score, spot, path)\n",
        "\n",
        "        return (-1, []) if best is None else (best[1], best[2])\n",
        "\n",
        "\n",
        "    # ======================================================================\n",
        "    # ➊     # ➌ ShipEnv._find_best_internal_spot (정리·단조 조건 포함)\n",
        "    # ======================================================================\n",
        "    def _find_best_internal_spot(self, car_id: int) -> tuple[int, list]:\n",
        "        start = self.car_locations.get(car_id, -1)\n",
        "        if start == -1:\n",
        "            return -1, []\n",
        "\n",
        "        # ── 깊이 단조 기본값 계산 ────────────────────────────\n",
        "        cur_depth = self._gate_depth(start)\n",
        "        later_depths = [\n",
        "            self._gate_depth(self.car_locations[c])\n",
        "            for c in self.cars_on_board\n",
        "            if self.cars[c]['dest'] > self.cars[car_id]['dest']\n",
        "        ]\n",
        "        min_later_depth = min(later_depths) if later_depths else 1_000_000\n",
        "\n",
        "        def evaluate_spot(spot: int) -> tuple[float, list] | None:\n",
        "            \"\"\"스코어가 낮을수록 좋은 자리, None → 불가\"\"\"\n",
        "            path = self._get_or_compute_path(start, spot)\n",
        "            if not path:\n",
        "                return None\n",
        "            inner = path[1:-1]\n",
        "\n",
        "            # 블로커·단조 조건\n",
        "            if any(self._is_hard_blocker(n) for n in inner):\n",
        "                return None\n",
        "            if any(self.node_status[n] != -1 for n in inner):\n",
        "                return None\n",
        "            depth = len(path) - 1\n",
        "            if depth > min_later_depth:      # 단조 위반\n",
        "                return None\n",
        "\n",
        "            # 스코어링: 거리 + 게이트 비우기 효과\n",
        "            unblock_gain = sum(\n",
        "                1 for cid in self.cars_on_board\n",
        "                if self.cars[cid]['dest'] == self.current_port and\n",
        "                   self.car_locations[cid] in inner\n",
        "            )\n",
        "            score = 0.5 * depth - 0.8 * unblock_gain + random.random() * 1e-3\n",
        "            return score, path\n",
        "\n",
        "        best = None\n",
        "        for spot in range(1, self.num_nodes):\n",
        "            if self.node_status[spot] != -1:\n",
        "                continue\n",
        "            res = evaluate_spot(spot)\n",
        "            if res and (best is None or res[0] < best[0]):\n",
        "                best = (res[0], spot, res[1])\n",
        "\n",
        "        if best:\n",
        "            _, spot, path = best\n",
        "            return spot, path\n",
        "\n",
        "        # ── fallback: 가장 가까운 빈 칸 ──────────────────────\n",
        "        cand = [\n",
        "            (len(self._get_or_compute_path(start, s)) - 1, s)\n",
        "            for s in range(1, self.num_nodes)\n",
        "            if self.node_status[s] == -1 and self._get_or_compute_path(start, s)\n",
        "        ]\n",
        "        if cand:\n",
        "            cand.sort()\n",
        "            spot = cand[0][1]\n",
        "            path = self._get_or_compute_path(start, spot)\n",
        "            return spot, path\n",
        "\n",
        "        return -1, []\n",
        "\n",
        "\n",
        "    def step(\n",
        "        self,\n",
        "        action: tuple[str, int]\n",
        "    ) -> tuple[Data, float, float, bool]:\n",
        "        \"\"\"\n",
        "        ShipEnv 1-step 시뮬레이션.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        action : (action_type:str, car_id:int)\n",
        "            * action_type ∈ {'LOAD','UNLOAD','RELOCATE_INTERNAL'}\n",
        "            * car_id == -1 는 PORT 단에서만 쓰는 dummy 값\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        state  : Data   ─ PyG 그래프(노드특성·글로벌특성 포함)\n",
        "        reward : float  ─ worker-level scalar reward\n",
        "        cost   : float  ─ pure path-moving cost (고정+거리)\n",
        "        done   : bool   ─ 항로가 끝나면 True\n",
        "        \"\"\"\n",
        "        act_type, cid = action\n",
        "        cost = 0.0\n",
        "        event_rew = 0.0                       # (extrinsic + intrinsic)\n",
        "\n",
        "        # ────────────────────────────────────────────────────\n",
        "        # 0) 반복/핑퐁 패널티  (cid == -1 이면 생략)\n",
        "        # ────────────────────────────────────────────────────\n",
        "        if cid != -1:\n",
        "            prev = self.last_car_action.get(cid)\n",
        "\n",
        "            # (a) 같은 행동 타입 연속            → 살짝 -0.1\n",
        "            if prev == act_type:\n",
        "                event_rew -= 0.5\n",
        "\n",
        "            # (b) 직전 UNLOAD → 곧바로 LOAD     → 강하게 -5.0\n",
        "            elif prev == 'UNLOAD' and act_type == 'LOAD':\n",
        "                event_rew -= 5.0\n",
        "\n",
        "            # 기록 갱신\n",
        "            self.last_car_action[cid] = act_type\n",
        "\n",
        "        # ────────────────────────────────────────────────────\n",
        "        # 1) LOAD\n",
        "        # ────────────────────────────────────────────────────\n",
        "        if act_type == 'LOAD':\n",
        "            tgt, path = self._find_best_spot(cid)\n",
        "            if tgt == -1:                          # 자리가 없음\n",
        "                self.load_fail_streak += 1         # 누적 +1\n",
        "                dyn_pen = -1.0 * (1 + 0.3 * self.load_fail_streak)\n",
        "                return self._get_state(), dyn_pen, 0.0, False\n",
        "            else:\n",
        "                # 자리를 찾았으면 streak 초기화\n",
        "                self.load_fail_streak = 0\n",
        "\n",
        "            cost += self._move_cost(len(path)-1, is_reloc=False)\n",
        "            self.node_status[tgt] = cid\n",
        "            self.car_locations[cid] = tgt\n",
        "            self.cars_on_board.add(cid)\n",
        "\n",
        "            if cid in self.temporarily_unloaded_cars:\n",
        "                self.temporarily_unloaded_cars.remove(cid)\n",
        "            else:\n",
        "                event_rew += 0.1   # “신규 선적” 소정 보상\n",
        "\n",
        "        # ────────────────────────────────────────────────────\n",
        "        # 2) UNLOAD\n",
        "        # ────────────────────────────────────────────────────\n",
        "        elif act_type == 'UNLOAD':\n",
        "            start = self.car_locations.get(cid)\n",
        "            if start is None:      # 차가 실제로 안 있다?\n",
        "                return self._get_state(), -1.0, 0.0, False\n",
        "\n",
        "            path = self._get_or_compute_path(start, 0)\n",
        "            cost += self._move_cost(len(path)-1, is_reloc=False)\n",
        "\n",
        "            # 선박 상태 업데이트\n",
        "            self.node_status[start] = -1\n",
        "            self.cars_on_board.remove(cid)\n",
        "            self.car_locations.pop(cid, None)\n",
        "\n",
        "            car = self.cars[cid]\n",
        "            if car['dest'] == self.current_port:          # 목적지 도착\n",
        "                self.delivered_cars.add(cid)\n",
        "                event_rew += 1.0\n",
        "            else:                                         # 임시 하역\n",
        "                self.temporarily_unloaded_cars.add(cid)\n",
        "                self.relocations_this_episode += 1\n",
        "                event_rew -= 0.1\n",
        "\n",
        "        # ────────────────────────────────────────────────────\n",
        "        # 3) RELOCATE_INTERNAL\n",
        "        # ────────────────────────────────────────────────────\n",
        "        elif act_type == 'RELOCATE_INTERNAL':\n",
        "            tgt, path = self._find_best_internal_spot(cid)\n",
        "            if tgt == -1:          # 자리 못 찾음\n",
        "                event_rew -= 0.2\n",
        "            else:\n",
        "                cost += self._move_cost(len(path)-1, is_reloc=True)\n",
        "                src = self.car_locations.get(cid)\n",
        "                if src is not None:\n",
        "                    self.node_status[src] = -1\n",
        "                self.node_status[tgt] = cid\n",
        "                self.car_locations[cid] = tgt\n",
        "                self.relocations_this_episode += 1\n",
        "                event_rew -= 0.02\n",
        "\n",
        "        # ────────────────────────────────────────────────────\n",
        "        # 4) Reward, done flag, state 반환\n",
        "        # ────────────────────────────────────────────────────\n",
        "        # ▶▶  COST 가 곧 음의 보상  ◀◀\n",
        "\n",
        "        reward = event_rew - cost        # cost 자체(양수)를 그대로 음보상\n",
        "        done = (self.current_port >= self.num_ports) and (len(self.cars_on_board) == 0)\n",
        "\n",
        "        return self._get_state(), reward, cost, done\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 섹션 3: 계층적 환경 Wrapper (리팩토링 버전)\n",
        "# ==============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.data import Batch\n",
        "\n",
        "class HierarchicalEnvWrapper:\n",
        "    def __init__(self, problem_data: dict, max_num_ports: int,\n",
        "                 worker_agent, config):\n",
        "        self.problem_data     = problem_data\n",
        "        self.max_num_ports    = max_num_ports\n",
        "        self.worker_agent     = worker_agent\n",
        "        self.config           = config\n",
        "        self.last_manager_action = None\n",
        "\n",
        "        # ── 핵심 객체 ─────────────────────────────\n",
        "        self.ship_env = ShipEnv(problem_data, max_num_ports)\n",
        "\n",
        "        # ★ 체류 스텝 카운터 초기화\n",
        "        self.steps_on_port = 0          # ← 없던 필드\n",
        "\n",
        "        # Manager action 인코딩\n",
        "        self.manager_action_map = {\n",
        "            0: 'CLEAR_BLOCKERS',\n",
        "            1: 'FINISH_UNLOAD',\n",
        "            2: 'FINISH_LOAD',\n",
        "            3: 'CLEAR_TEMP',\n",
        "            4: 'PROCEED_TO_NEXT_PORT'\n",
        "        }\n",
        "        self.goal_embedding = nn.Embedding(\n",
        "            self.config.MANAGER_ACTION_DIM,\n",
        "            self.config.GOAL_EMBED_DIM\n",
        "        ).to(self.worker_agent.device)\n",
        "\n",
        "    def _calculate_potential(self) -> float:\n",
        "        s = self.ship_env\n",
        "        # ① 도착지 ↑ 오름차순으로 현재 선내 차량을 정렬\n",
        "        ordered = sorted(\n",
        "            [(self.ship_env.cars[cid]['dest'], cid) for cid in s.cars_on_board]\n",
        "        )\n",
        "\n",
        "        rank = 0\n",
        "        surplus = 0\n",
        "        for dest, cid in ordered:\n",
        "            depth = s._gate_depth(s.car_locations[cid])\n",
        "            surplus += max(0, depth - rank)   # ‘이상적으로 필요한 거리(rank)’를 초과한 만큼\n",
        "            rank += 1\n",
        "\n",
        "        # 임시 하역은 깊이에 0.5칸짜리 불이익을 주면 충분\n",
        "        surplus += 0.5 * len(s.temporarily_unloaded_cars)\n",
        "\n",
        "        # **값이 작을수록 좋은 상태** → 그대로 음수 부호 안 써도 됨\n",
        "        return surplus\n",
        "\n",
        "\n",
        "    def reset(self, prob_data=None):\n",
        "        if prob_data is not None:\n",
        "            self.problem_data = prob_data\n",
        "            self.ship_env     = ShipEnv(self.problem_data, self.max_num_ports)\n",
        "        else:\n",
        "            self.ship_env.reset()\n",
        "\n",
        "        # ★ 포트 체류 카운터도 항상 리셋\n",
        "        self.steps_on_port       = 0\n",
        "        self.last_manager_action = None\n",
        "        return self._get_manager_state()\n",
        "\n",
        "\n",
        "    def _get_manager_state(self):\n",
        "        s = self.ship_env\n",
        "        total_slots = s.num_nodes - 1 if s.num_nodes > 1 else 1\n",
        "        port_norm = s.current_port / s.num_ports\n",
        "        free_slots_norm = sum(1 for n in s.node_status[1:] if n == -1) / total_slots\n",
        "        waiting_to_load = len([c for c in s.cars if c['origin']==s.current_port and c['id'] not in s.cars_on_board and c['id'] not in s.temporarily_unloaded_cars])\n",
        "        due_to_unload = len([c for c in s.cars if c['id'] in s.cars_on_board and c['dest']==s.current_port])\n",
        "        on_board_dests = [s.cars[cid]['dest'] for cid in s.cars_on_board]\n",
        "        avg_dest_dist = (sum(d - s.current_port for d in on_board_dests)/len(on_board_dests)) if on_board_dests else 0.0\n",
        "\n",
        "        return torch.tensor([\n",
        "            port_norm,\n",
        "            free_slots_norm,\n",
        "            waiting_to_load / s.total_cars,\n",
        "            due_to_unload / s.total_cars,\n",
        "            len(s.temporarily_unloaded_cars) / s.total_cars,\n",
        "            avg_dest_dist / s.num_ports\n",
        "        ], dtype=torch.float)\n",
        "\n",
        "    def _is_goal_achieved(self, goal_str: str) -> bool:\n",
        "        \"\"\"\n",
        "        각 목표(goal_str)가 충족되었는지 판단한다.\n",
        "\n",
        "        CLEAR_BLOCKERS = ① ‘하역 대상 차가 없음’\n",
        "                       + ② 게이트(노드 0)→각 차량까지의 **통로**에\n",
        "                            다른 차가 전혀 없을 때(True)\n",
        "\n",
        "        ── 수정 핵심 ─────────────────────────────────────────────\n",
        "        기존 path_blocked()는 자신의 자리(node_idx)도\n",
        "        ‘막힌 노드’로 계산해 항상 True가 되었다.\n",
        "        ⇒ 마지막 노드(자기 자리)는 제외하고 검사한다.\n",
        "        ────────────────────────────────────────────────────────\n",
        "        \"\"\"\n",
        "        s = self.ship_env\n",
        "\n",
        "        # ① 현재 항구에서 하역해야 할 차가 남아 있는가?\n",
        "        has_due_to_unload = any(\n",
        "            c['dest'] == s.current_port and c['id'] in s.cars_on_board\n",
        "            for c in s.cars\n",
        "        )\n",
        "\n",
        "        # ② 게이트 → 각 차량으로 이어지는 통로에 ‘다른’ 차가 있는가?\n",
        "        def path_blocked() -> bool:\n",
        "            for cid in s.cars_on_board:\n",
        "                node_idx = s.car_locations[cid]\n",
        "                path = s._get_or_compute_path(0, node_idx)\n",
        "                if not path: continue\n",
        "                # 도착 항구가 “현재 항구보다 빠른 차”가 경로 위에 있으면 블로킹\n",
        "                for n in path[1:-1]:\n",
        "                    other = s.node_status[n]\n",
        "                    if other == -1: continue\n",
        "                    if s.cars[other]['dest'] < s.current_port:   # 핵심 추가\n",
        "                        return True\n",
        "            return False\n",
        "\n",
        "\n",
        "        if goal_str == 'FINISH_UNLOAD':\n",
        "            return not has_due_to_unload\n",
        "\n",
        "        elif goal_str == 'CLEAR_BLOCKERS':\n",
        "            return (not has_due_to_unload) and (not path_blocked())\n",
        "\n",
        "        elif goal_str == 'FINISH_LOAD':\n",
        "            return not any(\n",
        "                c['origin'] == s.current_port and\n",
        "                c['id'] not in s.cars_on_board and\n",
        "                c['id'] not in s.temporarily_unloaded_cars\n",
        "                for c in s.cars\n",
        "            )\n",
        "\n",
        "        elif goal_str == 'CLEAR_TEMP':\n",
        "            return len(s.temporarily_unloaded_cars) == 0\n",
        "\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "\n",
        "\n",
        "    # ======================================================================\n",
        "    #  HierarchicalEnvWrapper.step  ―  최종 교체본\n",
        "    # ======================================================================\n",
        "    def step(self, manager_action_idx: int, *, greedy_worker: bool = False):\n",
        "        \"\"\"\n",
        "        Manager 액션을 받아 Worker 롤아웃까지 수행한 뒤\n",
        "        (다음 Manager 상태, Manager 보상, done, info) 반환\n",
        "        \"\"\"\n",
        "        # ────────────────────────── 기본 세팅 ──────────────────────────\n",
        "        goal_str         = self.manager_action_map[manager_action_idx]\n",
        "        goal_embed       = self.goal_embedding(\n",
        "            torch.tensor([manager_action_idx], device=self.worker_agent.device)\n",
        "        )\n",
        "        potential_before = self._calculate_potential()\n",
        "\n",
        "        # 항구 체류 스텝 카운터  ── NEW ──────────────────────────────────\n",
        "        self.steps_on_port += 1\n",
        "\n",
        "        # Worker 통계\n",
        "        worker_steps         = 0\n",
        "        total_cost           = 0.0\n",
        "        total_worker_reward  = 0.0\n",
        "        no_progress          = 0\n",
        "        no_progress_trigger  = False\n",
        "\n",
        "        # Storage (학습 모드일 때만)\n",
        "        worker_storage = None\n",
        "        if (goal_str != 'PROCEED_TO_NEXT_PORT') and (not greedy_worker):\n",
        "            worker_storage = PPOStorage(\n",
        "                self.config.WORKER_NUM_STEPS_PER_UPDATE,\n",
        "                (2,),                         # [action_type, car_id]\n",
        "                self.worker_agent.device\n",
        "            )\n",
        "\n",
        "        # ──────────────────────── 디버그·안정성 파라미터 ───────────────────────\n",
        "        BONUS_EVERY_N      = 100\n",
        "        PERIODIC_EVERY_N   = 150\n",
        "        # STREAK_LIMIT       = 15             # ★ “동일 행동·차량” 연속 허용치\n",
        "        EPS                = 1e-6\n",
        "        last_dbg_key       = None\n",
        "        bonus_skip_counter = 0\n",
        "        same_action_streak = 0              # ★ streak 카운터\n",
        "        last_worker_key    = None           # ★ (act_type, car_id)\n",
        "        # ────────────────────────────────────────────────────────────────────\n",
        "\n",
        "        # ─────────────────────────── Worker 루프 ───────────────────────────\n",
        "        if goal_str != 'PROCEED_TO_NEXT_PORT':\n",
        "            batch_graph = Batch.from_data_list(\n",
        "                [self.ship_env._get_state()]\n",
        "            ).to(self.worker_agent.device)\n",
        "\n",
        "            last_worker_key    = None\n",
        "            same_action_streak = 0        # streak 카운터\n",
        "\n",
        "            for i in range(self.config.WORKER_MAX_STEPS_PER_GOAL):\n",
        "                worker_steps = i + 1\n",
        "\n",
        "                # # 0-a) 동일 행동·차량 스트릭 초과   ── NEW ────────────────\n",
        "                # if same_action_streak >= STREAK_LIMIT:\n",
        "                #     logging.info(f\"[WRK] break — same action repeated {STREAK_LIMIT} times\")\n",
        "                #     no_progress_trigger = True\n",
        "                #     break\n",
        "\n",
        "                # 0-b) ‘진도 없음’ 한도 초과 시 탈출\n",
        "                if (goal_str == 'FINISH_LOAD') and (no_progress >= self.config.NO_PROGRESS_LIMIT):\n",
        "                    no_progress_trigger = True\n",
        "                    break\n",
        "\n",
        "                # 1) 목표 달성 여부\n",
        "                if self._is_goal_achieved(goal_str):\n",
        "                    break\n",
        "\n",
        "                # 2) 합법 액션\n",
        "                legal_actions = self.ship_env.get_legal_actions(for_worker=True)\n",
        "                if not legal_actions:\n",
        "                    break\n",
        "\n",
        "                # 3) 최신 그래프 덮어쓰기\n",
        "                tmp_state        = self.ship_env._get_state()\n",
        "                batch_graph.x    = tmp_state.x.to(batch_graph.x.device)\n",
        "                batch_graph.global_features = tmp_state.global_features.to(batch_graph.global_features.device)\n",
        "\n",
        "                # 4) 행동 선택\n",
        "                action, at, logp, ent, val = self.worker_agent.get_action_and_value(\n",
        "                    batch_graph, legal_actions, goal_embed, greedy=greedy_worker\n",
        "                )\n",
        "                action_type, car_id = action\n",
        "\n",
        "                # 5) Streak 업데이트  ── NEW ─────────────────────────────\n",
        "                worker_key = (action_type, car_id)\n",
        "                if worker_key == last_worker_key:\n",
        "                    same_action_streak += 1\n",
        "                else:\n",
        "                    same_action_streak = 1\n",
        "                last_worker_key = worker_key\n",
        "                # ───────────────────────────────────────────────────────\n",
        "\n",
        "                # 6) intrinsic 보너스 대상 플래그\n",
        "                is_temp_before = (car_id != -1) and (car_id in self.ship_env.temporarily_unloaded_cars)\n",
        "                is_due_before  = (car_id != -1) and (self.ship_env.cars[car_id]['dest'] == self.ship_env.current_port)\n",
        "\n",
        "                # 7) 환경 한 스텝\n",
        "                _, worker_reward, move_cost, overall_done = self.ship_env.step(action)\n",
        "                total_cost          += move_cost\n",
        "                intrinsic_reward     = 0.0\n",
        "                if goal_str == 'FINISH_UNLOAD' and action_type == 'UNLOAD' and is_due_before:\n",
        "                    intrinsic_reward += 0.8\n",
        "                elif goal_str == 'CLEAR_TEMP' and action_type == 'LOAD' and is_temp_before:\n",
        "                    intrinsic_reward += 0.4\n",
        "                worker_reward       += intrinsic_reward\n",
        "                total_worker_reward += worker_reward\n",
        "\n",
        "                # 8) no-progress 업데이트\n",
        "                load_succeeded = (action_type == 'LOAD') and (worker_reward > 0)\n",
        "                if goal_str == 'FINISH_LOAD' and load_succeeded:\n",
        "                    no_progress = 0\n",
        "                else:\n",
        "                    no_progress += 1\n",
        "\n",
        "                # 9) Storage 저장\n",
        "                if worker_storage is not None:\n",
        "                    worker_storage.add(tmp_state, at, logp, worker_reward, overall_done, val)\n",
        "\n",
        "                # 10) 디버그 로그 (… 생략, 기존과 동일) ----------------------\n",
        "\n",
        "                if overall_done:\n",
        "                    break\n",
        "\n",
        "            # --- Worker 루프 종료 후 streak 초기화 -----------------\n",
        "            same_action_streak = 0        # ← 다음 goal 로 이어지지 않게\n",
        "\n",
        "            # 11) PPO 업데이트\n",
        "            if worker_storage and worker_storage.step > 0 and not greedy_worker:\n",
        "                # ────────────────────────────────────────────────\n",
        "                # RuntimeError(\"… grad_fn …\") 방지용\n",
        "                # 롤아웃 때는 no-grad 로 돌았으므로,\n",
        "                # 업데이트 바로 직전에 grad 를 다시 켜 주는 한 줄.\n",
        "                # ────────────────────────────────────────────────\n",
        "                with torch.enable_grad():                     # ← 추가\n",
        "                    self.worker_agent.update(worker_storage,  # ← 기존 호출\n",
        "                                            goal_embed.detach())\n",
        "\n",
        "\n",
        "        # ───────────────────────── Manager 보상 계산 ─────────────────────────\n",
        "        goal_done    = self._is_goal_achieved(goal_str)\n",
        "        event_reward = 0.0\n",
        "\n",
        "        # 실패 패널티\n",
        "        if (worker_steps >= self.config.WORKER_MAX_STEPS_PER_GOAL) and (not goal_done):\n",
        "            penalty = (2 * self.config.TIMEOUT_PENALTY) if goal_str == 'CLEAR_BLOCKERS' else self.config.TIMEOUT_PENALTY\n",
        "            event_reward += penalty\n",
        "\n",
        "        # PBRS\n",
        "        potential_after = self._calculate_potential()\n",
        "        event_reward += self.config.SHAPING_REWARD_WEIGHT * (\n",
        "            self.config.MANAGER_GAMMA * potential_after - potential_before\n",
        "        )\n",
        "\n",
        "        # 항구 이동 명령 처리\n",
        "        if goal_str == 'PROCEED_TO_NEXT_PORT':\n",
        "            waiting = [c for c in self.ship_env.cars if c['origin']==self.ship_env.current_port\n",
        "                       and c['id'] not in self.ship_env.cars_on_board\n",
        "                       and c['id'] not in self.ship_env.temporarily_unloaded_cars]\n",
        "            can_go = not (waiting or self.ship_env.temporarily_unloaded_cars)\n",
        "            if can_go:\n",
        "                self.ship_env.current_port += 1\n",
        "                self.steps_on_port = 0            # ★ 체류 카운터 리셋\n",
        "                event_reward += 5.0\n",
        "                goal_done = True\n",
        "            else:\n",
        "                event_reward -= 10.0\n",
        "                goal_done = False\n",
        "\n",
        "        # ── 항구 체류 제한 패널티만 적용(강제 이동 X)  ── NEW ───────────────\n",
        "        MAX_STEP_PER_PORT = 4000\n",
        "        if self.steps_on_port > MAX_STEP_PER_PORT:\n",
        "            event_reward += self.config.TIMEOUT_PENALTY * 5   # 큰 패널티\n",
        "            logging.info(\"[MGR] Port-stay limit exceeded — penalty applied\")\n",
        "            self.steps_on_port = MAX_STEP_PER_PORT            # 더 증가 안 함\n",
        "        # ────────────────────────────────────────────────────────────────\n",
        "\n",
        "        # 최종 Manager reward\n",
        "        manager_reward  = event_reward - (total_cost * 0.001)\n",
        "        manager_reward -= self.config.STEP_PENALTY_WEIGHT * worker_steps\n",
        "        # if self.last_manager_action == manager_action_idx:\n",
        "        #     manager_reward += self.config.REPEAT_PENALTY\n",
        "        if no_progress_trigger:\n",
        "            manager_reward += self.config.NO_PROGRESS_PENALTY\n",
        "            goal_done = True              # ← 더 이상 같은 goal 반복 금지\n",
        "\n",
        "        self.last_manager_action = manager_action_idx\n",
        "\n",
        "        # 에피소드 종료 보너스/패널티\n",
        "        done = self.ship_env.current_port >= self.ship_env.num_ports\n",
        "        if done:\n",
        "            if len(self.ship_env.delivered_cars) == self.ship_env.total_cars:\n",
        "                manager_reward += 1000.0\n",
        "            else:\n",
        "                manager_reward -= (self.ship_env.total_cars - len(self.ship_env.delivered_cars)) * 10.0\n",
        "\n",
        "        # 다음 Manager state\n",
        "        next_state = self._get_manager_state()\n",
        "\n",
        "        info = {\n",
        "            'steps'              : worker_steps,\n",
        "            'goal'               : goal_str,\n",
        "            'success'            : goal_done,\n",
        "            'cost'               : total_cost,\n",
        "            'worker_total_reward': total_worker_reward,\n",
        "        }\n",
        "        return next_state, manager_reward, done, info\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 섹션 4: 에이전트 및 네트워크 (리팩토링 버전)\n",
        "# ==============================================================================\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from torch_geometric.data import Batch\n",
        "from torch_geometric.nn import GlobalAttention, global_mean_pool\n",
        "\n",
        "# --- PPO 알고리즘을 위한 경험 저장소 ---\n",
        "\n",
        "class PPOStorage:\n",
        "    \"\"\"PPO 학습을 위한 롤아웃(rollout) 데이터를 저장하고 관리하는 클래스.\"\"\"\n",
        "    def __init__(self, num_steps: int, action_shape: tuple, device: torch.device, state_shape: tuple = None, manager_action_dim: int = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_steps (int): 저장할 총 스텝 수.\n",
        "            action_shape (tuple): 행동의 형태.\n",
        "            device (torch.device): 데이터가 저장될 장치 (CPU 또는 CUDA).\n",
        "            state_shape (tuple, optional): 상태의 형태. 그래프 데이터가 아니면 지정.\n",
        "            manager_action_dim (int, optional): Manager의 경우, 액션 마스크를 저장하기 위해 필요.\n",
        "        \"\"\"\n",
        "        self.num_steps = num_steps\n",
        "        self.device = device\n",
        "        self.step = 0\n",
        "        self.is_graph_data = (state_shape is None)\n",
        "\n",
        "        # 데이터 저장 버퍼 초기화\n",
        "        if self.is_graph_data:\n",
        "            self.obs = [None] * num_steps\n",
        "        else:\n",
        "            self.obs = torch.zeros((num_steps,) + state_shape, device=device)\n",
        "\n",
        "        if isinstance(action_shape, int): action_shape = (action_shape,)\n",
        "        self.actions = torch.zeros((num_steps,) + action_shape, device=device, dtype=torch.long)\n",
        "        self.logprobs = torch.zeros(num_steps, device=device)\n",
        "        self.rewards = torch.zeros(num_steps, device=device)\n",
        "        self.dones = torch.zeros(num_steps, device=device)\n",
        "        self.values = torch.zeros(num_steps, device=device)\n",
        "\n",
        "        # [핵심 수정] Manager의 상태 의존적 액션 마스크를 저장할 공간\n",
        "        if manager_action_dim:\n",
        "            self.masks = torch.zeros((num_steps, manager_action_dim), device=device)\n",
        "        else:\n",
        "            self.masks = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.step = 0\n",
        "\n",
        "    def add(self, obs, action, logprob, reward, done, value, mask=None):\n",
        "        \"\"\"한 스텝의 경험 데이터를 저장합니다.\"\"\"\n",
        "        if self.step >= self.num_steps: return\n",
        "\n",
        "        if self.is_graph_data:\n",
        "            self.obs[self.step] = obs # GPU 메모리 절약을 위해 CPU에 저장\n",
        "        else:\n",
        "            self.obs[self.step].copy_(torch.as_tensor(obs, device=self.device))\n",
        "\n",
        "        self.actions[self.step] = action\n",
        "        self.logprobs[self.step] = logprob\n",
        "        self.rewards[self.step] = torch.tensor(reward, dtype=torch.float32)\n",
        "        self.dones[self.step] = torch.tensor(done, dtype=torch.float32)\n",
        "        self.values[self.step] = value.detach()\n",
        "\n",
        "        # [핵심 수정] Manager의 액션 마스크 저장\n",
        "        if self.masks is not None and mask is not None:\n",
        "            self.masks[self.step] = mask.detach()\n",
        "\n",
        "        self.step += 1\n",
        "\n",
        "    def is_full(self) -> bool:\n",
        "        return self.step >= self.num_steps\n",
        "\n",
        "    def compute_returns_and_advantages(self, last_value: torch.Tensor, gamma: float, gae_lambda: float):\n",
        "        \"\"\"GAE를 사용하여 보상(Return)과 어드밴티지(Advantage)를 계산합니다.\"\"\"\n",
        "        advantages = torch.zeros_like(self.rewards).to(self.device)\n",
        "        last_gae = 0.0\n",
        "        for t in reversed(range(self.num_steps)):\n",
        "            next_non_terminal = 1.0 - self.dones[t]\n",
        "            next_value = last_value if t == self.num_steps - 1 else self.values[t + 1]\n",
        "\n",
        "            delta = self.rewards[t] + gamma * next_value * next_non_terminal - self.values[t]\n",
        "            last_gae = delta + gamma * gae_lambda * next_non_terminal * last_gae\n",
        "            advantages[t] = last_gae\n",
        "\n",
        "        # 어드밴티지를 이용해 최종 Return 계산\n",
        "        self.returns = advantages + self.values\n",
        "        # 어드밴티지 정규화 (학습 안정화)\n",
        "        self.advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "\n",
        "\n",
        "# --- 신경망 모델 정의 ---\n",
        "# WorkerNetwork 클래스 전체\n",
        "class WorkerNetwork(nn.Module):\n",
        "    \"\"\"Worker 에이전트의 정책 및 가치 신경망\"\"\"\n",
        "    def __init__(self, node_feature_size: int, global_feature_size: int, max_cars: int, num_nodes: int, config: Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        embed_dim = self.config.GNN_EMBED_DIM\n",
        "        self.node_input_proj = nn.Linear(node_feature_size, embed_dim)\n",
        "        self.positional_encoding = nn.Embedding(num_nodes, embed_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4, dim_feedforward=embed_dim*4, dropout=0.1, activation='gelu', batch_first=True, norm_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
        "        self.att_pool = GlobalAttention(gate_nn=nn.Sequential(nn.Linear(embed_dim, embed_dim // 2), nn.ReLU(), nn.Linear(embed_dim // 2, 1)))\n",
        "\n",
        "        mlp_input_dim = embed_dim * 2 + global_feature_size + self.config.GOAL_EMBED_DIM\n",
        "        self.mlp = nn.Sequential(nn.Linear(mlp_input_dim, 512), nn.GELU(), nn.Linear(512, 256), nn.GELU())\n",
        "        self.actor_type_head = nn.Linear(256, 3)\n",
        "        self.actor_load_head = nn.Linear(256, max_cars)\n",
        "        self.actor_unload_head = nn.Linear(256, max_cars)\n",
        "        self.actor_relocate_head = nn.Linear(256, max_cars)\n",
        "        self.critic_head = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, data, goal_embedding):\n",
        "        current_device = goal_embedding.device\n",
        "        x, global_feats, batch_index = data.x.to(current_device), data.global_features.to(current_device), data.batch.to(current_device)\n",
        "        ptr = data.ptr.to(current_device) if hasattr(data, 'ptr') and data.ptr is not None else torch.tensor([0, x.size(0)], device=current_device)\n",
        "\n",
        "        node_embeddings = self.node_input_proj(x)\n",
        "        pos_enc_list = [self.positional_encoding(torch.arange(ptr[i+1] - ptr[i], device=current_device)) for i in range(len(ptr)-1)]\n",
        "        pos_enc = torch.cat(pos_enc_list) if pos_enc_list else torch.empty(0, self.config.GNN_EMBED_DIM, device=current_device)\n",
        "        if node_embeddings.size(0) == pos_enc.size(0): node_embeddings += pos_enc\n",
        "\n",
        "        if len(ptr) > 1 and x.shape[0] > 0: max_len = (ptr[1:] - ptr[:-1]).max().item()\n",
        "        elif x.shape[0] > 0: max_len = x.shape[0]\n",
        "        else: max_len = 1\n",
        "\n",
        "        padded_x, masks_list = [], []\n",
        "        if len(ptr) > 1:\n",
        "            for i in range(len(ptr) - 1):\n",
        "                start, end = ptr[i], ptr[i+1]\n",
        "                graph_len, current_nodes = end - start, node_embeddings[start:end]\n",
        "                pad = torch.zeros(max_len - graph_len, self.config.GNN_EMBED_DIM, device=current_device)\n",
        "                padded_x.append(torch.cat([current_nodes, pad]))\n",
        "                mask = torch.ones(max_len, dtype=torch.bool, device=current_device)\n",
        "                mask[:graph_len] = False\n",
        "                masks_list.append(mask)\n",
        "        else:\n",
        "            padded_x.append(node_embeddings)\n",
        "            masks_list.append(torch.zeros(node_embeddings.shape[0], dtype=torch.bool, device=current_device))\n",
        "\n",
        "        padded_x, attention_mask = torch.stack(padded_x), torch.stack(masks_list)\n",
        "        transformer_out = self.transformer_encoder(padded_x, src_key_padding_mask=attention_mask)\n",
        "        transformer_out_flat = transformer_out[~attention_mask]\n",
        "\n",
        "        graph_emb_mean = global_mean_pool(transformer_out_flat, batch_index)\n",
        "        graph_emb_att = self.att_pool(transformer_out_flat, batch_index)\n",
        "        graph_emb = torch.cat([graph_emb_mean, graph_emb_att], dim=1)\n",
        "\n",
        "        if goal_embedding.shape[0] != graph_emb.shape[0]:\n",
        "            goal_embedding = goal_embedding.expand(graph_emb.shape[0], -1)\n",
        "\n",
        "        combined_features = torch.cat([graph_emb, global_feats, goal_embedding], dim=1)\n",
        "        final_features = self.mlp(combined_features)\n",
        "\n",
        "        # [수정] 반환 값에 graph_emb 추가\n",
        "        return (self.actor_type_head(final_features), self.actor_load_head(final_features),\n",
        "                self.actor_unload_head(final_features), self.actor_relocate_head(final_features),\n",
        "                self.critic_head(final_features).squeeze(-1), graph_emb)\n",
        "\n",
        "\n",
        "\n",
        "class ManagerNetwork(nn.Module):\n",
        "    def __init__(self, state_dim: int, action_dim: int, num_layers: int = 2, nhead: int = 4):\n",
        "        super().__init__()\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.action_embed = nn.Embedding(action_dim, 32)\n",
        "\n",
        "        self.input_proj = nn.Linear(state_dim + 32, 128)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=128,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=256,\n",
        "            batch_first=True,\n",
        "            activation='gelu'\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.actor_head = nn.Linear(128, action_dim)\n",
        "        self.critic_head = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, prev_action_idx: torch.LongTensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state: [B, state_dim]\n",
        "            prev_action_idx: [B]\n",
        "        Returns:\n",
        "            logits: [B, action_dim]\n",
        "            value:  [B]\n",
        "        \"\"\"\n",
        "        a_emb = self.action_embed(prev_action_idx)            # [B, 32]\n",
        "        x = torch.cat([state, a_emb], dim=-1)                 # [B, state_dim + 32]\n",
        "        x = self.input_proj(x).unsqueeze(1)                   # [B, 1, 128]\n",
        "\n",
        "        encoded = self.encoder(x)                             # [B, 1, 128]\n",
        "        h = encoded.squeeze(1)                                # [B, 128]\n",
        "\n",
        "        logits = self.actor_head(h)                           # [B, action_dim]\n",
        "        value  = self.critic_head(h).squeeze(-1)              # [B]\n",
        "        return logits, value\n",
        "\n",
        "# --- 에이전트 클래스 정의 ---\n",
        "\n",
        "class ManagerAgent:\n",
        "    \"\"\"상위 레벨의 목표를 결정하는 Manager 에이전트.\"\"\"\n",
        "    def __init__(self, config: Config, device: torch.device, env_wrapper: HierarchicalEnvWrapper):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.env_wrapper = env_wrapper # 현재 환경 상태에 접근하여 액션 마스킹\n",
        "        self.action_map = env_wrapper.manager_action_map\n",
        "        self.type_to_idx = {v: k for k, v in self.env_wrapper.manager_action_map.items()}\n",
        "        self.idx_to_type = {v: k for k, v in self.type_to_idx.items()}\n",
        "\n",
        "        self.network = ManagerNetwork(config.MANAGER_STATE_DIM, config.MANAGER_ACTION_DIM).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.network.parameters(), lr=config.MANAGER_LR, eps=1e-5)\n",
        "\n",
        "\n",
        "    def _build_action_mask(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        달성된 goal, 혹은 물리적으로 불가능한 goal의 로짓을 -1e9 로 내려서\n",
        "        softmax 확률 0 이 되게 만든다.\n",
        "        \"\"\"\n",
        "        mask = torch.zeros(1, self.config.MANAGER_ACTION_DIM, device=self.device)\n",
        "        s_env = self.env_wrapper.ship_env # ship_env에 더 쉽게 접근\n",
        "\n",
        "        # 1. 이미 달성된 목표 마스킹\n",
        "        for idx, goal in self.env_wrapper.manager_action_map.items():\n",
        "            if self.env_wrapper._is_goal_achieved(goal):\n",
        "                mask[0, idx] = -1e9\n",
        "\n",
        "        # 💡 --- [핵심 추가] --- 💡\n",
        "        # 2. 'PROCEED_TO_NEXT_PORT'가 불가능한 조건일 때 마스킹\n",
        "        #   - 항구에 내려야 할 차가 있거나, 임시로 내린 차가 있으면 PROCEED 불가\n",
        "        waiting_cars = any(c['origin'] == s_env.current_port and c['id'] not in s_env.cars_on_board and c['id'] not in s_env.temporarily_unloaded_cars for c in s_env.cars)\n",
        "        has_temp_unloaded = bool(s_env.temporarily_unloaded_cars)\n",
        "\n",
        "        if waiting_cars or has_temp_unloaded:\n",
        "            proceed_idx = self.type_to_idx['PROCEED_TO_NEXT_PORT']\n",
        "            mask[0, proceed_idx] = -1e9\n",
        "        # 💡 --- [수정 완료] --- 💡\n",
        "        # 로그에 action 0-4 마스크 상태가 찍힐 것. 전부 -1e9 (금지)로 나오면 마스킹 오류.\n",
        "        logging.debug(f\"[MASK] allowed={[(i, m.item()==0) for i,m in enumerate(mask[0])]}  temp=# {len(s_env.temporarily_unloaded_cars)}  wait-load? {waiting_cars}\")\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def get_action_and_value(\n",
        "        self,\n",
        "        state: torch.Tensor,\n",
        "        legal_actions: list[tuple[str,int]],\n",
        "        prev_action_idx: torch.LongTensor,\n",
        "        greedy: bool = False\n",
        "    ) -> tuple: # [수정] 반환 타입 튜플로 명시\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state: [state_dim] 크기의 텐서\n",
        "            legal_actions: 가능한 행동 리스트 (예: [(\"FINISH_LOAD\",-1), …])\n",
        "            prev_action_idx: 이전에 선택한 매니저 액션 인덱스 ([1]-shape LongTensor)\n",
        "            greedy: True면 탐욕적으로(max) 선택\n",
        "\n",
        "        Returns:\n",
        "            [greedy=False] (action_tensor, logp, ent, value, mask)\n",
        "            - action_tensor (Tensor): 학습용 액션 텐서 ([1]-shape LongTensor)\n",
        "            - logp (Tensor): 선택 확률의 로그값\n",
        "            - ent (Tensor): 선택 분포의 엔트로피\n",
        "            - value (Tensor): 상태 가치 추정 (스칼라)\n",
        "            - mask (Tensor): PPO 업데이트에 사용할 상태 의존적 마스크\n",
        "\n",
        "            [greedy=True] (action_idx, None, None, None, None)\n",
        "            - action_idx (int): 선택된 매니저 액션 인덱스 (0~4)\n",
        "        \"\"\"\n",
        "        # 1) 평가 모드\n",
        "        self.network.eval()\n",
        "        with torch.no_grad():\n",
        "            # 2) 배치 차원 추가: [state_dim] → [1, state_dim]\n",
        "            batch_state = state.unsqueeze(0).to(self.device)\n",
        "\n",
        "            # 3) 달성 불가능하거나 이미 달성된 목표 로짓 마스킹\n",
        "            mask = self._build_action_mask()[0]          # [action_dim]\n",
        "\n",
        "            # 4) 네트워크 호출: logits [1,action_dim], value [1,]\n",
        "            logits, value = self.network(batch_state, prev_action_idx)\n",
        "            logits = logits[0] + mask                    # [action_dim]\n",
        "\n",
        "            # 5) legal_actions 에 따라 가능한 목표만 남기기\n",
        "            allowed = {act for act, _ in legal_actions}\n",
        "            type_mask = torch.full_like(logits, -1e9)\n",
        "            for idx, t_str in self.idx_to_type.items():\n",
        "                if t_str in allowed:\n",
        "                    type_mask[idx] = 0.0\n",
        "            masked_logits = logits + type_mask\n",
        "\n",
        "            dist = Categorical(logits=masked_logits)\n",
        "\n",
        "            # [핵심 수정] greedy(평가) 모드와 학습 모드의 반환 값을 분리하여 명확히 함\n",
        "            if greedy:\n",
        "                type_idx = torch.argmax(masked_logits)\n",
        "                self.network.train() # 모드 복귀\n",
        "                return type_idx.item(), None, None, None, None\n",
        "\n",
        "            # 학습 모드\n",
        "            type_idx = dist.sample() # 0-dim 텐서\n",
        "            logp     = dist.log_prob(type_idx)\n",
        "            ent      = dist.entropy()\n",
        "\n",
        "            # PPO 저장을 위해 (1,) 형태로 변환\n",
        "            action_tensor = type_idx.unsqueeze(0)\n",
        "\n",
        "        # 7) 학습 모드 복귀\n",
        "        self.network.train()\n",
        "\n",
        "        # [핵심 수정] 학습에 필요한 모든 값을 올바른 순서와 타입으로 반환\n",
        "        return action_tensor, logp, ent, value[0], mask\n",
        "\n",
        "\n",
        "    def update(self, storage: PPOStorage) -> dict:\n",
        "        \"\"\"저장된 경험 데이터를 사용하여 PPO 업데이트를 수행합니다.\"\"\"\n",
        "        # 마지막 상태의 가치 계산\n",
        "        with torch.no_grad():\n",
        "            # 마지막 상태는 storage.obs에 저장되어 있지만, 이전 액션이 필요함\n",
        "            # 이 로직은 간단하게 마지막 obs와 마지막 action을 가져와서 처리해야 함\n",
        "            last_obs = storage.obs[-1].unsqueeze(0).to(self.device)\n",
        "            last_prev_action = storage.actions[-2] if storage.step > 1 else torch.zeros(1, dtype=torch.long, device=self.device)\n",
        "\n",
        "            _, last_value = self.network(last_obs, last_prev_action)\n",
        "\n",
        "\n",
        "        # GAE와 Return 계산\n",
        "        storage.compute_returns_and_advantages(last_value, self.config.MANAGER_GAMMA, self.config.PPO_GAE_LAMBDA)\n",
        "\n",
        "        # 학습을 위한 데이터 준비\n",
        "        # obs는 PPOStorage에 tensor로 저장되도록 수정되었음을 가정\n",
        "        b_obs = torch.stack(list(storage.obs)).to(self.device)\n",
        "        b_actions = storage.actions.squeeze(-1).to(self.device)\n",
        "        b_logprobs = storage.logprobs.to(self.device)\n",
        "        b_returns = storage.returns.to(self.device)\n",
        "        b_advantages = storage.advantages.to(self.device)\n",
        "        b_masks = storage.masks.to(self.device)\n",
        "\n",
        "        # 이전 액션(b_prev_actions) 배치 생성\n",
        "        b_prev_actions = torch.cat(\n",
        "            (torch.zeros(1, dtype=torch.long, device=self.device), b_actions[:-1]),\n",
        "            dim=0\n",
        "        )\n",
        "\n",
        "        batch_size = storage.step\n",
        "        minibatch_size = max(1, batch_size // self.config.PPO_NUM_MINIBATCHES)\n",
        "\n",
        "        # PPO 업데이트 루프\n",
        "        for _ in range(self.config.PPO_UPDATE_EPOCHS):\n",
        "            perm_indices = np.random.permutation(batch_size)\n",
        "            for start in range(0, batch_size, minibatch_size):\n",
        "                end = start + minibatch_size\n",
        "                mb_idx = perm_indices[start:end]\n",
        "\n",
        "                # 미니배치 데이터 생성\n",
        "                mb_states = b_obs[mb_idx]\n",
        "                mb_actions = b_actions[mb_idx]\n",
        "                mb_logprobs = b_logprobs[mb_idx]\n",
        "                mb_returns = b_returns[mb_idx]\n",
        "                mb_advantages = b_advantages[mb_idx]\n",
        "                mb_masks = b_masks[mb_idx]\n",
        "                mb_prev_actions = b_prev_actions[mb_idx] # 이전 액션 미니배치\n",
        "\n",
        "                # 네트워크를 통해 새로운 로그확률, 가치, 엔트로피 계산\n",
        "                logits, new_values = self.network(mb_states, mb_prev_actions)\n",
        "\n",
        "                # 저장된 마스크를 적용하여 확률 분포 재계산\n",
        "                final_logits = logits + mb_masks\n",
        "                dist = Categorical(logits=final_logits)\n",
        "                new_logprobs = dist.log_prob(mb_actions)\n",
        "                entropy = dist.entropy()\n",
        "\n",
        "                # PPO 손실 계산\n",
        "                log_ratio = new_logprobs - mb_logprobs\n",
        "                ratio = log_ratio.exp()\n",
        "\n",
        "                surr1 = mb_advantages * ratio\n",
        "                surr2 = mb_advantages * torch.clamp(ratio, 1 - self.config.PPO_CLIP_COEF, 1 + self.config.PPO_CLIP_COEF)\n",
        "\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "                value_loss = F.mse_loss(new_values, mb_returns)\n",
        "\n",
        "                loss = (policy_loss +\n",
        "                        self.config.PPO_VALUE_COEF * value_loss -\n",
        "                        self.config.MANAGER_ENTROPY_COEF * entropy.mean())\n",
        "\n",
        "                # 역전파 및 최적화\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.network.parameters(), self.config.PPO_MAX_GRAD_NORM)\n",
        "                self.optimizer.step()\n",
        "\n",
        "        return {\n",
        "            \"policy_loss\": policy_loss.item(),\n",
        "            \"value_loss\": value_loss.item(),\n",
        "            \"entropy\": entropy.mean().item()\n",
        "        }\n",
        "\n",
        "class WorkerAgent:\n",
        "    \"\"\"하위 레벨의 세부 행동을 결정하는 Worker 에이전트.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        node_feature_size: int,\n",
        "        global_feature_size: int,\n",
        "        max_cars: int,\n",
        "        max_nodes: int,\n",
        "        config: Config,\n",
        "        device: torch.device\n",
        "    ):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        # v11 시그니처에 맞춰 Config 객체만 넘기도록 수정\n",
        "        self.network = WorkerNetwork(\n",
        "            node_feature_size,\n",
        "            global_feature_size,\n",
        "            max_cars,\n",
        "            max_nodes,\n",
        "            config\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.network.parameters(),\n",
        "            lr=config.WORKER_LR,\n",
        "            eps=1e-5\n",
        "        )\n",
        "\n",
        "        # 행동 타입 ↔ 인덱스 매핑\n",
        "        self.type_to_idx = {'LOAD': 0, 'UNLOAD': 1, 'RELOCATE_INTERNAL': 2}\n",
        "        self.idx_to_type = {v: k for k, v in self.type_to_idx.items()}\n",
        "\n",
        "\n",
        "    def get_action_and_value(\n",
        "                              self,\n",
        "                              batch_graph: Batch,           # ← Data가 아니라 pre-batched 그래프를 받습니다\n",
        "                              legal_actions: list,\n",
        "                              goal_embedding: torch.Tensor,\n",
        "                              greedy: bool = False\n",
        "                            ) -> tuple:\n",
        "\n",
        "        \"\"\"현재 상태와 목표에 따라 세부 행동을 결정합니다.\"\"\"\n",
        "        self.network.eval() # 평가 모드\n",
        "        with torch.no_grad():\n",
        "            type_logits, load_logits, unload_logits, relocate_logits, value, _ = self.network(batch_graph, goal_embedding)\n",
        "            type_logits, load_logits, unload_logits, relocate_logits = type_logits[0], load_logits[0], unload_logits[0], relocate_logits[0]\n",
        "\n",
        "            # --- 합법적인 행동(Legal Action)에 대한 마스킹 ---\n",
        "            # 각 행동 타입과 차량 ID에 대해 가능한 행동만 남깁니다.\n",
        "            allowed_types_str = {act for act, _ in legal_actions}\n",
        "            type_mask = torch.full_like(type_logits, -1e9)\n",
        "            for idx, t_str in self.idx_to_type.items():\n",
        "                if t_str in allowed_types_str:\n",
        "                    type_mask[idx] = 0.0\n",
        "\n",
        "            allowed_load_ids = {aid for (act, aid) in legal_actions if act == 'LOAD'}\n",
        "            load_mask = torch.full_like(load_logits, -1e9)\n",
        "            for idx in allowed_load_ids: load_mask[idx] = 0.0\n",
        "\n",
        "            allowed_unload_ids = {aid for (act, aid) in legal_actions if act == 'UNLOAD'}\n",
        "            unload_mask = torch.full_like(unload_logits, -1e9)\n",
        "            for idx in allowed_unload_ids: unload_mask[idx] = 0.0\n",
        "\n",
        "            allowed_relocate_ids = {aid for (act, aid) in legal_actions if act == 'RELOCATE_INTERNAL'}\n",
        "            relocate_mask = torch.full_like(relocate_logits, -1e9)\n",
        "            for idx in allowed_relocate_ids: relocate_mask[idx] = 0.0\n",
        "\n",
        "            # 마스크 적용\n",
        "            masked_type_logits = type_logits + type_mask\n",
        "\n",
        "            # --- 계층적 샘플링 (Hierarchical Sampling) ---\n",
        "            # 1. 행동 타입 결정\n",
        "            type_dist = Categorical(logits=masked_type_logits)\n",
        "            type_idx = torch.argmax(masked_type_logits) if greedy else type_dist.sample()\n",
        "            type_str = self.idx_to_type[int(type_idx.item())]\n",
        "\n",
        "            # 2. 결정된 타입에 따라 차량 ID 결정\n",
        "            car_idx_tensor = torch.tensor(-1, device=self.device, dtype=torch.long)\n",
        "            car_dist = None\n",
        "            if type_str == 'LOAD':\n",
        "                masked_logits = load_logits + load_mask\n",
        "                car_dist = Categorical(logits=masked_logits)\n",
        "                car_idx_tensor = torch.argmax(masked_logits) if greedy else car_dist.sample()\n",
        "            elif type_str == 'UNLOAD':\n",
        "                masked_logits = unload_logits + unload_mask\n",
        "                car_dist = Categorical(logits=masked_logits)\n",
        "                car_idx_tensor = torch.argmax(masked_logits) if greedy else car_dist.sample()\n",
        "            elif type_str == 'RELOCATE_INTERNAL':\n",
        "                masked_logits = relocate_logits + relocate_mask\n",
        "                car_dist = Categorical(logits=masked_logits)\n",
        "                car_idx_tensor = torch.argmax(masked_logits) if greedy else car_dist.sample()\n",
        "\n",
        "            action_tuple = (type_str, int(car_idx_tensor.item()))\n",
        "\n",
        "            # 학습 모드일 때만 로그 확률과 엔트로피 계산\n",
        "            if greedy:\n",
        "                return action_tuple, None, None, None, value.squeeze(0)\n",
        "\n",
        "            action_tensor = torch.tensor([type_idx.item(), car_idx_tensor.item()], device=self.device)\n",
        "            log_prob_type = type_dist.log_prob(type_idx)\n",
        "            ent_type = type_dist.entropy()\n",
        "\n",
        "            log_prob_car = torch.tensor(0.0, device=self.device)\n",
        "            ent_car = torch.tensor(0.0, device=self.device)\n",
        "            if car_dist is not None:\n",
        "                log_prob_car = car_dist.log_prob(car_idx_tensor)\n",
        "                ent_car = car_dist.entropy()\n",
        "\n",
        "            total_log_prob = log_prob_type + log_prob_car\n",
        "            total_entropy = ent_type + ent_car\n",
        "\n",
        "        self.network.train() # 학습 모드로 전환\n",
        "        return action_tuple, action_tensor, total_log_prob, total_entropy, value.squeeze(0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _traj_success(traj):\n",
        "        meta = getattr(traj[-1][0], \"meta\", {})\n",
        "        return meta.get(\"delivered\", -1) == meta.get(\"total\", -2)\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_one(path):\n",
        "        with open(path, \"rb\") as f:\n",
        "            t = pickle.load(f)\n",
        "        if WorkerAgent._traj_success(t):\n",
        "            return t\n",
        "        logging.info(f\"  · drop FAILED traj  → {os.path.basename(path)}\")\n",
        "        return []\n",
        "\n",
        "    #───────────────────────────────────────────────────────────────\n",
        "    #  (WorkerAgent 메소드)  ─  Expert-pkl 모방 학습 루틴\n",
        "    #───────────────────────────────────────────────────────────────\n",
        "    def pretrain_with_imitation(\n",
        "        self,\n",
        "        expert_data_paths: list[str],\n",
        "        epochs: int,\n",
        "        lr: float,\n",
        "        batch_size: int,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        전문가(pkl) 궤적을 이용해 Worker 네트워크를 사전 학습한다.\n",
        "        ───────────────────────────────────────────────────────────\n",
        "        ▸ expert_data_paths : '*.pkl' 파일 목록\n",
        "        ▸ 각 pkl = [(state : Data, action : (str, int)), …]\n",
        "        \"\"\"\n",
        "        logging.info(\"[Phase 1] Starting Imitation Learning for Worker Agent…\")\n",
        "\n",
        "        # 1) 궤적 로드 ───────────────────────────────────────────\n",
        "        expert_pairs: list[tuple[Data, tuple[str, int]]] = []\n",
        "        valid_types = set(self.type_to_idx.keys())      # {'LOAD', …}\n",
        "\n",
        "        for path in expert_data_paths:\n",
        "            if not os.path.exists(path):\n",
        "                logging.warning(f\"  · Not found → {path}\")\n",
        "                continue\n",
        "            with open(path, \"rb\") as f:\n",
        "                traj = pickle.load(f)\n",
        "            expert_pairs.extend( (s, a) for s, a in traj if a[0] in valid_types )\n",
        "\n",
        "        if not expert_pairs:\n",
        "            logging.warning(\"  · No usable expert samples → skip.\")\n",
        "            return\n",
        "\n",
        "        max_n = getattr(self.config, \"MAX_EXPERT_SAMPLES\", None)\n",
        "        if max_n and len(expert_pairs) > max_n:\n",
        "            random.shuffle(expert_pairs)\n",
        "            expert_pairs = expert_pairs[:max_n]\n",
        "        logging.info(f\"  · Total Samples: {len(expert_pairs):,}\")\n",
        "\n",
        "        # 2) 옵티마이저 & 학습 루프 ───────────────────────────────\n",
        "        optim_ = optim.Adam(self.network.parameters(), lr=lr)\n",
        "        self.network.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            random.shuffle(expert_pairs)\n",
        "            total_loss, nb = 0.0, 0\n",
        "\n",
        "            for idx in range(0, len(expert_pairs), batch_size):\n",
        "                batch = expert_pairs[idx: idx + batch_size]\n",
        "                if not batch: continue\n",
        "\n",
        "                states, acts = zip(*batch)\n",
        "                g = Batch.from_data_list(states).to(self.device)\n",
        "\n",
        "                # action → 텐서\n",
        "                a_types = torch.as_tensor(\n",
        "                    [self.type_to_idx[a[0]] for a in acts],\n",
        "                    device=self.device, dtype=torch.long)\n",
        "                a_cars  = torch.as_tensor(\n",
        "                    [a[1] for a in acts],\n",
        "                    device=self.device, dtype=torch.long)\n",
        "\n",
        "                dummy_goal = torch.zeros(\n",
        "                    g.num_graphs, self.config.GOAL_EMBED_DIM,\n",
        "                    device=self.device)\n",
        "\n",
        "                # ── forward & loss ─────────────────────────────\n",
        "                with torch.set_grad_enabled(True):\n",
        "                    t_logit, l_logit, u_logit, r_logit, _v, _ = \\\n",
        "                        self.network(g, dummy_goal)\n",
        "\n",
        "                    loss_type = F.cross_entropy(t_logit, a_types)\n",
        "\n",
        "                    loss_load   = F.cross_entropy(l_logit, a_cars, reduction=\"none\")\n",
        "                    loss_unload = F.cross_entropy(u_logit, a_cars, reduction=\"none\")\n",
        "                    loss_reloc  = F.cross_entropy(r_logit, a_cars, reduction=\"none\")\n",
        "\n",
        "                    m_load     = (a_types == self.type_to_idx['LOAD'])\n",
        "                    m_unload   = (a_types == self.type_to_idx['UNLOAD'])\n",
        "                    m_reloc    = (a_types == self.type_to_idx['RELOCATE_INTERNAL'])\n",
        "\n",
        "                    loss_car = loss_type.new_zeros(1)  # grad X 텐서\n",
        "                    if m_load.any().item():\n",
        "                        loss_car = loss_car + loss_load[m_load].mean()\n",
        "                    if m_unload.any().item():\n",
        "                        loss_car = loss_car + loss_unload[m_unload].mean()\n",
        "                    if m_reloc.any().item():\n",
        "                        loss_car = loss_car + loss_reloc[m_reloc].mean()\n",
        "\n",
        "                    loss = loss_type + loss_car\n",
        "\n",
        "                    # backward\n",
        "                    optim_.zero_grad()\n",
        "                    loss.backward()\n",
        "                    nn.utils.clip_grad_norm_(self.network.parameters(),\n",
        "                                             self.config.PPO_MAX_GRAD_NORM)\n",
        "                    optim_.step()\n",
        "\n",
        "                total_loss += loss.item(); nb += 1\n",
        "\n",
        "            if nb and (epoch + 1) % 10 == 0:\n",
        "                logging.info(f\"    Epoch {epoch+1:4d}/{epochs}  \"\n",
        "                             f\"avg-loss {total_loss/nb:.4f}\")\n",
        "\n",
        "        logging.info(\"[Phase 1] Imitation Learning Finished.\")\n",
        "\n",
        "\n",
        "    def evaluate_actions(self, states: list[Data], actions: torch.Tensor, goal_embedding: torch.Tensor) -> tuple:\n",
        "      batch_data = Batch.from_data_list(states).to(self.device)\n",
        "\n",
        "      \"\"\"\n",
        "      PPO 업데이트를 위해, 주어진 상태(states)에서 특정 행동(actions)을 했을 때의\n",
        "      로그 확률(log_prob), 엔트로피(entropy), 가치(value)를 다시 계산합니다.\n",
        "\n",
        "      Args:\n",
        "          states (list[Data]): 상태(그래프) 데이터의 리스트.\n",
        "          actions (torch.Tensor): [액션 타입, 차량 ID] 형태의 행동 텐서.\n",
        "          goal_embedding (torch.Tensor): 현재 목표에 대한 임베딩 텐서.\n",
        "\n",
        "      Returns:\n",
        "          tuple: (로그 확률, 엔트로피, 가치) 텐서.\n",
        "      \"\"\"\n",
        "      type_logits, load_logits, unload_logits, relocate_logits, values, graph_emb = \\\n",
        "          self.network(batch_data, goal_embedding)\n",
        "\n",
        "      action_types = actions[:, 0]\n",
        "      action_cars  = actions[:, 1]\n",
        "\n",
        "      # 각 행동 헤드에 대한 확률 분포 생성\n",
        "      type_dist      = Categorical(logits=type_logits)\n",
        "      load_dist      = Categorical(logits=load_logits)\n",
        "      unload_dist    = Categorical(logits=unload_logits)\n",
        "      relocate_dist  = Categorical(logits=relocate_logits)\n",
        "\n",
        "      # 행동 타입 로그 확률\n",
        "      log_probs_type = type_dist.log_prob(action_types)\n",
        "\n",
        "      # 하위 행동 로그 확률 미리 계산\n",
        "      log_probs_load     = load_dist.log_prob(action_cars)\n",
        "      log_probs_unload   = unload_dist.log_prob(action_cars)\n",
        "      log_probs_relocate = relocate_dist.log_prob(action_cars)\n",
        "\n",
        "      # 실제 취한 타입에 해당하는 하위 행동 확률만 선택\n",
        "      mask_load     = (action_types == self.type_to_idx['LOAD']).float()\n",
        "      mask_unload   = (action_types == self.type_to_idx['UNLOAD']).float()\n",
        "      mask_relocate = (action_types == self.type_to_idx['RELOCATE_INTERNAL']).float()\n",
        "\n",
        "      log_probs = log_probs_type \\\n",
        "                  + log_probs_load     * mask_load \\\n",
        "                  + log_probs_unload   * mask_unload \\\n",
        "                  + log_probs_relocate * mask_relocate\n",
        "\n",
        "      # 엔트로피 계산\n",
        "      type_probs = F.softmax(type_logits, dim=-1)\n",
        "      entropy = (\n",
        "          type_dist.entropy()\n",
        "          + type_probs[:, self.type_to_idx['LOAD']]     * load_dist.entropy()\n",
        "          + type_probs[:, self.type_to_idx['UNLOAD']]   * unload_dist.entropy()\n",
        "          + type_probs[:, self.type_to_idx['RELOCATE_INTERNAL']] * relocate_dist.entropy()\n",
        "      )\n",
        "\n",
        "      return log_probs, entropy, values\n",
        "\n",
        "    def update(self, storage: PPOStorage, goal_embedding: torch.Tensor) -> dict:\n",
        "        \"\"\"저장된 Worker의 경험 데이터를 사용하여 PPO 업데이트를 수행합니다.\"\"\"\n",
        "        self.network.train()\n",
        "\n",
        "        # 1) 마지막 상태를 배치로 만들어줍니다.\n",
        "        last_state = storage.obs[storage.step - 1]\n",
        "        last_state_batch = Batch.from_data_list([last_state]).to(self.device)\n",
        "        # 마지막 상태의 가치 계산\n",
        "        with torch.no_grad():\n",
        "            # network 반환: (type_logits, load_logits, unload_logits, relocate_logits, critic_value, graph_emb)\n",
        "            _, _, _, _, critic_value, _ = self.network(last_state_batch, goal_embedding)\n",
        "            last_value = critic_value.squeeze(0)\n",
        "\n",
        "\n",
        "\n",
        "        # GAE와 Return 계산\n",
        "        storage.compute_returns_and_advantages(last_value, self.config.WORKER_GAMMA, self.config.PPO_GAE_LAMBDA)\n",
        "\n",
        "        # 학습 데이터 준비 (리스트는 그대로 두고, 텐서는 device로 이동)\n",
        "        b_obs = storage.obs # 리스트이므로 device로 옮기지 않음\n",
        "        b_actions = storage.actions.to(self.device)\n",
        "        b_logprobs = storage.logprobs.to(self.device)\n",
        "        b_returns = storage.returns.to(self.device)\n",
        "        b_advantages = storage.advantages.to(self.device)\n",
        "\n",
        "        batch_size = storage.step\n",
        "        if batch_size == 0: return {}\n",
        "        minibatch_size = max(1, batch_size // self.config.PPO_NUM_MINIBATCHES)\n",
        "\n",
        "        # PPO 업데이트 루프\n",
        "        for _ in range(self.config.PPO_UPDATE_EPOCHS):\n",
        "            perm_indices = np.random.permutation(batch_size)\n",
        "            for start in range(0, batch_size, minibatch_size):\n",
        "                end = start + minibatch_size\n",
        "                mb_idx = perm_indices[start:end]\n",
        "\n",
        "                # 미니배치 데이터 생성\n",
        "                mb_states = [b_obs[i] for i in mb_idx]\n",
        "                mb_actions = b_actions[mb_idx]\n",
        "                mb_logprobs = b_logprobs[mb_idx]\n",
        "                mb_returns = b_returns[mb_idx]\n",
        "                mb_advantages = b_advantages[mb_idx]\n",
        "\n",
        "                new_logprobs, entropy, new_values = self.evaluate_actions(mb_states, mb_actions, goal_embedding)\n",
        "\n",
        "                # PPO 손실 계산\n",
        "                log_ratio = new_logprobs - mb_logprobs\n",
        "                ratio = log_ratio.exp()\n",
        "\n",
        "                surr1 = mb_advantages * ratio\n",
        "                surr2 = mb_advantages * torch.clamp(ratio, 1 - self.config.PPO_CLIP_COEF, 1 + self.config.PPO_CLIP_COEF)\n",
        "\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "                value_loss = F.mse_loss(new_values, mb_returns)\n",
        "\n",
        "                loss = (policy_loss +\n",
        "                        self.config.PPO_VALUE_COEF * value_loss -\n",
        "                        self.config.WORKER_ENTROPY_COEF * entropy.mean())\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.network.parameters(), self.config.PPO_MAX_GRAD_NORM)\n",
        "                self.optimizer.step()\n",
        "\n",
        "        # 학습 로그를 위해 손실 값 반환\n",
        "        return {\n",
        "            \"policy_loss\": policy_loss.item(),\n",
        "            \"value_loss\": value_loss.item(),\n",
        "            \"entropy\": entropy.mean().item()\n",
        "        }\n",
        "\n",
        "# ==============================================================================\n",
        "# 섹션 5: 평가 및 메인 루프 (리팩토링 버전)\n",
        "# ==============================================================================\n",
        "import time\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def evaluate_agent(manager_agent: ManagerAgent, worker_agent: WorkerAgent, problems: list[dict], max_num_ports: int, config: Config):\n",
        "    \"\"\"\n",
        "    현재 에이전트의 성능을 평가하고, 다양한 지표를 기록합니다.\n",
        "    \"\"\"\n",
        "    logging.info(\"=\" * 20 + \" AGENT EVALUATION START \" + \"=\" * 20)\n",
        "    manager_agent.network.eval()\n",
        "    worker_agent.network.eval()\n",
        "\n",
        "    total_success_count = 0\n",
        "    total_costs = []\n",
        "    total_relocations = []\n",
        "    manager_action_counts = collections.defaultdict(int)\n",
        "\n",
        "    original_env_wrapper = manager_agent.env_wrapper\n",
        "    MAX_EVAL_MANAGER_STEPS = 4000\n",
        "\n",
        "    # 💡 --- [핵심] 환경 객체를 루프 밖에서 한 번만 생성 --- 💡\n",
        "    # 첫 번째 문제로 h_env를 초기화하고, 이후에는 reset으로 재사용\n",
        "    prob = random.choice(problems)\n",
        "    h_env = HierarchicalEnvWrapper(prob, max_num_ports, worker_agent, config)\n",
        "    manager_agent.env_wrapper = h_env\n",
        "\n",
        "    for episode_num in range(config.EVAL_EPISODES):\n",
        "        # 💡 [수정] 매번 새로 생성하는 대신, reset 메서드로 문제만 교체\n",
        "        if episode_num > 0:\n",
        "            prob = random.choice(problems)\n",
        "            # h_env.reset()은 manager_state를 반환하지 않으므로, 직접 상태를 가져와야 합니다.\n",
        "            h_env.reset(prob_data=prob)\n",
        "            manager_state = h_env._get_manager_state()\n",
        "        else:\n",
        "            # 첫 에피소드는 이미 h_env 생성 시 리셋됨\n",
        "            manager_state = h_env._get_manager_state()\n",
        "\n",
        "        if hasattr(manager_agent, 'prev_action_idx'):\n",
        "             manager_agent.prev_action_idx.zero_()\n",
        "\n",
        "        overall_done = False\n",
        "        episode_cost = 0.0\n",
        "        current_episode_steps = 0\n",
        "\n",
        "        while not overall_done:\n",
        "            if current_episode_steps > MAX_EVAL_MANAGER_STEPS:\n",
        "                logging.warning(f\"\\nEval Episode [{episode_num+1}] reached max steps limit. Breaking loop.\")\n",
        "                break\n",
        "\n",
        "            with torch.no_grad():\n",
        "                legal_actions = h_env.ship_env.get_legal_actions(for_worker=False)\n",
        "                prev_action_idx = manager_agent.prev_action_idx if hasattr(manager_agent, 'prev_action_idx') else torch.zeros(1, dtype=torch.long, device=manager_agent.device)\n",
        "\n",
        "                manager_action_idx, _, _, _, _ = manager_agent.get_action_and_value(\n",
        "                    manager_state.to(manager_agent.device), legal_actions, prev_action_idx, greedy=True\n",
        "                )\n",
        "\n",
        "            if hasattr(manager_agent, 'prev_action_idx'):\n",
        "                manager_agent.prev_action_idx = torch.tensor(\n",
        "                    [manager_action_idx], dtype=torch.long, device=manager_agent.device\n",
        "                )\n",
        "\n",
        "            manager_action_counts[h_env.manager_action_map[manager_action_idx]] += 1\n",
        "            current_goal_str = h_env.manager_action_map[manager_action_idx]\n",
        "            print(f\"\\r  Eval Ep[{episode_num+1}/{config.EVAL_EPISODES}] Step[{current_episode_steps+1}]: Trying Goal -> {current_goal_str.ljust(25)}\", end=\"\")\n",
        "\n",
        "            manager_state, _, overall_done, info = h_env.step(\n",
        "                manager_action_idx, greedy_worker=True\n",
        "            )\n",
        "            episode_cost += info.get('cost', 0.0)\n",
        "            current_episode_steps += 1\n",
        "\n",
        "        print()\n",
        "\n",
        "        if len(h_env.ship_env.delivered_cars) == h_env.ship_env.total_cars:\n",
        "            total_success_count += 1\n",
        "        total_costs.append(episode_cost)\n",
        "        total_relocations.append(h_env.ship_env.relocations_this_episode)\n",
        "        logging.info(f\"  Eval Episode [{episode_num+1}/{config.EVAL_EPISODES}] Finished. Success: {len(h_env.ship_env.delivered_cars) == h_env.ship_env.total_cars}\")\n",
        "\n",
        "    # 평가가 끝나면 원래 환경으로 복원\n",
        "    manager_agent.env_wrapper = original_env_wrapper\n",
        "    manager_agent.network.train()\n",
        "    worker_agent.network.train()\n",
        "\n",
        "    # 최종 결과 계산 및 로깅\n",
        "    success_rate = total_success_count / config.EVAL_EPISODES\n",
        "    avg_cost = np.mean(total_costs) if total_costs else 0.0\n",
        "    avg_relocations = np.mean(total_relocations) if total_relocations else 0.0\n",
        "\n",
        "    logging.info(\"-\" * 54)\n",
        "    logging.info(f\"[EVAL RESULT] Success Rate: {success_rate*100:.1f}%\")\n",
        "    logging.info(f\"[EVAL RESULT] Average Cost: {avg_cost:.2f}\")\n",
        "    logging.info(f\"[EVAL RESULT] Average Relocations: {avg_relocations:.2f}\")\n",
        "\n",
        "    total_actions = sum(manager_action_counts.values())\n",
        "    if total_actions > 0:\n",
        "        logging.info(\"[EVAL RESULT] Manager Action Distribution:\")\n",
        "        for action_idx in sorted(h_env.manager_action_map.keys()):\n",
        "            count = manager_action_counts.get(h_env.manager_action_map[action_idx], 0)\n",
        "            action_str = h_env.manager_action_map[action_idx]\n",
        "            percentage = (count / total_actions) * 100\n",
        "            logging.info(f\"  - {action_str:<25s}: {count} times ({percentage:.1f}%)\")\n",
        "\n",
        "    logging.info(\"=\" * 22 + \" EVALUATION END \" + \"=\" * 22 + \"\\n\")\n",
        "\n",
        "    return {\n",
        "        \"success_rate\": success_rate,\n",
        "        \"avg_cost\": avg_cost,\n",
        "        \"avg_relocations\": avg_relocations\n",
        "    }\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    mp.set_start_method(\"spawn\", force=True) # 멀티 프로세\n",
        "    # --- 1. 초기 설정 및 환경 구성 ---\n",
        "    setup_logger()\n",
        "    config = Config()\n",
        "    writer = SummaryWriter(log_dir=config.LOG_DIR)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    logging.info(f\"Using device: {device}\")\n",
        "\n",
        "    assert torch.cuda.is_available(), \"CUDA가 활성화되어 있지 않습니다!\"\n",
        "    print(\"✅ CUDA 활성화 확인\")\n",
        "\n",
        "    all_problem_files = [os.path.join(config.PROBLEM_DIR, f) for f in os.listdir(config.PROBLEM_DIR) if f.endswith('.json')]\n",
        "    easy_problem_files = [f for f in all_problem_files if any(name in f for name in ['prob1', 'prob2', 'prob4'])]\n",
        "    easy_problems = [json.load(open(f)) for f in easy_problem_files if os.path.exists(f)]\n",
        "    all_problems = [json.load(open(f)) for f in all_problem_files if os.path.exists(f)]\n",
        "    if not all_problems:\n",
        "        logging.error(f\"No problem files found in {config.PROBLEM_DIR}. Exiting.\")\n",
        "        exit()\n",
        "    if not easy_problems:\n",
        "        logging.warning(f\"No easy problems found. Starting with all problems.\")\n",
        "        easy_problems = all_problems\n",
        "\n",
        "    max_cars = max(sum(q for _, q in p.get('K', [])) for p in all_problems)\n",
        "    max_num_ports = max(p.get('P', 1) for p in all_problems)\n",
        "    max_nodes = max(p.get('N', 1) for p in all_problems)\n",
        "    node_feature_size = 4\n",
        "    global_feature_size = 3 + max_num_ports\n",
        "\n",
        "    # --- 2. 에이전트 및 환경 초기화 ---\n",
        "    worker_agent = WorkerAgent(node_feature_size, global_feature_size, max_cars, max_nodes, config, device)\n",
        "    current_prob = random.choice(easy_problems)\n",
        "    h_env_raw = HierarchicalEnvWrapper(current_prob, max_num_ports, worker_agent, config)\n",
        "\n",
        "    # ↓↓↓  추가 ↓↓↓\n",
        "    h_env = EpisodeTracker(h_env_raw, log_dir=config.LOG_DIR)\n",
        "    # ↑↑↑  추가 ↑↑↑\n",
        "\n",
        "    manager_agent = ManagerAgent(config, device, h_env)\n",
        "    manager_agent.prev_action_idx = torch.zeros(1, dtype=torch.long, device=device)\n",
        "\n",
        "    # --- 3. Worker 사전 훈련 (모방 학습) ---\n",
        "    print(\"\\nDEBUG: >>>>>>>>>> STEP 1: Calling pretrain_with_imitation NOW...\")\n",
        "    worker_agent.pretrain_with_imitation(\n",
        "        config.EXPERT_DATA_PATHS, config.IMITATION_LEARNING_EPOCHS, config.IMITATION_LR, config.IMITATION_BATCH_SIZE\n",
        "    )\n",
        "    print(\"DEBUG: <<<<<<<<<< STEP 1: pretrain_with_imitation FINISHED.\\n\")\n",
        "\n",
        "    # --- 4. 계층적 강화학습 메인 루프 ---\n",
        "    logging.info(\"\\n[Phase 2] Starting Hierarchical Reinforcement Learning...\")\n",
        "\n",
        "    manager_state = h_env.reset(prob_data=current_prob)\n",
        "    manager_storage = PPOStorage(config.MANAGER_NUM_STEPS_PER_UPDATE, (1,), device,\n",
        "                                 state_shape=(config.MANAGER_STATE_DIM,),\n",
        "                                 manager_action_dim=config.MANAGER_ACTION_DIM)\n",
        "\n",
        "    episode_rewards, episode_costs, episode_manager_steps = 0, 0, 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Interval 통계를 위한 변수 초기화\n",
        "    interval_rewards = 0.0\n",
        "    interval_goals = defaultdict(int)\n",
        "    interval_successes = 0\n",
        "    interval_costs = 0.0\n",
        "    interval_worker_rewards = 0.0\n",
        "    interval_worker_steps = 0\n",
        "\n",
        "    for manager_step in range(1, config.TOTAL_MANAGER_STEPS + 1):\n",
        "        is_curriculum_phase = manager_step < config.CURRICULUM_STEPS\n",
        "\n",
        "        if is_curriculum_phase:\n",
        "            possible_goals = []\n",
        "            if not h_env._is_goal_achieved('FINISH_UNLOAD'): possible_goals.append(1)\n",
        "            if not h_env._is_goal_achieved('CLEAR_TEMP'): possible_goals.append(3)\n",
        "            if not h_env._is_goal_achieved('FINISH_LOAD'): possible_goals.append(2)\n",
        "            if possible_goals: manager_action_idx = random.choice(possible_goals)\n",
        "            else: manager_action_idx = 4\n",
        "            manager_action_tensor = torch.tensor([manager_action_idx], device=device)\n",
        "            m_log_prob, m_value, m_mask = torch.tensor(0.0), torch.tensor(0.0), None\n",
        "            manager_agent.prev_action_idx = manager_action_tensor.clone()\n",
        "        else:\n",
        "            legal_actions = h_env.ship_env.get_legal_actions(for_worker=False)\n",
        "            prev_idx = manager_agent.prev_action_idx\n",
        "            with torch.no_grad():\n",
        "                manager_action_tensor, m_log_prob, _, m_value, m_mask = manager_agent.get_action_and_value(\n",
        "                    manager_state, legal_actions, prev_idx, greedy=False\n",
        "                )\n",
        "            manager_action_idx = manager_action_tensor.item()\n",
        "            manager_agent.prev_action_idx = manager_action_tensor.clone()\n",
        "\n",
        "        next_manager_state, manager_reward, overall_done, worker_info = h_env.step(manager_action_idx)\n",
        "\n",
        "        episode_rewards += manager_reward\n",
        "        episode_costs += worker_info.get('cost', 0.0)\n",
        "        episode_manager_steps += 1\n",
        "\n",
        "        # 매 스텝마다 Interval 통계 누적\n",
        "        interval_rewards += manager_reward\n",
        "        interval_goals[worker_info.get('goal', 'N/A')] += 1\n",
        "        if worker_info.get('success', False):\n",
        "            interval_successes += 1\n",
        "        interval_costs += worker_info.get('cost', 0.0)\n",
        "        interval_worker_rewards += worker_info.get('worker_total_reward', 0.0) # h_env.step에서 이 값을 반환해야 함\n",
        "        interval_worker_steps += worker_info.get('steps', 0)\n",
        "\n",
        "        if not is_curriculum_phase:\n",
        "            manager_storage.add(manager_state, manager_action_tensor, m_log_prob, manager_reward, overall_done, m_value, m_mask)\n",
        "\n",
        "        manager_state = next_manager_state\n",
        "\n",
        "        if overall_done:\n",
        "            s = h_env.ship_env\n",
        "            success_ratio = len(s.delivered_cars) / s.total_cars if s.total_cars > 0 else 0\n",
        "            logging.info(f\"EPISODE DONE (M-Step: {manager_step}) | Success: {success_ratio*100:.1f}% | Total Reward: {episode_rewards:.2f} | Total Cost: {episode_costs:.2f} | Length: {episode_manager_steps} steps\")\n",
        "            writer.add_scalar(\"Episode/TotalReward\", episode_rewards, manager_step)\n",
        "            writer.add_scalar(\"Episode/TotalCost\", episode_costs, manager_step)\n",
        "            writer.add_scalar(\"Episode/SuccessRatio\", success_ratio, manager_step)\n",
        "            writer.add_scalar(\"Episode/Length\", episode_manager_steps, manager_step)\n",
        "\n",
        "            if manager_step < config.CURRICULUM_TRANSITION_STEP:\n",
        "                current_prob = random.choice(easy_problems)\n",
        "            else:\n",
        "                if manager_step - episode_manager_steps < config.CURRICULUM_TRANSITION_STEP:\n",
        "                     logging.info(\"=\"*20 + \" SWITCHING TO FULL PROBLEM SET \" + \"=\"*20)\n",
        "                current_prob = random.choice(all_problems)\n",
        "\n",
        "            manager_state = h_env.reset(prob_data=current_prob)\n",
        "            manager_agent.prev_action_idx.zero_()\n",
        "            episode_rewards, episode_costs, episode_manager_steps = 0, 0, 0\n",
        "\n",
        "        if not is_curriculum_phase and manager_storage.is_full():\n",
        "            loss_info = manager_agent.update(manager_storage)\n",
        "            if loss_info:\n",
        "                writer.add_scalar(\"Train/Manager_PolicyLoss\", loss_info[\"policy_loss\"], manager_step)\n",
        "                writer.add_scalar(\"Train/Manager_ValueLoss\", loss_info[\"value_loss\"], manager_step)\n",
        "                writer.add_scalar(\"Train/Manager_Entropy\", loss_info[\"entropy\"], manager_step)\n",
        "            manager_storage.reset()\n",
        "\n",
        "        # 💡 --- [수정] 주기적 로그 출력 (중복 제거 및 최종 버전) --- 💡\n",
        "        if manager_step % config.PRINT_INTERVAL_MANAGER_STEPS == 0:\n",
        "            elapsed_time = time.time() - start_time\n",
        "            steps_per_sec = config.PRINT_INTERVAL_MANAGER_STEPS / elapsed_time if elapsed_time > 0 else 0\n",
        "\n",
        "            avg_reward = interval_rewards / config.PRINT_INTERVAL_MANAGER_STEPS\n",
        "            interval_success_rate = (interval_successes / config.PRINT_INTERVAL_MANAGER_STEPS) * 100\n",
        "            avg_cost_per_step = interval_costs / config.PRINT_INTERVAL_MANAGER_STEPS\n",
        "            # 💡 [추가] Worker 평균 보상 계산\n",
        "            avg_worker_rew = interval_worker_rewards / interval_worker_steps if interval_worker_steps > 0 else 0\n",
        "\n",
        "            goal_dist_str = \", \".join([f\"{k.split('_')[-1]}:{v}\" for k, v in sorted(interval_goals.items())])\n",
        "\n",
        "            logging.info(\n",
        "                f\"M-Step {manager_step:6d} | Avg M-Rew: {avg_reward:7.2f} | \"\n",
        "                f\"W-Rew: {avg_worker_rew:6.3f} | \" # Worker 평균 보상 출력\n",
        "                f\"Success: {interval_success_rate:3.0f}% | Avg Cost: {avg_cost_per_step:8.1f} | \"\n",
        "                f\"Goals: [{goal_dist_str}] | SPS: {steps_per_sec:.2f}\"\n",
        "            )\n",
        "            goal_dist_str = \", \".join([f\"{k.split('_')[-1]}:{v}\" for k, v in sorted(interval_goals.items())])\n",
        "\n",
        "            logging.info(\n",
        "                f\"M-Step {manager_step:6d} | Avg Rew: {avg_reward:7.2f} | \"\n",
        "                f\"Success: {interval_success_rate:3.0f}% | Avg Cost: {avg_cost_per_step:8.1f} | \"\n",
        "                f\"Goals: [{goal_dist_str}] | SPS: {steps_per_sec:.2f}\"\n",
        "            )\n",
        "\n",
        "            # 다음 Interval을 위해 통계 변수 초기화\n",
        "            interval_worker_rewards = 0.0\n",
        "            interval_worker_steps = 0\n",
        "            interval_rewards = 0.0\n",
        "            interval_goals.clear()\n",
        "            interval_successes = 0\n",
        "            interval_costs = 0.0\n",
        "            start_time = time.time()\n",
        "\n",
        "        # 주기적 평가 및 모델 저장\n",
        "        if manager_step > 0 and manager_step % config.EVAL_INTERVAL_MANAGER_STEPS == 0:\n",
        "            if manager_step == config.CURRICULUM_STEPS:\n",
        "                logging.info(\"=\"*20 + \" CURRICULUM FINISHED \" + \"=\"*20)\n",
        "\n",
        "            eval_results = evaluate_agent(manager_agent, worker_agent, all_problems, max_num_ports, config)\n",
        "            writer.add_scalar(\"Eval/SuccessRate\", eval_results[\"success_rate\"] * 100.0, manager_step)\n",
        "            writer.add_scalar(\"Eval/AvgCost\", eval_results[\"avg_cost\"], manager_step)\n",
        "            writer.add_scalar(\"Eval/AvgRelocations\", eval_results[\"avg_relocations\"], manager_step)\n",
        "\n",
        "            torch.save(worker_agent.network.state_dict(), os.path.join(config.MODEL_DIR, f\"worker_model_step_{manager_step}.pth\"))\n",
        "            torch.save(manager_agent.network.state_dict(), os.path.join(config.MODEL_DIR, f\"manager_model_step_{manager_step}.pth\"))\n",
        "\n",
        "    writer.close()\n",
        "    logging.info(\"--- V12 Refactored Training Completed ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KWwYc4yKt_-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Wq6CxevywILM"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPjWCPGnImRflQZyFmWHN5k",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10b3083f07594018a2d7083f320c2a1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3fbc29bb1fb40f4b8401d67163debcd",
              "IPY_MODEL_3b18db1c78b74f138500c1cd8a6c64eb",
              "IPY_MODEL_0b8243e20c2f4c19b6a0b6ea9f3ed3bb"
            ],
            "layout": "IPY_MODEL_3984f7b3222d4e459531ec76993d6564"
          }
        },
        "f3fbc29bb1fb40f4b8401d67163debcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_098bddb2790a43eb8130699124e3f0cd",
            "placeholder": "​",
            "style": "IPY_MODEL_2cece6120ff34048b93a960adcca2f2d",
            "value": "BFS-cache: 100%"
          }
        },
        "3b18db1c78b74f138500c1cd8a6c64eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10af330243c24df6a0f066e2763eb05a",
            "max": 131769,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9da3066d65744e69b34160f904ac73e9",
            "value": 131769
          }
        },
        "0b8243e20c2f4c19b6a0b6ea9f3ed3bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ade220bad3a64b13a525cdaff6852a38",
            "placeholder": "​",
            "style": "IPY_MODEL_140fcfcac8054861b811b901ce6286db",
            "value": " 131769/131769 [02:24&lt;00:00, 907.63pair/s]"
          }
        },
        "3984f7b3222d4e459531ec76993d6564": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "098bddb2790a43eb8130699124e3f0cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cece6120ff34048b93a960adcca2f2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10af330243c24df6a0f066e2763eb05a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9da3066d65744e69b34160f904ac73e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ade220bad3a64b13a525cdaff6852a38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "140fcfcac8054861b811b901ce6286db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70982f5ede254274836c763a2b18fa85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d908a324add4e198054b0e8ec3a8d37",
              "IPY_MODEL_7b90f8811e6b4fcba6ad9a081ae4d776",
              "IPY_MODEL_19acf994b0da47a783444480a9536a62"
            ],
            "layout": "IPY_MODEL_ec8dabdb014746c7b62526b22075a208"
          }
        },
        "6d908a324add4e198054b0e8ec3a8d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9467fb546f794957a628bd9156e1ca9b",
            "placeholder": "​",
            "style": "IPY_MODEL_dc984cd885d44bf1a3cdc0a5bb64bf98",
            "value": "Ports: 100%"
          }
        },
        "7b90f8811e6b4fcba6ad9a081ae4d776": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b30db2159c64495bb46eec3e9d73e8d3",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e17892e2a78c4246a042c81235e96160",
            "value": 15
          }
        },
        "19acf994b0da47a783444480a9536a62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49b92639664040c1b0a1099460bbdb91",
            "placeholder": "​",
            "style": "IPY_MODEL_e76fe8e4eca94d34a4552d054837e8da",
            "value": " 15/15 [00:00&lt;00:00, 1044.03port/s]"
          }
        },
        "ec8dabdb014746c7b62526b22075a208": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9467fb546f794957a628bd9156e1ca9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc984cd885d44bf1a3cdc0a5bb64bf98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b30db2159c64495bb46eec3e9d73e8d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e17892e2a78c4246a042c81235e96160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49b92639664040c1b0a1099460bbdb91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e76fe8e4eca94d34a4552d054837e8da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}