{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psiudo/ooo/blob/main/v11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_odh6XsYqQG",
        "outputId": "55d2e32f-16a6-43a3-9b9f-ec705d036f48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "âœ… Google Driveê°€ ì„±ê³µì ìœ¼ë¡œ ë§ˆìš´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "# ==================================\n",
        "# ì…€ 1: Google Drive ë§ˆìš´íŠ¸\n",
        "# ==================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"âœ… Google Driveê°€ ì„±ê³µì ìœ¼ë¡œ ë§ˆìš´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUAVUj5OZBE-",
        "outputId": "c91ff690-9919-4507-a681-ffaf9e4422ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.6.15)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "âœ… í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "# ==================================\n",
        "# ì…€ 2: í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "# ==================================\n",
        "!pip install --upgrade torch-geometric\n",
        "print(\"âœ… í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTEBlsvyN9Aj",
        "outputId": "0568596b-5823-41f2-8691-f7db7068780a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba) (0.43.0)\n",
            "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.11/dist-packages (from numba) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mIPxSNJZeo6",
        "outputId": "47cbaae0-f1df-43e5-86a1-2c9b4d68e8dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-03 08:24:21 - INFO - Using device: cuda\n",
            "âœ… CUDA í™œì„±í™” í™•ì¸\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DEBUG: >>>>>>>>>> STEP 1: Calling pretrain_with_imitation NOW...\n",
            "2025-07-03 08:26:43 - INFO - [Phase 1] Starting Imitation Learning for Worker Agentâ€¦\n",
            "2025-07-03 08:26:45 - INFO -   Â· Total Samples: 5,420\n",
            "2025-07-03 08:27:57 - INFO -     Epoch   10/200  avg-loss 11.2702\n",
            "2025-07-03 08:29:10 - INFO -     Epoch   20/200  avg-loss 9.8362\n",
            "2025-07-03 08:30:22 - INFO -     Epoch   30/200  avg-loss 8.7999\n",
            "2025-07-03 08:31:34 - INFO -     Epoch   40/200  avg-loss 8.1010\n",
            "2025-07-03 08:32:47 - INFO -     Epoch   50/200  avg-loss 7.6587\n",
            "2025-07-03 08:33:59 - INFO -     Epoch   60/200  avg-loss 7.2841\n",
            "2025-07-03 08:35:12 - INFO -     Epoch   70/200  avg-loss 7.0093\n",
            "2025-07-03 08:36:24 - INFO -     Epoch   80/200  avg-loss 6.7883\n",
            "2025-07-03 08:37:37 - INFO -     Epoch   90/200  avg-loss 6.5952\n",
            "2025-07-03 08:38:50 - INFO -     Epoch  100/200  avg-loss 6.4202\n",
            "2025-07-03 08:40:02 - INFO -     Epoch  110/200  avg-loss 6.2706\n"
          ]
        }
      ],
      "source": [
        "########## v11 : HRL íŠ¸ëœìŠ¤í¬ë¨¸ GNN ë³´ìƒí•¨ìˆ˜ì˜ unambiguity ##########\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# ì„¹ì…˜ 1: ì„¤ì • ë° ê¸°ë³¸ ìœ í‹¸ë¦¬í‹° (ë¦¬íŒ©í† ë§ ë²„ì „)\n",
        "# ==============================================================================\n",
        "import json\n",
        "import torch.optim as optim\n",
        "import collections\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "import logging\n",
        "from torch_geometric.data import Data\n",
        "import pickle\n",
        "from torch_geometric.utils import to_dense_batch\n",
        "import logging\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from collections import defaultdict, deque\n",
        "import multiprocessing as mp\n",
        "import random\n",
        "\n",
        "import numba\n",
        "from numba.core import types\n",
        "from numba.typed import List\n",
        "\n",
        "def setup_logger():\n",
        "    \"\"\"\n",
        "    ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ë¡œê¹… ì„¤ì •ì„ ëª¨ë‘ ì´ˆê¸°í™”í•˜ê³ ,\n",
        "    ìš°ë¦¬ê°€ ì›í•˜ëŠ” ì„¤ì •ìœ¼ë¡œ ê°•ì œ ì ìš©í•˜ëŠ” í•¨ìˆ˜.\n",
        "    \"\"\"\n",
        "    # ë£¨íŠ¸ ë¡œê±°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
        "    root_logger = logging.getLogger()\n",
        "    root_logger.setLevel(logging.INFO) # ë¡œê·¸ ë ˆë²¨ ì„¤ì •\n",
        "\n",
        "    # ë£¨íŠ¸ ë¡œê±°ì— ì—°ê²°ëœ ëª¨ë“  ê¸°ì¡´ í•¸ë“¤ëŸ¬(Handler)ë¥¼ ì œê±°í•©ë‹ˆë‹¤. (ê°€ì¥ ì¤‘ìš”)\n",
        "    for handler in root_logger.handlers[:]:\n",
        "        root_logger.removeHandler(handler)\n",
        "\n",
        "    # ìš°ë¦¬ê°€ ì›í•˜ëŠ” ìƒˆë¡œìš´ í•¸ë“¤ëŸ¬ë¥¼ ìƒì„±í•˜ì—¬ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "    handler = logging.StreamHandler(sys.stdout) # ë¡œê·¸ë¥¼ ì½˜ì†”ì— ì¶œë ¥\n",
        "    formatter = logging.Formatter(\n",
        "        '%(asctime)s - %(levelname)s - %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S'\n",
        "    )\n",
        "    handler.setFormatter(formatter)\n",
        "    root_logger.addHandler(handler)\n",
        "\n",
        "\n",
        "class Config:\n",
        "    \"\"\"í•™ìŠµê³¼ ê´€ë ¨ëœ ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤\"\"\"\n",
        "\n",
        "    # --- ê²½ë¡œ ì„¤ì • ---\n",
        "    DRIVE_PROJECT_ROOT = '/content/drive/MyDrive/OptiChallenge'\n",
        "    PROBLEM_DIR  = os.path.join(DRIVE_PROJECT_ROOT, 'Exercise_Problems')\n",
        "    LOG_DIR      = os.path.join(DRIVE_PROJECT_ROOT, 'v12_logs_refactored')\n",
        "    MODEL_DIR    = os.path.join(DRIVE_PROJECT_ROOT, 'v12_models_refactored')\n",
        "\n",
        "    # ğŸ’¡ ìƒˆ expert pkl  â€˜expert_probX_TYY.pklâ€™ í˜•ì‹ìœ¼ë¡œ ìˆë‹¤ê³  ê°€ì •\n",
        "    EXPERT_DIR   = os.path.join(DRIVE_PROJECT_ROOT)\n",
        "    EXPERT_GLOB  = \"expert_prob[1248]_T*.pkl\"\n",
        "    MAX_EXPERT_SAMPLES = 40_000\n",
        "\n",
        "    @property\n",
        "    def EXPERT_DATA_PATHS(self):\n",
        "        import glob\n",
        "        paths = sorted(glob.glob(os.path.join(self.EXPERT_DIR, self.EXPERT_GLOB)))\n",
        "        if not paths:\n",
        "            logging.warning(f\"[Config] No expert pkl found under {self.EXPERT_DIR}\")\n",
        "        return paths\n",
        "\n",
        "    # --- í•™ìŠµ ì œì–´ ---\n",
        "    TOTAL_MANAGER_STEPS = 500_000       # Manager ì—ì´ì „íŠ¸ì˜ ì´ í•™ìŠµ ìŠ¤í…\n",
        "    CURRICULUM_STEPS = 2000            # ì „ë¬¸ê°€ ì •ì±…ì„ ëª¨ë°©í•˜ëŠ” ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ìŠ¤í… ìˆ˜\n",
        "    PRINT_INTERVAL_MANAGER_STEPS = 20  # í•™ìŠµ ì¤‘ ë¡œê·¸ ì¶œë ¥ ê°„ê²© (Manager ìŠ¤í… ê¸°ì¤€)\n",
        "    EVAL_INTERVAL_MANAGER_STEPS = 2000 # ëª¨ë¸ í‰ê°€ ë° ì €ì¥ ê°„ê²© (Manager ìŠ¤í… ê¸°ì¤€)\n",
        "    EVAL_EPISODES = 10                  # í‰ê°€ ì‹œ ì‹¤í–‰í•  ì—í”¼ì†Œë“œ ìˆ˜\n",
        "    CURRICULUM_TRANSITION_STEP = 20000 # ğŸ’¡ [ì¶”ê°€] ë¬¸ì œ ë‚œì´ë„ ì»¤ë¦¬í˜ëŸ¼ ì „í™˜ ì‹œì \n",
        "\n",
        "\n",
        "    # --- ëª¨ë°© í•™ìŠµ (Worker) ---\n",
        "    IMITATION_LEARNING_EPOCHS = 200      # Worker ëª¨ë°© í•™ìŠµ ì—í­ ìˆ˜\n",
        "    IMITATION_LR = 1e-4                 # Worker ëª¨ë°© í•™ìŠµ Learning Rate\n",
        "    IMITATION_BATCH_SIZE = 512          # Worker ëª¨ë°© í•™ìŠµ ë°°ì¹˜ í¬ê¸°\n",
        "\n",
        "    # --- Manager ì—ì´ì „íŠ¸ ì„¤ì • ---\n",
        "    MANAGER_STATE_DIM = 6               # Manager ìƒíƒœ ë²¡í„°ì˜ ì°¨ì›\n",
        "    MANAGER_ACTION_DIM = 5              # Manager í–‰ë™ì˜ ê°€ì§“ìˆ˜\n",
        "    MANAGER_LR = 3e-4                   # Manager Learning Rate\n",
        "    MANAGER_GAMMA = 0.99                # Manager í• ì¸ìœ¨ (Gamma)\n",
        "    MANAGER_ENTROPY_COEF = 0.05         # Manager ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ ê³„ìˆ˜ (íƒí—˜ ì¥ë ¤)\n",
        "    MANAGER_NUM_STEPS_PER_UPDATE = 512  # Manager ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í…\n",
        "\n",
        "    # [ì‹ ê·œ] ë³´ìƒ ì²´ê³„ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "    REPEAT_PENALTY = -1             # ê°™ì€ í–‰ë™ ë°˜ë³µ í˜ë„í‹°\n",
        "    STEP_PENALTY_WEIGHT = 0.001       # Worker ìŠ¤í…ë‹¹ ì‹œê°„ í˜ë„í‹° ê°€ì¤‘ì¹˜\n",
        "    NO_PROGRESS_PENALTY = -2.0        # Workerê°€ ì§„ì²™ ì—†ì´ ì¢…ë£Œ ì‹œ í˜ë„í‹°\n",
        "    SHAPING_REWARD_WEIGHT = 1.5       # PBRS ë³´ìƒ ê°•ë„ ì¡°ì ˆ ê³„ìˆ˜\n",
        "    NO_PROGRESS_LIMIT = 50            # Worker ì§„ì²™ íŒì • í•œë„ (ê¸°ì¡´ 50 í•˜ë“œì½”ë”© ê°’ ëŒ€ì²´)\\\n",
        "    TIMEOUT_PENALTY = -10.0\n",
        "\n",
        "    # --- Worker ì—ì´ì „íŠ¸ ì„¤ì • ---\n",
        "    WORKER_LR = 3e-4                    # Worker Learning Rate\n",
        "    WORKER_GAMMA = 0.95                 # Worker í• ì¸ìœ¨ (Gamma)\n",
        "    WORKER_ENTROPY_COEF = 0.005         # Worker ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ ê³„ìˆ˜\n",
        "    WORKER_MAX_STEPS_PER_GOAL = 300     # Managerì˜ ëª©í‘œ í•˜ë‚˜ë‹¹ Workerê°€ ìˆ˜í–‰í•  ìµœëŒ€ ìŠ¤í…\n",
        "    WORKER_NUM_STEPS_PER_UPDATE = 1024\n",
        "\n",
        "    # --- PPO ì•Œê³ ë¦¬ì¦˜ ê³µí†µ ì„¤ì • ---\n",
        "    PPO_UPDATE_EPOCHS = 4               # í•œ ë²ˆì˜ ì—…ë°ì´íŠ¸ ì‹œ ì—í­ ìˆ˜\n",
        "    PPO_NUM_MINIBATCHES = 8             # ë¯¸ë‹ˆë°°ì¹˜ ê°œìˆ˜\n",
        "    PPO_CLIP_COEF = 0.2                 # PPO í´ë¦¬í•‘ ê³„ìˆ˜\n",
        "    PPO_GAE_LAMBDA = 0.95               # GAE(Generalized Advantage Estimation) ëŒë‹¤ê°’\n",
        "    PPO_VALUE_COEF = 1.0                # ê°€ì¹˜ í•¨ìˆ˜ ì†ì‹¤(Value Loss) ê³„ìˆ˜\n",
        "    PPO_MAX_GRAD_NORM = 0.5             # Gradient Clipping ìµœëŒ€ L2 Norm\n",
        "\n",
        "    # --- ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ì„¤ì • ---\n",
        "    NODE_FEATURE_DIM = 4      # [is_occupied, dest_diff, blocking_count, is_relocatable]\n",
        "    GNN_EMBED_DIM = 128       # GNNì˜ ê¸°ë³¸ ì„ë² ë”© ì°¨ì›\n",
        "    GOAL_EMBED_DIM = 16       # ëª©í‘œ ì„ë² ë”© ë²¡í„° ì°¨ì›\n",
        "    # Workerì˜ GNN ì¶œë ¥ì„ Manager ìƒíƒœë¡œ ì‚¬ìš© (mean_pool + att_pool)\n",
        "    # MANAGER_STATE_DIM = GNN_EMBED_DIM * 2\n",
        "\n",
        "# --- ê²½ë¡œ ìƒì„± ---\n",
        "# í•™ìŠµ ë¡œê·¸ì™€ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "os.makedirs(Config.LOG_DIR, exist_ok=True)\n",
        "os.makedirs(Config.MODEL_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "# ğŸ’¡ --- ì´ í•¨ìˆ˜ë¡œ ê¸°ì¡´ì˜ ëª¨ë“  get_shortest_path ê´€ë ¨ í•¨ìˆ˜ë¥¼ êµì²´ --- ğŸ’¡\n",
        "@numba.jit(nopython=True)\n",
        "def get_shortest_path(adj_list, start, end, num_nodes):\n",
        "    \"\"\"\n",
        "    Numbaì— ì™„ë²½íˆ í˜¸í™˜ë˜ëŠ” ê°€ì¥ í‘œì¤€ì ì´ê³  ì•ˆì •ì ì¸ BFS í•¨ìˆ˜.\n",
        "    - ë¶€ëª¨ ë…¸ë“œ ì¶”ì  ë°©ì‹ì„ ì‚¬ìš©\n",
        "    - NumPy ë°°ì—´ê³¼ ê¸°ë³¸ ë¦¬ìŠ¤íŠ¸ë§Œ ì‚¬ìš©\n",
        "    \"\"\"\n",
        "    if start == end:\n",
        "        # Numbaë¥¼ ìœ„í•´ íƒ€ì…ì„ ëª…ì‹œì ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ìƒì„±\n",
        "        path = numba.typed.List()\n",
        "        path.append(start)\n",
        "        return path\n",
        "\n",
        "    # ë¶€ëª¨ ë…¸ë“œë¥¼ ê¸°ë¡í•  NumPy ë°°ì—´ (-1ë¡œ ì´ˆê¸°í™”)\n",
        "    parents = np.full(num_nodes, -1, dtype=np.int64)\n",
        "\n",
        "    # ë°©ë¬¸ ê¸°ë¡ì„ ìœ„í•œ boolean NumPy ë°°ì—´\n",
        "    visited = np.zeros(num_nodes, dtype=np.bool_)\n",
        "\n",
        "    # íë¡œ ì‚¬ìš©í•  ë‹¨ìˆœ ë¦¬ìŠ¤íŠ¸\n",
        "    queue = numba.typed.List()\n",
        "\n",
        "    queue.append(start)\n",
        "    visited[start] = True\n",
        "    head = 0 # íì˜ ë§¨ ì•ì„ ê°€ë¦¬í‚¤ëŠ” í¬ì¸í„°\n",
        "\n",
        "    path_found = False\n",
        "    while head < len(queue):\n",
        "        current = queue[head]\n",
        "        head += 1\n",
        "\n",
        "        if current == end:\n",
        "            path_found = True\n",
        "            break\n",
        "\n",
        "        for neighbor in adj_list[current]:\n",
        "            if not visited[neighbor]:\n",
        "                visited[neighbor] = True\n",
        "                parents[neighbor] = current\n",
        "                queue.append(neighbor)\n",
        "\n",
        "    # ê²½ë¡œ ì—­ì¶”ì \n",
        "    if path_found:\n",
        "        path = numba.typed.List()\n",
        "        curr = end\n",
        "        while curr != -1:\n",
        "            path.append(curr)\n",
        "            curr = parents[curr]\n",
        "        return path[::-1] # ì—­ìˆœì´ë¯€ë¡œ ë’¤ì§‘ì–´ì„œ ë°˜í™˜\n",
        "\n",
        "    return numba.typed.List.empty_list(numba.int64)\n",
        "\n",
        "# ğŸ’¡ --- êµì²´ ì™„ë£Œ --- ğŸ’¡\n",
        "\n",
        "class ShipEnv:\n",
        "    \"\"\"í™”ë¬¼ì„ ì˜ ìƒíƒœì™€ í–‰ë™ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í™˜ê²½ í´ë˜ìŠ¤ (Numba ìµœì¢… ìµœì í™” ì ìš©)\"\"\"\n",
        "    def __init__(self, problem_data: dict, max_num_ports: int):\n",
        "        self.num_nodes = problem_data.get('N', 1)\n",
        "        self.num_ports = problem_data.get('P', 1)\n",
        "        self.fixed_cost = float(problem_data.get('F', 100))\n",
        "        self.max_num_ports = max_num_ports\n",
        "        self.load_fail_streak: int = 0\n",
        "\n",
        "        # ğŸ’¡ --- [í•µì‹¬] Numba í˜¸í™˜ì„ ìœ„í•œ ê·¸ë˜í”„ ë°ì´í„° êµ¬ì¡°í™” --- ğŸ’¡\n",
        "        adj_list = [numba.typed.List.empty_list(numba.int64) for _ in range(self.num_nodes)]\n",
        "        edge_list_for_tensor = []\n",
        "        for u, v in problem_data.get('E', []):\n",
        "            adj_list[u].append(v)\n",
        "            adj_list[v].append(u)\n",
        "            edge_list_for_tensor.extend([[u, v], [v, u]])\n",
        "        self.adj_list = adj_list # Numba í•¨ìˆ˜ì— ë„˜ê²¨ì£¼ê¸° ìœ„í•´ ì €ì¥\n",
        "        # ğŸ’¡ --- ìˆ˜ì • ì™„ë£Œ --- ğŸ’¡\n",
        "\n",
        "        self.edge_index_tensor = torch.tensor(edge_list_for_tensor, dtype=torch.long).t().contiguous()\n",
        "\n",
        "        # ëª¨ë“  ë…¸ë“œ ìŒ ê°„ì˜ ìµœë‹¨ ê²½ë¡œë¥¼ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ ìºì‹±\n",
        "        self.shortest_paths = {}\n",
        "        for i in range(self.num_nodes):\n",
        "            self.shortest_paths[i] = {}\n",
        "            for j in range(self.num_nodes):\n",
        "                # ğŸ’¡ [í•µì‹¬] ìˆ˜ì •ëœ get_shortest_path í•¨ìˆ˜ í˜¸ì¶œ\n",
        "                path_result = get_shortest_path(self.adj_list, i, j, self.num_nodes)\n",
        "                self.shortest_paths[i][j] = list(path_result) # ê²°ê³¼ë¥¼ ì¼ë°˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥\n",
        "\n",
        "        self.cars = []\n",
        "        car_id_counter = 0\n",
        "        for demand_idx, (demand, quantity) in enumerate(problem_data.get('K', [])):\n",
        "            origin, dest = demand\n",
        "            for _ in range(quantity):\n",
        "                self.cars.append({'id': car_id_counter, 'demand_id': demand_idx, 'origin': origin, 'dest': dest})\n",
        "                car_id_counter += 1\n",
        "        self.total_cars = len(self.cars)\n",
        "        self.reset()\n",
        "\n",
        "    def _get_or_compute_path(self, start: int, end: int) -> list | None:\n",
        "        \"\"\"ë¯¸ë¦¬ ê³„ì‚°ëœ ê²½ë¡œë¥¼ ìºì‹œì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n",
        "        return self.shortest_paths.get(start, {}).get(end, None)\n",
        "\n",
        "    def _calculate_path_cost(self, path: list | None) -> float:\n",
        "        if not path or len(path) <= 1: return 0.0\n",
        "        return float(self.fixed_cost + (len(path) - 1))\n",
        "\n",
        "    def reset(self) -> Data:\n",
        "        self.current_port: int = 0\n",
        "        self.node_status: list[int] = [-1] * self.num_nodes\n",
        "        self.car_locations: dict[int, int] = {}\n",
        "        self.cars_on_board: set[int] = set()\n",
        "        self.temporarily_unloaded_cars: set[int] = set()\n",
        "        self.delivered_cars: set[int] = set()\n",
        "        self.relocations_this_episode: int = 0\n",
        "        self.last_car_action = {}\n",
        "        # [NEW] ì—í”¼ì†Œë“œê°€ ë°”ë€Œë©´ ì‹¤íŒ¨ ëˆ„ì ë„ ì´ˆê¸°í™”\n",
        "        self.load_fail_streak: int = 0\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self) -> Data:\n",
        "      node_features = []\n",
        "      for i in range(self.num_nodes):\n",
        "          if i == 0:\n",
        "              node_features.append([0.0, 0.0, 0.0, 0.0]); continue\n",
        "          car_id = self.node_status[i]\n",
        "          if car_id == -1:\n",
        "              node_features.append([0.0, 0.0, 0.0, 0.0])\n",
        "          else:\n",
        "              car = self.cars[car_id]\n",
        "              dest_diff = float(car['dest'] - self.current_port)\n",
        "              path_to_gate = self._get_or_compute_path(i, 0)\n",
        "\n",
        "              # ğŸ’¡ --- [í•µì‹¬ ìµœì í™”] ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ setìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í™•ì¸ ì†ë„ í–¥ìƒ --- ğŸ’¡\n",
        "              if path_to_gate:\n",
        "                  path_to_gate_set = set(path_to_gate[1:])\n",
        "                  blocking_count = sum(1 for node_idx, status in enumerate(self.node_status)\n",
        "                                      if status != -1 and node_idx in path_to_gate_set)\n",
        "              else:\n",
        "                  blocking_count = 0\n",
        "              # ğŸ’¡ --- ìµœì í™” ì™„ë£Œ --- ğŸ’¡\n",
        "\n",
        "              is_relocatable = 1.0 if car['dest'] != self.current_port else 0.0\n",
        "              node_features.append([1.0, dest_diff, float(blocking_count), is_relocatable])\n",
        "\n",
        "\n",
        "      waiting_cars = [c for c in self.cars if c['origin'] == self.current_port and c['id'] not in self.cars_on_board and c['id'] not in self.temporarily_unloaded_cars]\n",
        "      waiting_dest_counts = [0.0] * self.max_num_ports\n",
        "      for car in waiting_cars:\n",
        "          if car['dest'] < self.max_num_ports: waiting_dest_counts[car['dest']] += 1.0\n",
        "      global_features = [float(self.current_port), float(len(waiting_cars)), float(len(self.temporarily_unloaded_cars))] + waiting_dest_counts\n",
        "\n",
        "      return Data(x=torch.tensor(node_features, dtype=torch.float),\n",
        "                  edge_index=self.edge_index_tensor,\n",
        "                  global_features=torch.tensor([global_features], dtype=torch.float))\n",
        "\n",
        "    def get_legal_actions(self, for_worker: bool = False) -> list[tuple[str, int]]:\n",
        "        actions = []\n",
        "        any_empty_spot = any(status == -1 for status in self.node_status[1:])\n",
        "        if any_empty_spot:\n",
        "            load_candidates = [c for c in self.cars if (c['origin'] == self.current_port and c['id'] not in self.cars_on_board and c['id'] not in self.temporarily_unloaded_cars) or c['id'] in self.temporarily_unloaded_cars]\n",
        "            for car in load_candidates: actions.append(('LOAD', car['id']))\n",
        "            for car_id in self.cars_on_board: actions.append(('RELOCATE_INTERNAL', car_id))\n",
        "        for car_id in self.cars_on_board: actions.append(('UNLOAD', car_id))\n",
        "        if not for_worker: actions.append(('PROCEED_TO_NEXT_PORT', -1))\n",
        "        return actions\n",
        "\n",
        "\n",
        "    # ShipEnv ë‚´ (ê¸°ì¡´ ë©”ì†Œë“œë“¤ ë°”ë¡œ ìœ„/ì•„ë˜ ì•„ë¬´ ê³³)\n",
        "    def _is_hard_blocker(self, node_idx: int) -> bool:\n",
        "        \"\"\"\n",
        "        ê²Œì´íŠ¸ì—ì„œ ê°€ê¹ê³  ê³§ ë¹ ì§ˆ ì°¨ëŠ” â€˜ì†Œí”„íŠ¸ ë¸”ë¡œì»¤â€™ë¡œ ë³´ê³ \n",
        "        ì •ë§ ì¹˜ì›Œì•¼ë§Œ í†µê³¼ ê°€ëŠ¥í•œ ì°¨ë§Œ True.\n",
        "        \"\"\"\n",
        "        cid = self.node_status[node_idx]\n",
        "        if cid == -1:\n",
        "            return False\n",
        "\n",
        "        car = self.cars[cid]\n",
        "\n",
        "        # (1) ì§€ê¸ˆ í•˜ì—­ í•­êµ¬ë©´ ê¸ˆë°© ë‚´ë¦°ë‹¤ â†’ False\n",
        "        if car['dest'] == self.current_port:\n",
        "            return False\n",
        "        # (2) ê²Œì´íŠ¸ì—ì„œ ë‘-ì„¸ ì¹¸ ì´ë‚´(manhattan depth <3)ë©´ í†µê³¼ ëŒ€ê¸°ì—´ â†’ False\n",
        "        gate_depth = len(self._get_or_compute_path(node_idx, 0)) - 1\n",
        "        return gate_depth >= 3\n",
        "\n",
        "\n",
        "    def _find_best_spot(self, car_id_to_load: int) -> tuple[int, list]:\n",
        "        car_dest = self.cars[car_id_to_load]['dest']\n",
        "        cars_to_leave_later = {\n",
        "            cid for cid in self.cars_on_board\n",
        "            if self.cars[cid]['dest'] > car_dest\n",
        "        }\n",
        "\n",
        "        best = None\n",
        "        for spot in range(1, self.num_nodes):\n",
        "            if self.node_status[spot] != -1:\n",
        "                continue\n",
        "            path = self._get_or_compute_path(0, spot)\n",
        "            if not path:\n",
        "                continue\n",
        "\n",
        "            # â”€â”€ ì°¨ë‹¨ ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "            path_set = set(path[1:])\n",
        "            hard_blocks = sum(self._is_hard_blocker(n) for n in path_set)\n",
        "\n",
        "            # (1) í•˜ë“œ ë¸”ë¡ 2ê°œ ì´ˆê³¼ë©´ í›„ë³´ ìì²´ë¥¼ ë²„ë¦°ë‹¤\n",
        "            HARD_BLOCK_LIMIT = 2\n",
        "            if hard_blocks > HARD_BLOCK_LIMIT:\n",
        "                continue\n",
        "\n",
        "            soft_blocks = sum(\n",
        "                1 for n in path_set\n",
        "                if self.node_status[n] != -1\n",
        "            ) - hard_blocks\n",
        "            later_blocks = sum(\n",
        "                1 for cid in cars_to_leave_later\n",
        "                if self.car_locations[cid] in path_set\n",
        "            )\n",
        "\n",
        "            dist = len(path) - 1\n",
        "            dest_penalty = abs(car_dest - self.current_port) ** 0.5\n",
        "\n",
        "            score = (\n",
        "                1.0 * hard_blocks   +   # ë°˜ë“œì‹œ ì¹˜ì›Œì•¼ í•  ë²½\n",
        "                0.3 * soft_blocks   +   # ê³§ ë¹ ì§ˆ ë²½\n",
        "                0.7 * later_blocks  +   # ë‚˜ì¤‘ ì¶œí•­ ì°¨ë¥¼ ë§‰ëŠ” ì •ë„\n",
        "                0.02 * dist         +   # ë©€ë¦¬ ë°€ì–´ë„£ê¸°\n",
        "                0.01 * dest_penalty     # ëª©ì ì§€ í¸ì°¨ ì™„í™”\n",
        "            )\n",
        "\n",
        "\n",
        "            score += random.uniform(0.0, 0.001)   # tiny tie-breaker\n",
        "\n",
        "            cand = (score, spot, path)\n",
        "            best = min(best, cand) if best else cand\n",
        "\n",
        "        return (-1, []) if best is None else (best[1], best[2])\n",
        "\n",
        "    def _find_best_internal_spot(self, car_id_to_move: int) -> tuple[int, list]:\n",
        "        start = self.car_locations.get(car_id_to_move, None)\n",
        "        if start is None:\n",
        "            return -1, []\n",
        "\n",
        "        tried: set[int] = set()\n",
        "        MAX_RETRY = 5                       # ë‹¤ë¥¸ ìë¦¬ ìµœëŒ€ 5ê°œê¹Œì§€ ì‹œë„\n",
        "        for _ in range(MAX_RETRY):\n",
        "            # â”€â”€ ìµœì†Ÿê°’ í›„ë³´ ì„ ì • (ì´ì „ê³¼ ë™ì¼ ë¡œì§) â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "            best = None\n",
        "            for spot in range(1, self.num_nodes):\n",
        "                if spot in tried or self.node_status[spot] != -1:\n",
        "                    continue\n",
        "                path = self._get_or_compute_path(start, spot)\n",
        "                if not path:\n",
        "                    continue\n",
        "                inner = set(path[1:-1])\n",
        "                if any(self.node_status[n] != -1 for n in inner):\n",
        "                    continue\n",
        "                unblock_gain = sum(\n",
        "                    1 for cid in self.cars_on_board\n",
        "                    if self.cars[cid]['dest'] == self.current_port and\n",
        "                      self.car_locations[cid] in inner\n",
        "                )\n",
        "                dist       = len(path) - 1\n",
        "                gate_depth = dist\n",
        "                score      = 0.5 * dist - 0.8 * unblock_gain + 0.05 * gate_depth\n",
        "                best       = min(best, (score, spot, path)) if best else (score, spot, path)\n",
        "\n",
        "            # í›„ë³´ê°€ ì—†ì—ˆë‹¤\n",
        "            if best is None:\n",
        "                return -1, []\n",
        "\n",
        "            spot, path = best[1], best[2]\n",
        "            # ì‹¤ì œë¡œ ê¸¸ì´ ê¹¨ë—í•˜ë©´ ì—¬ê¸°ì„œ ë\n",
        "            if not any(self.node_status[n] != -1 for n in path[1:-1]):\n",
        "                return spot, path\n",
        "\n",
        "            # ì•„ë‹ˆë©´ ë‹¤ì‹œ ì‹œë„\n",
        "            tried.add(spot)\n",
        "\n",
        "        return -1, []\n",
        "\n",
        "    def step(\n",
        "        self,\n",
        "        action: tuple[str, int]\n",
        "    ) -> tuple[Data, float, float, bool]:\n",
        "        \"\"\"\n",
        "        ShipEnv 1-step ì‹œë®¬ë ˆì´ì…˜.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        action : (action_type:str, car_id:int)\n",
        "            * action_type âˆˆ {'LOAD','UNLOAD','RELOCATE_INTERNAL'}\n",
        "            * car_id == -1 ëŠ” PORT ë‹¨ì—ì„œë§Œ ì“°ëŠ” dummy ê°’\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        state  : Data   â”€ PyG ê·¸ë˜í”„(ë…¸ë“œíŠ¹ì„±Â·ê¸€ë¡œë²ŒíŠ¹ì„± í¬í•¨)\n",
        "        reward : float  â”€ worker-level scalar reward\n",
        "        cost   : float  â”€ pure path-moving cost (ê³ ì •+ê±°ë¦¬)\n",
        "        done   : bool   â”€ í•­ë¡œê°€ ëë‚˜ë©´ True\n",
        "        \"\"\"\n",
        "        act_type, cid = action\n",
        "        cost = 0.0\n",
        "        event_rew = 0.0                       # (extrinsic + intrinsic)\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # 0) ë°˜ë³µ/í•‘í íŒ¨ë„í‹°  (cid == -1 ì´ë©´ ìƒëµ)\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        if cid != -1:\n",
        "            prev = self.last_car_action.get(cid)\n",
        "\n",
        "            # (a) ê°™ì€ í–‰ë™ íƒ€ì… ì—°ì†            â†’ ì‚´ì§ -0.1\n",
        "            if prev == act_type:\n",
        "                event_rew -= 0.5\n",
        "\n",
        "            # (b) ì§ì „ UNLOAD â†’ ê³§ë°”ë¡œ LOAD     â†’ ê°•í•˜ê²Œ -5.0\n",
        "            elif prev == 'UNLOAD' and act_type == 'LOAD':\n",
        "                event_rew -= 5.0\n",
        "\n",
        "            # ê¸°ë¡ ê°±ì‹ \n",
        "            self.last_car_action[cid] = act_type\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # 1) LOAD\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        if act_type == 'LOAD':\n",
        "            tgt, path = self._find_best_spot(cid)\n",
        "            if tgt == -1:                          # ìë¦¬ê°€ ì—†ìŒ\n",
        "                self.load_fail_streak += 1         # ëˆ„ì  +1\n",
        "                dyn_pen = -1.0 * (1 + 0.3 * self.load_fail_streak)\n",
        "                return self._get_state(), dyn_pen, 0.0, False\n",
        "            else:\n",
        "                # ìë¦¬ë¥¼ ì°¾ì•˜ìœ¼ë©´ streak ì´ˆê¸°í™”\n",
        "                self.load_fail_streak = 0\n",
        "\n",
        "            cost += self._calculate_path_cost(path)\n",
        "            self.node_status[tgt] = cid\n",
        "            self.car_locations[cid] = tgt\n",
        "            self.cars_on_board.add(cid)\n",
        "\n",
        "            if cid in self.temporarily_unloaded_cars:\n",
        "                self.temporarily_unloaded_cars.remove(cid)\n",
        "            else:\n",
        "                event_rew += 0.1   # â€œì‹ ê·œ ì„ ì â€ ì†Œì • ë³´ìƒ\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # 2) UNLOAD\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        elif act_type == 'UNLOAD':\n",
        "            start = self.car_locations.get(cid)\n",
        "            if start is None:      # ì°¨ê°€ ì‹¤ì œë¡œ ì•ˆ ìˆë‹¤?\n",
        "                return self._get_state(), -1.0, 0.0, False\n",
        "\n",
        "            path = self._get_or_compute_path(start, 0)\n",
        "            cost += self._calculate_path_cost(path)\n",
        "\n",
        "            # ì„ ë°• ìƒíƒœ ì—…ë°ì´íŠ¸\n",
        "            self.node_status[start] = -1\n",
        "            self.cars_on_board.remove(cid)\n",
        "            self.car_locations.pop(cid, None)\n",
        "\n",
        "            car = self.cars[cid]\n",
        "            if car['dest'] == self.current_port:          # ëª©ì ì§€ ë„ì°©\n",
        "                self.delivered_cars.add(cid)\n",
        "                event_rew += 1.0\n",
        "            else:                                         # ì„ì‹œ í•˜ì—­\n",
        "                self.temporarily_unloaded_cars.add(cid)\n",
        "                self.relocations_this_episode += 1\n",
        "                event_rew -= 0.1\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # 3) RELOCATE_INTERNAL\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        elif act_type == 'RELOCATE_INTERNAL':\n",
        "            tgt, path = self._find_best_internal_spot(cid)\n",
        "            if tgt == -1:          # ìë¦¬ ëª» ì°¾ìŒ\n",
        "                event_rew -= 0.2\n",
        "            else:\n",
        "                cost += self._calculate_path_cost(path)\n",
        "                src = self.car_locations.get(cid)\n",
        "                if src is not None:\n",
        "                    self.node_status[src] = -1\n",
        "                self.node_status[tgt] = cid\n",
        "                self.car_locations[cid] = tgt\n",
        "                self.relocations_this_episode += 1\n",
        "                event_rew -= 0.02\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # 4) Reward, done flag, state ë°˜í™˜\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        # ê±°ë¦¬ë¹„ìš©ì€ 0.1 ë°°ë§Œ ë²Œì ìœ¼ë¡œ í™˜ì‚°\n",
        "        reward = event_rew - (cost / self.fixed_cost) * 0.1 if self.fixed_cost else event_rew\n",
        "        done   = self.current_port >= self.num_ports\n",
        "\n",
        "        return self._get_state(), reward, cost, done\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# ì„¹ì…˜ 3: ê³„ì¸µì  í™˜ê²½ Wrapper (ë¦¬íŒ©í† ë§ ë²„ì „)\n",
        "# ==============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.data import Batch\n",
        "\n",
        "class HierarchicalEnvWrapper:\n",
        "    def __init__(self, problem_data: dict, max_num_ports: int,\n",
        "                 worker_agent, config):\n",
        "        self.problem_data     = problem_data\n",
        "        self.max_num_ports    = max_num_ports\n",
        "        self.worker_agent     = worker_agent\n",
        "        self.config           = config\n",
        "        self.last_manager_action = None\n",
        "\n",
        "        # â”€â”€ í•µì‹¬ ê°ì²´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        self.ship_env = ShipEnv(problem_data, max_num_ports)\n",
        "\n",
        "        # â˜… ì²´ë¥˜ ìŠ¤í… ì¹´ìš´í„° ì´ˆê¸°í™”\n",
        "        self.steps_on_port = 0          # â† ì—†ë˜ í•„ë“œ\n",
        "\n",
        "        # Manager action ì¸ì½”ë”©\n",
        "        self.manager_action_map = {\n",
        "            0: 'CLEAR_BLOCKERS',\n",
        "            1: 'FINISH_UNLOAD',\n",
        "            2: 'FINISH_LOAD',\n",
        "            3: 'CLEAR_TEMP',\n",
        "            4: 'PROCEED_TO_NEXT_PORT'\n",
        "        }\n",
        "        self.goal_embedding = nn.Embedding(\n",
        "            self.config.MANAGER_ACTION_DIM,\n",
        "            self.config.GOAL_EMBED_DIM\n",
        "        ).to(self.worker_agent.device)\n",
        "\n",
        "\n",
        "    def _calculate_potential(self) -> float:\n",
        "        \"\"\"ìƒíƒœì˜ ì ì¬ì  ê°€ì¹˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤ (Potential-Based Reward Shapingìš©).\"\"\"\n",
        "        s = self.ship_env\n",
        "\n",
        "        # ì²˜ë¦¬í•´ì•¼ í•  ì‘ì—…ì´ ì ì„ìˆ˜ë¡ ì ì¬ ê°€ì¹˜ê°€ ë†’ìŠµë‹ˆë‹¤ (ë”°ë¼ì„œ ìŒìˆ˜ë¡œ ê³„ì‚°).\n",
        "\n",
        "        # 1. í˜„ì¬ í•­êµ¬ì—ì„œ ì‹¤ì–´ì•¼ í•  ëŒ€ê¸° ì°¨ëŸ‰ ìˆ˜\n",
        "        waiting_to_load = len([\n",
        "            c for c in s.cars if c['origin'] == s.current_port and\n",
        "            c['id'] not in s.cars_on_board and\n",
        "            c['id'] not in s.temporarily_unloaded_cars\n",
        "        ])\n",
        "\n",
        "        # 2. í˜„ì¬ í•­êµ¬ì—ì„œ ë‚´ë ¤ì•¼ í•  ì°¨ëŸ‰ ìˆ˜ (ê°€ì¤‘ì¹˜ ë¶€ì—¬)\n",
        "        due_to_unload = len([\n",
        "            c for c in s.cars if c['id'] in s.cars_on_board and\n",
        "            c['dest'] == s.current_port\n",
        "        ])\n",
        "\n",
        "        # 3. ì„ì‹œë¡œ ë‚´ë¦° ì°¨ëŸ‰ ìˆ˜ (ë‹¤ì‹œ ì‹¤ì–´ì•¼ í•¨)\n",
        "        temp_unloaded = len(s.temporarily_unloaded_cars)\n",
        "\n",
        "        # ì ì¬ë ¥ì€ ì´ë“¤ì˜ ìŒìˆ˜ ê°€ì¤‘í•©. (ì‘ì—…ì´ ë§ì„ìˆ˜ë¡ ê°€ì¹˜ê°€ ë‚®ìŒ)\n",
        "        # íŠ¹íˆ 'ë‚´ë ¤ì•¼ í•  ì°¨ëŸ‰'ì„ ì²˜ë¦¬í•˜ëŠ” ê²ƒì„ ë” ì¤‘ìš”í•˜ê²Œ ë³´ê¸° ìœ„í•´ ê°€ì¤‘ì¹˜ 1.5 ë¶€ì—¬\n",
        "        potential = - (1.0 * waiting_to_load + 1.5 * due_to_unload + 1.0 * temp_unloaded)\n",
        "        return potential\n",
        "\n",
        "\n",
        "    def reset(self, prob_data=None):\n",
        "        if prob_data is not None:\n",
        "            self.problem_data = prob_data\n",
        "            self.ship_env     = ShipEnv(self.problem_data, self.max_num_ports)\n",
        "        else:\n",
        "            self.ship_env.reset()\n",
        "\n",
        "        # â˜… í¬íŠ¸ ì²´ë¥˜ ì¹´ìš´í„°ë„ í•­ìƒ ë¦¬ì…‹\n",
        "        self.steps_on_port       = 0\n",
        "        self.last_manager_action = None\n",
        "        return self._get_manager_state()\n",
        "\n",
        "\n",
        "    def _get_manager_state(self):\n",
        "        s = self.ship_env\n",
        "        total_slots = s.num_nodes - 1 if s.num_nodes > 1 else 1\n",
        "        port_norm = s.current_port / s.num_ports\n",
        "        free_slots_norm = sum(1 for n in s.node_status[1:] if n == -1) / total_slots\n",
        "        waiting_to_load = len([c for c in s.cars if c['origin']==s.current_port and c['id'] not in s.cars_on_board and c['id'] not in s.temporarily_unloaded_cars])\n",
        "        due_to_unload = len([c for c in s.cars if c['id'] in s.cars_on_board and c['dest']==s.current_port])\n",
        "        on_board_dests = [s.cars[cid]['dest'] for cid in s.cars_on_board]\n",
        "        avg_dest_dist = (sum(d - s.current_port for d in on_board_dests)/len(on_board_dests)) if on_board_dests else 0.0\n",
        "\n",
        "        return torch.tensor([\n",
        "            port_norm,\n",
        "            free_slots_norm,\n",
        "            waiting_to_load / s.total_cars,\n",
        "            due_to_unload / s.total_cars,\n",
        "            len(s.temporarily_unloaded_cars) / s.total_cars,\n",
        "            avg_dest_dist / s.num_ports\n",
        "        ], dtype=torch.float)\n",
        "\n",
        "    def _is_goal_achieved(self, goal_str):\n",
        "        \"\"\"\n",
        "        ê° ëª©í‘œ(goal_str)ê°€ ì¶©ì¡±ë˜ì—ˆëŠ”ì§€ íŒë‹¨í•œë‹¤.\n",
        "        CLEAR_BLOCKERSëŠ” â€˜í•˜ì—­ ëŒ€ìƒ ì°¨ê°€ ì—†ìŒâ€™ + â€˜ê²Œì´íŠ¸ ê²½ë¡œì— ì°¨ê°€ ì—†ìŒâ€™ ë‘ ì¡°ê±´ì„ ëª¨ë‘ ë§Œì¡±í•´ì•¼ True.\n",
        "        \"\"\"\n",
        "        s = self.ship_env\n",
        "\n",
        "        # í˜„ì¬ í•­êµ¬ì—ì„œ í•˜ì—­ ëŒ€ìƒ(ëª©ì ì§€ == current_port)ì´ ë‚¨ì•„ ìˆë‚˜?\n",
        "        has_due_to_unload = any(\n",
        "            c['dest'] == s.current_port and c['id'] in s.cars_on_board\n",
        "            for c in s.cars\n",
        "        )\n",
        "\n",
        "        # ê²Œì´íŠ¸(ë…¸ë“œ 0) â†’ ê° ë…¸ë“œ ê²½ë¡œ ì¤‘, ë§‰íŒ ë…¸ë“œê°€ ì¡´ì¬í•˜ë‚˜?\n",
        "        def path_blocked():\n",
        "            for cid in s.cars_on_board:\n",
        "                # ì´ë¯¸ ê²Œì´íŠ¸ê¹Œì§€ ë‚˜ì™”ë‹¤ê°€ temp ë¡œ ë‚´ë ¤ê°„ ì°¨ëŸ‰ì´ë©´ blocker ì•„ë‹˜\n",
        "                if cid in s.temporarily_unloaded_cars:\n",
        "                    continue\n",
        "                node_idx = s.car_locations.get(cid, None)\n",
        "                if node_idx is None:\n",
        "                    continue\n",
        "                path = s._get_or_compute_path(0, node_idx)\n",
        "                if not path:\n",
        "                    continue\n",
        "                if any(s.node_status[n] != -1 for n in path[1:]):\n",
        "                    return True\n",
        "            return False\n",
        "\n",
        "        if goal_str == 'FINISH_UNLOAD':\n",
        "            return not has_due_to_unload                     # í•˜ì—­ ëŒ€ìƒì´ ë” ì´ìƒ ì—†ìŒ\n",
        "        elif goal_str == 'CLEAR_BLOCKERS':\n",
        "            return (not has_due_to_unload) and (not path_blocked())\n",
        "        elif goal_str == 'FINISH_LOAD':\n",
        "            return not any(\n",
        "                c['origin'] == s.current_port and\n",
        "                c['id'] not in s.cars_on_board and\n",
        "                c['id'] not in s.temporarily_unloaded_cars\n",
        "                for c in s.cars\n",
        "            )\n",
        "        elif goal_str == 'CLEAR_TEMP':\n",
        "            return len(s.temporarily_unloaded_cars) == 0\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "\n",
        "    # ======================================================================\n",
        "    #  HierarchicalEnvWrapper.step  â€•  ìµœì¢… êµì²´ë³¸\n",
        "    # ======================================================================\n",
        "    def step(self, manager_action_idx: int, *, greedy_worker: bool = False):\n",
        "        \"\"\"\n",
        "        Manager ì•¡ì…˜ì„ ë°›ì•„ Worker ë¡¤ì•„ì›ƒê¹Œì§€ ìˆ˜í–‰í•œ ë’¤\n",
        "        (ë‹¤ìŒ Manager ìƒíƒœ, Manager ë³´ìƒ, done, info) ë°˜í™˜\n",
        "        \"\"\"\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ë³¸ ì„¸íŒ… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        goal_str         = self.manager_action_map[manager_action_idx]\n",
        "        goal_embed       = self.goal_embedding(\n",
        "            torch.tensor([manager_action_idx], device=self.worker_agent.device)\n",
        "        )\n",
        "        potential_before = self._calculate_potential()\n",
        "\n",
        "        # í•­êµ¬ ì²´ë¥˜ ìŠ¤í… ì¹´ìš´í„°  â”€â”€ NEW â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        self.steps_on_port += 1\n",
        "\n",
        "        # Worker í†µê³„\n",
        "        worker_steps         = 0\n",
        "        total_cost           = 0.0\n",
        "        total_worker_reward  = 0.0\n",
        "        no_progress          = 0\n",
        "        no_progress_trigger  = False\n",
        "\n",
        "        # Storage (í•™ìŠµ ëª¨ë“œì¼ ë•Œë§Œ)\n",
        "        worker_storage = None\n",
        "        if (goal_str != 'PROCEED_TO_NEXT_PORT') and (not greedy_worker):\n",
        "            worker_storage = PPOStorage(\n",
        "                self.config.WORKER_NUM_STEPS_PER_UPDATE,\n",
        "                (2,),                         # [action_type, car_id]\n",
        "                self.worker_agent.device\n",
        "            )\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë””ë²„ê·¸Â·ì•ˆì •ì„± íŒŒë¼ë¯¸í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        BONUS_EVERY_N      = 100\n",
        "        PERIODIC_EVERY_N   = 150\n",
        "        STREAK_LIMIT       = 15             # â˜… â€œë™ì¼ í–‰ë™Â·ì°¨ëŸ‰â€ ì—°ì† í—ˆìš©ì¹˜\n",
        "        EPS                = 1e-6\n",
        "        last_dbg_key       = None\n",
        "        bonus_skip_counter = 0\n",
        "        same_action_streak = 0              # â˜… streak ì¹´ìš´í„°\n",
        "        last_worker_key    = None           # â˜… (act_type, car_id)\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Worker ë£¨í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        if goal_str != 'PROCEED_TO_NEXT_PORT':\n",
        "            batch_graph = Batch.from_data_list(\n",
        "                [self.ship_env._get_state()]\n",
        "            ).to(self.worker_agent.device)\n",
        "\n",
        "            last_worker_key    = None\n",
        "            same_action_streak = 0        # streak ì¹´ìš´í„°\n",
        "\n",
        "            for i in range(self.config.WORKER_MAX_STEPS_PER_GOAL):\n",
        "                worker_steps = i + 1\n",
        "\n",
        "                # 0-a) ë™ì¼ í–‰ë™Â·ì°¨ëŸ‰ ìŠ¤íŠ¸ë¦­ ì´ˆê³¼   â”€â”€ NEW â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "                if same_action_streak >= STREAK_LIMIT:\n",
        "                    logging.info(f\"[WRK] break â€” same action repeated {STREAK_LIMIT} times\")\n",
        "                    no_progress_trigger = True\n",
        "                    break\n",
        "\n",
        "                # 0-b) â€˜ì§„ë„ ì—†ìŒâ€™ í•œë„ ì´ˆê³¼ ì‹œ íƒˆì¶œ\n",
        "                if (goal_str == 'FINISH_LOAD') and (no_progress >= self.config.NO_PROGRESS_LIMIT):\n",
        "                    no_progress_trigger = True\n",
        "                    break\n",
        "\n",
        "                # 1) ëª©í‘œ ë‹¬ì„± ì—¬ë¶€\n",
        "                if self._is_goal_achieved(goal_str):\n",
        "                    break\n",
        "\n",
        "                # 2) í•©ë²• ì•¡ì…˜\n",
        "                legal_actions = self.ship_env.get_legal_actions(for_worker=True)\n",
        "                if not legal_actions:\n",
        "                    break\n",
        "\n",
        "                # 3) ìµœì‹  ê·¸ë˜í”„ ë®ì–´ì“°ê¸°\n",
        "                tmp_state        = self.ship_env._get_state()\n",
        "                batch_graph.x    = tmp_state.x.to(batch_graph.x.device)\n",
        "                batch_graph.global_features = tmp_state.global_features.to(batch_graph.global_features.device)\n",
        "\n",
        "                # 4) í–‰ë™ ì„ íƒ\n",
        "                action, at, logp, ent, val = self.worker_agent.get_action_and_value(\n",
        "                    batch_graph, legal_actions, goal_embed, greedy=greedy_worker\n",
        "                )\n",
        "                action_type, car_id = action\n",
        "\n",
        "                # 5) Streak ì—…ë°ì´íŠ¸  â”€â”€ NEW â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "                worker_key = (action_type, car_id)\n",
        "                if worker_key == last_worker_key:\n",
        "                    same_action_streak += 1\n",
        "                else:\n",
        "                    same_action_streak = 1\n",
        "                last_worker_key = worker_key\n",
        "                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "                # 6) intrinsic ë³´ë„ˆìŠ¤ ëŒ€ìƒ í”Œë˜ê·¸\n",
        "                is_temp_before = (car_id != -1) and (car_id in self.ship_env.temporarily_unloaded_cars)\n",
        "                is_due_before  = (car_id != -1) and (self.ship_env.cars[car_id]['dest'] == self.ship_env.current_port)\n",
        "\n",
        "                # 7) í™˜ê²½ í•œ ìŠ¤í…\n",
        "                _, worker_reward, move_cost, overall_done = self.ship_env.step(action)\n",
        "                total_cost          += move_cost\n",
        "                intrinsic_reward     = 0.0\n",
        "                if goal_str == 'FINISH_UNLOAD' and action_type == 'UNLOAD' and is_due_before:\n",
        "                    intrinsic_reward += 0.8\n",
        "                elif goal_str == 'CLEAR_TEMP' and action_type == 'LOAD' and is_temp_before:\n",
        "                    intrinsic_reward += 0.4\n",
        "                worker_reward       += intrinsic_reward\n",
        "                total_worker_reward += worker_reward\n",
        "\n",
        "                # 8) no-progress ì—…ë°ì´íŠ¸\n",
        "                load_succeeded = (action_type == 'LOAD') and (worker_reward > 0)\n",
        "                if goal_str == 'FINISH_LOAD' and load_succeeded:\n",
        "                    no_progress = 0\n",
        "                else:\n",
        "                    no_progress += 1\n",
        "\n",
        "                # 9) Storage ì €ì¥\n",
        "                if worker_storage is not None:\n",
        "                    worker_storage.add(tmp_state, at, logp, worker_reward, overall_done, val)\n",
        "\n",
        "                # 10) ë””ë²„ê·¸ ë¡œê·¸ (â€¦ ìƒëµ, ê¸°ì¡´ê³¼ ë™ì¼) ----------------------\n",
        "\n",
        "                if overall_done:\n",
        "                    break\n",
        "\n",
        "            # --- Worker ë£¨í”„ ì¢…ë£Œ í›„ streak ì´ˆê¸°í™” -----------------\n",
        "            same_action_streak = 0        # â† ë‹¤ìŒ goal ë¡œ ì´ì–´ì§€ì§€ ì•Šê²Œ\n",
        "\n",
        "            # 11) PPO ì—…ë°ì´íŠ¸\n",
        "            if worker_storage and worker_storage.step > 0 and not greedy_worker:\n",
        "                self.worker_agent.update(worker_storage, goal_embed.detach())\n",
        "\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Manager ë³´ìƒ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        goal_done    = self._is_goal_achieved(goal_str)\n",
        "        event_reward = 0.0\n",
        "\n",
        "        # ì‹¤íŒ¨ íŒ¨ë„í‹°\n",
        "        if (worker_steps >= self.config.WORKER_MAX_STEPS_PER_GOAL) and (not goal_done):\n",
        "            penalty = (2 * self.config.TIMEOUT_PENALTY) if goal_str == 'CLEAR_BLOCKERS' else self.config.TIMEOUT_PENALTY\n",
        "            event_reward += penalty\n",
        "\n",
        "        # PBRS\n",
        "        potential_after = self._calculate_potential()\n",
        "        event_reward += self.config.SHAPING_REWARD_WEIGHT * (\n",
        "            self.config.MANAGER_GAMMA * potential_after - potential_before\n",
        "        )\n",
        "\n",
        "        # í•­êµ¬ ì´ë™ ëª…ë ¹ ì²˜ë¦¬\n",
        "        if goal_str == 'PROCEED_TO_NEXT_PORT':\n",
        "            waiting = [c for c in self.ship_env.cars if c['origin']==self.ship_env.current_port\n",
        "                       and c['id'] not in self.ship_env.cars_on_board\n",
        "                       and c['id'] not in self.ship_env.temporarily_unloaded_cars]\n",
        "            can_go = not (waiting or self.ship_env.temporarily_unloaded_cars)\n",
        "            if can_go:\n",
        "                self.ship_env.current_port += 1\n",
        "                self.steps_on_port = 0            # â˜… ì²´ë¥˜ ì¹´ìš´í„° ë¦¬ì…‹\n",
        "                event_reward += 5.0\n",
        "                goal_done = True\n",
        "            else:\n",
        "                event_reward -= 10.0\n",
        "                goal_done = False\n",
        "\n",
        "        # â”€â”€ í•­êµ¬ ì²´ë¥˜ ì œí•œ íŒ¨ë„í‹°ë§Œ ì ìš©(ê°•ì œ ì´ë™ X)  â”€â”€ NEW â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        MAX_STEP_PER_PORT = 4000\n",
        "        if self.steps_on_port > MAX_STEP_PER_PORT:\n",
        "            event_reward += self.config.TIMEOUT_PENALTY * 5   # í° íŒ¨ë„í‹°\n",
        "            logging.info(\"[MGR] Port-stay limit exceeded â€” penalty applied\")\n",
        "            self.steps_on_port = MAX_STEP_PER_PORT            # ë” ì¦ê°€ ì•ˆ í•¨\n",
        "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "        # ìµœì¢… Manager reward\n",
        "        manager_reward  = event_reward - (total_cost * 0.001)\n",
        "        manager_reward -= self.config.STEP_PENALTY_WEIGHT * worker_steps\n",
        "        if self.last_manager_action == manager_action_idx:\n",
        "            manager_reward += self.config.REPEAT_PENALTY\n",
        "        if no_progress_trigger:\n",
        "            manager_reward += self.config.NO_PROGRESS_PENALTY\n",
        "\n",
        "        self.last_manager_action = manager_action_idx\n",
        "\n",
        "        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ë³´ë„ˆìŠ¤/íŒ¨ë„í‹°\n",
        "        done = self.ship_env.current_port >= self.ship_env.num_ports\n",
        "        if done:\n",
        "            if len(self.ship_env.delivered_cars) == self.ship_env.total_cars:\n",
        "                manager_reward += 1000.0\n",
        "            else:\n",
        "                manager_reward -= (self.ship_env.total_cars - len(self.ship_env.delivered_cars)) * 10.0\n",
        "\n",
        "        # ë‹¤ìŒ Manager state\n",
        "        next_state = self._get_manager_state()\n",
        "\n",
        "        info = {\n",
        "            'steps'              : worker_steps,\n",
        "            'goal'               : goal_str,\n",
        "            'success'            : goal_done,\n",
        "            'cost'               : total_cost,\n",
        "            'worker_total_reward': total_worker_reward,\n",
        "        }\n",
        "        return next_state, manager_reward, done, info\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# ì„¹ì…˜ 4: ì—ì´ì „íŠ¸ ë° ë„¤íŠ¸ì›Œí¬ (ë¦¬íŒ©í† ë§ ë²„ì „)\n",
        "# ==============================================================================\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from torch_geometric.data import Batch\n",
        "from torch_geometric.nn import GlobalAttention, global_mean_pool\n",
        "\n",
        "# --- PPO ì•Œê³ ë¦¬ì¦˜ì„ ìœ„í•œ ê²½í—˜ ì €ì¥ì†Œ ---\n",
        "\n",
        "class PPOStorage:\n",
        "    \"\"\"PPO í•™ìŠµì„ ìœ„í•œ ë¡¤ì•„ì›ƒ(rollout) ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤.\"\"\"\n",
        "    def __init__(self, num_steps: int, action_shape: tuple, device: torch.device, state_shape: tuple = None, manager_action_dim: int = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_steps (int): ì €ì¥í•  ì´ ìŠ¤í… ìˆ˜.\n",
        "            action_shape (tuple): í–‰ë™ì˜ í˜•íƒœ.\n",
        "            device (torch.device): ë°ì´í„°ê°€ ì €ì¥ë  ì¥ì¹˜ (CPU ë˜ëŠ” CUDA).\n",
        "            state_shape (tuple, optional): ìƒíƒœì˜ í˜•íƒœ. ê·¸ë˜í”„ ë°ì´í„°ê°€ ì•„ë‹ˆë©´ ì§€ì •.\n",
        "            manager_action_dim (int, optional): Managerì˜ ê²½ìš°, ì•¡ì…˜ ë§ˆìŠ¤í¬ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´ í•„ìš”.\n",
        "        \"\"\"\n",
        "        self.num_steps = num_steps\n",
        "        self.device = device\n",
        "        self.step = 0\n",
        "        self.is_graph_data = (state_shape is None)\n",
        "\n",
        "        # ë°ì´í„° ì €ì¥ ë²„í¼ ì´ˆê¸°í™”\n",
        "        if self.is_graph_data:\n",
        "            self.obs = [None] * num_steps\n",
        "        else:\n",
        "            self.obs = torch.zeros((num_steps,) + state_shape, device=device)\n",
        "\n",
        "        if isinstance(action_shape, int): action_shape = (action_shape,)\n",
        "        self.actions = torch.zeros((num_steps,) + action_shape, device=device, dtype=torch.long)\n",
        "        self.logprobs = torch.zeros(num_steps, device=device)\n",
        "        self.rewards = torch.zeros(num_steps, device=device)\n",
        "        self.dones = torch.zeros(num_steps, device=device)\n",
        "        self.values = torch.zeros(num_steps, device=device)\n",
        "\n",
        "        # [í•µì‹¬ ìˆ˜ì •] Managerì˜ ìƒíƒœ ì˜ì¡´ì  ì•¡ì…˜ ë§ˆìŠ¤í¬ë¥¼ ì €ì¥í•  ê³µê°„\n",
        "        if manager_action_dim:\n",
        "            self.masks = torch.zeros((num_steps, manager_action_dim), device=device)\n",
        "        else:\n",
        "            self.masks = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.step = 0\n",
        "\n",
        "    def add(self, obs, action, logprob, reward, done, value, mask=None):\n",
        "        \"\"\"í•œ ìŠ¤í…ì˜ ê²½í—˜ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\"\"\"\n",
        "        if self.step >= self.num_steps: return\n",
        "\n",
        "        if self.is_graph_data:\n",
        "            self.obs[self.step] = obs.cpu() # GPU ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ CPUì— ì €ì¥\n",
        "        else:\n",
        "            self.obs[self.step].copy_(torch.as_tensor(obs, device=self.device))\n",
        "\n",
        "        self.actions[self.step] = action\n",
        "        self.logprobs[self.step] = logprob\n",
        "        self.rewards[self.step] = torch.tensor(reward, dtype=torch.float32)\n",
        "        self.dones[self.step] = torch.tensor(done, dtype=torch.float32)\n",
        "        self.values[self.step] = value.detach()\n",
        "\n",
        "        # [í•µì‹¬ ìˆ˜ì •] Managerì˜ ì•¡ì…˜ ë§ˆìŠ¤í¬ ì €ì¥\n",
        "        if self.masks is not None and mask is not None:\n",
        "            self.masks[self.step] = mask.detach()\n",
        "\n",
        "        self.step += 1\n",
        "\n",
        "    def is_full(self) -> bool:\n",
        "        return self.step >= self.num_steps\n",
        "\n",
        "    def compute_returns_and_advantages(self, last_value: torch.Tensor, gamma: float, gae_lambda: float):\n",
        "        \"\"\"GAEë¥¼ ì‚¬ìš©í•˜ì—¬ ë³´ìƒ(Return)ê³¼ ì–´ë“œë°´í‹°ì§€(Advantage)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\"\"\"\n",
        "        advantages = torch.zeros_like(self.rewards).to(self.device)\n",
        "        last_gae = 0.0\n",
        "        for t in reversed(range(self.num_steps)):\n",
        "            next_non_terminal = 1.0 - self.dones[t]\n",
        "            next_value = last_value if t == self.num_steps - 1 else self.values[t + 1]\n",
        "\n",
        "            delta = self.rewards[t] + gamma * next_value * next_non_terminal - self.values[t]\n",
        "            last_gae = delta + gamma * gae_lambda * next_non_terminal * last_gae\n",
        "            advantages[t] = last_gae\n",
        "\n",
        "        # ì–´ë“œë°´í‹°ì§€ë¥¼ ì´ìš©í•´ ìµœì¢… Return ê³„ì‚°\n",
        "        self.returns = advantages + self.values\n",
        "        # ì–´ë“œë°´í‹°ì§€ ì •ê·œí™” (í•™ìŠµ ì•ˆì •í™”)\n",
        "        self.advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "\n",
        "\n",
        "# --- ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜ ---\n",
        "# WorkerNetwork í´ë˜ìŠ¤ ì „ì²´\n",
        "class WorkerNetwork(nn.Module):\n",
        "    \"\"\"Worker ì—ì´ì „íŠ¸ì˜ ì •ì±… ë° ê°€ì¹˜ ì‹ ê²½ë§\"\"\"\n",
        "    def __init__(self, node_feature_size: int, global_feature_size: int, max_cars: int, num_nodes: int, config: Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        embed_dim = self.config.GNN_EMBED_DIM\n",
        "        self.node_input_proj = nn.Linear(node_feature_size, embed_dim)\n",
        "        self.positional_encoding = nn.Embedding(num_nodes, embed_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4, dim_feedforward=embed_dim*4, dropout=0.1, activation='gelu', batch_first=True, norm_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
        "        self.att_pool = GlobalAttention(gate_nn=nn.Sequential(nn.Linear(embed_dim, embed_dim // 2), nn.ReLU(), nn.Linear(embed_dim // 2, 1)))\n",
        "\n",
        "        mlp_input_dim = embed_dim * 2 + global_feature_size + self.config.GOAL_EMBED_DIM\n",
        "        self.mlp = nn.Sequential(nn.Linear(mlp_input_dim, 512), nn.GELU(), nn.Linear(512, 256), nn.GELU())\n",
        "        self.actor_type_head = nn.Linear(256, 3)\n",
        "        self.actor_load_head = nn.Linear(256, max_cars)\n",
        "        self.actor_unload_head = nn.Linear(256, max_cars)\n",
        "        self.actor_relocate_head = nn.Linear(256, max_cars)\n",
        "        self.critic_head = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, data, goal_embedding):\n",
        "        current_device = goal_embedding.device\n",
        "        x, global_feats, batch_index = data.x.to(current_device), data.global_features.to(current_device), data.batch.to(current_device)\n",
        "        ptr = data.ptr.to(current_device) if hasattr(data, 'ptr') and data.ptr is not None else torch.tensor([0, x.size(0)], device=current_device)\n",
        "\n",
        "        node_embeddings = self.node_input_proj(x)\n",
        "        pos_enc_list = [self.positional_encoding(torch.arange(ptr[i+1] - ptr[i], device=current_device)) for i in range(len(ptr)-1)]\n",
        "        pos_enc = torch.cat(pos_enc_list) if pos_enc_list else torch.empty(0, self.config.GNN_EMBED_DIM, device=current_device)\n",
        "        if node_embeddings.size(0) == pos_enc.size(0): node_embeddings += pos_enc\n",
        "\n",
        "        if len(ptr) > 1 and x.shape[0] > 0: max_len = (ptr[1:] - ptr[:-1]).max().item()\n",
        "        elif x.shape[0] > 0: max_len = x.shape[0]\n",
        "        else: max_len = 1\n",
        "\n",
        "        padded_x, masks_list = [], []\n",
        "        if len(ptr) > 1:\n",
        "            for i in range(len(ptr) - 1):\n",
        "                start, end = ptr[i], ptr[i+1]\n",
        "                graph_len, current_nodes = end - start, node_embeddings[start:end]\n",
        "                pad = torch.zeros(max_len - graph_len, self.config.GNN_EMBED_DIM, device=current_device)\n",
        "                padded_x.append(torch.cat([current_nodes, pad]))\n",
        "                mask = torch.ones(max_len, dtype=torch.bool, device=current_device)\n",
        "                mask[:graph_len] = False\n",
        "                masks_list.append(mask)\n",
        "        else:\n",
        "            padded_x.append(node_embeddings)\n",
        "            masks_list.append(torch.zeros(node_embeddings.shape[0], dtype=torch.bool, device=current_device))\n",
        "\n",
        "        padded_x, attention_mask = torch.stack(padded_x), torch.stack(masks_list)\n",
        "        transformer_out = self.transformer_encoder(padded_x, src_key_padding_mask=attention_mask)\n",
        "        transformer_out_flat = transformer_out[~attention_mask]\n",
        "\n",
        "        graph_emb_mean = global_mean_pool(transformer_out_flat, batch_index)\n",
        "        graph_emb_att = self.att_pool(transformer_out_flat, batch_index)\n",
        "        graph_emb = torch.cat([graph_emb_mean, graph_emb_att], dim=1)\n",
        "\n",
        "        if goal_embedding.shape[0] != graph_emb.shape[0]:\n",
        "            goal_embedding = goal_embedding.expand(graph_emb.shape[0], -1)\n",
        "\n",
        "        combined_features = torch.cat([graph_emb, global_feats, goal_embedding], dim=1)\n",
        "        final_features = self.mlp(combined_features)\n",
        "\n",
        "        # [ìˆ˜ì •] ë°˜í™˜ ê°’ì— graph_emb ì¶”ê°€\n",
        "        return (self.actor_type_head(final_features), self.actor_load_head(final_features),\n",
        "                self.actor_unload_head(final_features), self.actor_relocate_head(final_features),\n",
        "                self.critic_head(final_features).squeeze(-1), graph_emb)\n",
        "\n",
        "\n",
        "\n",
        "class ManagerNetwork(nn.Module):\n",
        "    def __init__(self, state_dim: int, action_dim: int, num_layers: int = 2, nhead: int = 4):\n",
        "        super().__init__()\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.action_embed = nn.Embedding(action_dim, 32)\n",
        "\n",
        "        self.input_proj = nn.Linear(state_dim + 32, 128)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=128,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=256,\n",
        "            batch_first=True,\n",
        "            activation='gelu'\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.actor_head = nn.Linear(128, action_dim)\n",
        "        self.critic_head = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, prev_action_idx: torch.LongTensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state: [B, state_dim]\n",
        "            prev_action_idx: [B]\n",
        "        Returns:\n",
        "            logits: [B, action_dim]\n",
        "            value:  [B]\n",
        "        \"\"\"\n",
        "        a_emb = self.action_embed(prev_action_idx)            # [B, 32]\n",
        "        x = torch.cat([state, a_emb], dim=-1)                 # [B, state_dim + 32]\n",
        "        x = self.input_proj(x).unsqueeze(1)                   # [B, 1, 128]\n",
        "\n",
        "        encoded = self.encoder(x)                             # [B, 1, 128]\n",
        "        h = encoded.squeeze(1)                                # [B, 128]\n",
        "\n",
        "        logits = self.actor_head(h)                           # [B, action_dim]\n",
        "        value  = self.critic_head(h).squeeze(-1)              # [B]\n",
        "        return logits, value\n",
        "\n",
        "# --- ì—ì´ì „íŠ¸ í´ë˜ìŠ¤ ì •ì˜ ---\n",
        "\n",
        "class ManagerAgent:\n",
        "    \"\"\"ìƒìœ„ ë ˆë²¨ì˜ ëª©í‘œë¥¼ ê²°ì •í•˜ëŠ” Manager ì—ì´ì „íŠ¸.\"\"\"\n",
        "    def __init__(self, config: Config, device: torch.device, env_wrapper: HierarchicalEnvWrapper):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.env_wrapper = env_wrapper # í˜„ì¬ í™˜ê²½ ìƒíƒœì— ì ‘ê·¼í•˜ì—¬ ì•¡ì…˜ ë§ˆìŠ¤í‚¹\n",
        "        self.action_map = env_wrapper.manager_action_map\n",
        "        self.type_to_idx = {v: k for k, v in self.env_wrapper.manager_action_map.items()}\n",
        "        self.idx_to_type = {v: k for k, v in self.type_to_idx.items()}\n",
        "\n",
        "        self.network = ManagerNetwork(config.MANAGER_STATE_DIM, config.MANAGER_ACTION_DIM).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.network.parameters(), lr=config.MANAGER_LR, eps=1e-5)\n",
        "\n",
        "\n",
        "    def _build_action_mask(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        ë‹¬ì„±ëœ goal, í˜¹ì€ ë¬¼ë¦¬ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥í•œ goalì˜ ë¡œì§“ì„ -1e9 ë¡œ ë‚´ë ¤ì„œ\n",
        "        softmax í™•ë¥  0 ì´ ë˜ê²Œ ë§Œë“ ë‹¤.\n",
        "        \"\"\"\n",
        "        mask = torch.zeros(1, self.config.MANAGER_ACTION_DIM, device=self.device)\n",
        "        s_env = self.env_wrapper.ship_env # ship_envì— ë” ì‰½ê²Œ ì ‘ê·¼\n",
        "\n",
        "        # 1. ì´ë¯¸ ë‹¬ì„±ëœ ëª©í‘œ ë§ˆìŠ¤í‚¹\n",
        "        for idx, goal in self.env_wrapper.manager_action_map.items():\n",
        "            if self.env_wrapper._is_goal_achieved(goal):\n",
        "                mask[0, idx] = -1e9\n",
        "\n",
        "        # ğŸ’¡ --- [í•µì‹¬ ì¶”ê°€] --- ğŸ’¡\n",
        "        # 2. 'PROCEED_TO_NEXT_PORT'ê°€ ë¶ˆê°€ëŠ¥í•œ ì¡°ê±´ì¼ ë•Œ ë§ˆìŠ¤í‚¹\n",
        "        #   - í•­êµ¬ì— ë‚´ë ¤ì•¼ í•  ì°¨ê°€ ìˆê±°ë‚˜, ì„ì‹œë¡œ ë‚´ë¦° ì°¨ê°€ ìˆìœ¼ë©´ PROCEED ë¶ˆê°€\n",
        "        waiting_cars = any(c['origin'] == s_env.current_port and c['id'] not in s_env.cars_on_board and c['id'] not in s_env.temporarily_unloaded_cars for c in s_env.cars)\n",
        "        has_temp_unloaded = bool(s_env.temporarily_unloaded_cars)\n",
        "\n",
        "        if waiting_cars or has_temp_unloaded:\n",
        "            proceed_idx = self.type_to_idx['PROCEED_TO_NEXT_PORT']\n",
        "            mask[0, proceed_idx] = -1e9\n",
        "        # ğŸ’¡ --- [ìˆ˜ì • ì™„ë£Œ] --- ğŸ’¡\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def get_action_and_value(\n",
        "        self,\n",
        "        state: torch.Tensor,\n",
        "        legal_actions: list[tuple[str,int]],\n",
        "        prev_action_idx: torch.LongTensor,\n",
        "        greedy: bool = False\n",
        "    ) -> tuple: # [ìˆ˜ì •] ë°˜í™˜ íƒ€ì… íŠœí”Œë¡œ ëª…ì‹œ\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state: [state_dim] í¬ê¸°ì˜ í…ì„œ\n",
        "            legal_actions: ê°€ëŠ¥í•œ í–‰ë™ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: [(\"FINISH_LOAD\",-1), â€¦])\n",
        "            prev_action_idx: ì´ì „ì— ì„ íƒí•œ ë§¤ë‹ˆì € ì•¡ì…˜ ì¸ë±ìŠ¤ ([1]-shape LongTensor)\n",
        "            greedy: Trueë©´ íƒìš•ì ìœ¼ë¡œ(max) ì„ íƒ\n",
        "\n",
        "        Returns:\n",
        "            [greedy=False] (action_tensor, logp, ent, value, mask)\n",
        "            - action_tensor (Tensor): í•™ìŠµìš© ì•¡ì…˜ í…ì„œ ([1]-shape LongTensor)\n",
        "            - logp (Tensor): ì„ íƒ í™•ë¥ ì˜ ë¡œê·¸ê°’\n",
        "            - ent (Tensor): ì„ íƒ ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼\n",
        "            - value (Tensor): ìƒíƒœ ê°€ì¹˜ ì¶”ì • (ìŠ¤ì¹¼ë¼)\n",
        "            - mask (Tensor): PPO ì—…ë°ì´íŠ¸ì— ì‚¬ìš©í•  ìƒíƒœ ì˜ì¡´ì  ë§ˆìŠ¤í¬\n",
        "\n",
        "            [greedy=True] (action_idx, None, None, None, None)\n",
        "            - action_idx (int): ì„ íƒëœ ë§¤ë‹ˆì € ì•¡ì…˜ ì¸ë±ìŠ¤ (0~4)\n",
        "        \"\"\"\n",
        "        # 1) í‰ê°€ ëª¨ë“œ\n",
        "        self.network.eval()\n",
        "        with torch.no_grad():\n",
        "            # 2) ë°°ì¹˜ ì°¨ì› ì¶”ê°€: [state_dim] â†’ [1, state_dim]\n",
        "            batch_state = state.unsqueeze(0).to(self.device)\n",
        "\n",
        "            # 3) ë‹¬ì„± ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜ ì´ë¯¸ ë‹¬ì„±ëœ ëª©í‘œ ë¡œì§“ ë§ˆìŠ¤í‚¹\n",
        "            mask = self._build_action_mask()[0]          # [action_dim]\n",
        "\n",
        "            # 4) ë„¤íŠ¸ì›Œí¬ í˜¸ì¶œ: logits [1,action_dim], value [1,]\n",
        "            logits, value = self.network(batch_state, prev_action_idx)\n",
        "            logits = logits[0] + mask                    # [action_dim]\n",
        "\n",
        "            # 5) legal_actions ì— ë”°ë¼ ê°€ëŠ¥í•œ ëª©í‘œë§Œ ë‚¨ê¸°ê¸°\n",
        "            allowed = {act for act, _ in legal_actions}\n",
        "            type_mask = torch.full_like(logits, -1e9)\n",
        "            for idx, t_str in self.idx_to_type.items():\n",
        "                if t_str in allowed:\n",
        "                    type_mask[idx] = 0.0\n",
        "            masked_logits = logits + type_mask\n",
        "\n",
        "            dist = Categorical(logits=masked_logits)\n",
        "\n",
        "            # [í•µì‹¬ ìˆ˜ì •] greedy(í‰ê°€) ëª¨ë“œì™€ í•™ìŠµ ëª¨ë“œì˜ ë°˜í™˜ ê°’ì„ ë¶„ë¦¬í•˜ì—¬ ëª…í™•íˆ í•¨\n",
        "            if greedy:\n",
        "                type_idx = torch.argmax(masked_logits)\n",
        "                self.network.train() # ëª¨ë“œ ë³µê·€\n",
        "                return type_idx.item(), None, None, None, None\n",
        "\n",
        "            # í•™ìŠµ ëª¨ë“œ\n",
        "            type_idx = dist.sample() # 0-dim í…ì„œ\n",
        "            logp     = dist.log_prob(type_idx)\n",
        "            ent      = dist.entropy()\n",
        "\n",
        "            # PPO ì €ì¥ì„ ìœ„í•´ (1,) í˜•íƒœë¡œ ë³€í™˜\n",
        "            action_tensor = type_idx.unsqueeze(0)\n",
        "\n",
        "        # 7) í•™ìŠµ ëª¨ë“œ ë³µê·€\n",
        "        self.network.train()\n",
        "\n",
        "        # [í•µì‹¬ ìˆ˜ì •] í•™ìŠµì— í•„ìš”í•œ ëª¨ë“  ê°’ì„ ì˜¬ë°”ë¥¸ ìˆœì„œì™€ íƒ€ì…ìœ¼ë¡œ ë°˜í™˜\n",
        "        return action_tensor, logp, ent, value[0], mask\n",
        "\n",
        "\n",
        "    def update(self, storage: PPOStorage) -> dict:\n",
        "        \"\"\"ì €ì¥ëœ ê²½í—˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ PPO ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\"\"\"\n",
        "        # ë§ˆì§€ë§‰ ìƒíƒœì˜ ê°€ì¹˜ ê³„ì‚°\n",
        "        with torch.no_grad():\n",
        "            # ë§ˆì§€ë§‰ ìƒíƒœëŠ” storage.obsì— ì €ì¥ë˜ì–´ ìˆì§€ë§Œ, ì´ì „ ì•¡ì…˜ì´ í•„ìš”í•¨\n",
        "            # ì´ ë¡œì§ì€ ê°„ë‹¨í•˜ê²Œ ë§ˆì§€ë§‰ obsì™€ ë§ˆì§€ë§‰ actionì„ ê°€ì ¸ì™€ì„œ ì²˜ë¦¬í•´ì•¼ í•¨\n",
        "            last_obs = storage.obs[-1].unsqueeze(0).to(self.device)\n",
        "            last_prev_action = storage.actions[-2] if storage.step > 1 else torch.zeros(1, dtype=torch.long, device=self.device)\n",
        "\n",
        "            _, last_value = self.network(last_obs, last_prev_action)\n",
        "\n",
        "\n",
        "        # GAEì™€ Return ê³„ì‚°\n",
        "        storage.compute_returns_and_advantages(last_value, self.config.MANAGER_GAMMA, self.config.PPO_GAE_LAMBDA)\n",
        "\n",
        "        # í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„\n",
        "        # obsëŠ” PPOStorageì— tensorë¡œ ì €ì¥ë˜ë„ë¡ ìˆ˜ì •ë˜ì—ˆìŒì„ ê°€ì •\n",
        "        b_obs = torch.stack(list(storage.obs)).to(self.device)\n",
        "        b_actions = storage.actions.squeeze(-1).to(self.device)\n",
        "        b_logprobs = storage.logprobs.to(self.device)\n",
        "        b_returns = storage.returns.to(self.device)\n",
        "        b_advantages = storage.advantages.to(self.device)\n",
        "        b_masks = storage.masks.to(self.device)\n",
        "\n",
        "        # ì´ì „ ì•¡ì…˜(b_prev_actions) ë°°ì¹˜ ìƒì„±\n",
        "        b_prev_actions = torch.cat(\n",
        "            (torch.zeros(1, dtype=torch.long, device=self.device), b_actions[:-1]),\n",
        "            dim=0\n",
        "        )\n",
        "\n",
        "        batch_size = storage.step\n",
        "        minibatch_size = max(1, batch_size // self.config.PPO_NUM_MINIBATCHES)\n",
        "\n",
        "        # PPO ì—…ë°ì´íŠ¸ ë£¨í”„\n",
        "        for _ in range(self.config.PPO_UPDATE_EPOCHS):\n",
        "            perm_indices = np.random.permutation(batch_size)\n",
        "            for start in range(0, batch_size, minibatch_size):\n",
        "                end = start + minibatch_size\n",
        "                mb_idx = perm_indices[start:end]\n",
        "\n",
        "                # ë¯¸ë‹ˆë°°ì¹˜ ë°ì´í„° ìƒì„±\n",
        "                mb_states = b_obs[mb_idx]\n",
        "                mb_actions = b_actions[mb_idx]\n",
        "                mb_logprobs = b_logprobs[mb_idx]\n",
        "                mb_returns = b_returns[mb_idx]\n",
        "                mb_advantages = b_advantages[mb_idx]\n",
        "                mb_masks = b_masks[mb_idx]\n",
        "                mb_prev_actions = b_prev_actions[mb_idx] # ì´ì „ ì•¡ì…˜ ë¯¸ë‹ˆë°°ì¹˜\n",
        "\n",
        "                # ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ìƒˆë¡œìš´ ë¡œê·¸í™•ë¥ , ê°€ì¹˜, ì—”íŠ¸ë¡œí”¼ ê³„ì‚°\n",
        "                logits, new_values = self.network(mb_states, mb_prev_actions)\n",
        "\n",
        "                # ì €ì¥ëœ ë§ˆìŠ¤í¬ë¥¼ ì ìš©í•˜ì—¬ í™•ë¥  ë¶„í¬ ì¬ê³„ì‚°\n",
        "                final_logits = logits + mb_masks\n",
        "                dist = Categorical(logits=final_logits)\n",
        "                new_logprobs = dist.log_prob(mb_actions)\n",
        "                entropy = dist.entropy()\n",
        "\n",
        "                # PPO ì†ì‹¤ ê³„ì‚°\n",
        "                log_ratio = new_logprobs - mb_logprobs\n",
        "                ratio = log_ratio.exp()\n",
        "\n",
        "                surr1 = mb_advantages * ratio\n",
        "                surr2 = mb_advantages * torch.clamp(ratio, 1 - self.config.PPO_CLIP_COEF, 1 + self.config.PPO_CLIP_COEF)\n",
        "\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "                value_loss = F.mse_loss(new_values, mb_returns)\n",
        "\n",
        "                loss = (policy_loss +\n",
        "                        self.config.PPO_VALUE_COEF * value_loss -\n",
        "                        self.config.MANAGER_ENTROPY_COEF * entropy.mean())\n",
        "\n",
        "                # ì—­ì „íŒŒ ë° ìµœì í™”\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.network.parameters(), self.config.PPO_MAX_GRAD_NORM)\n",
        "                self.optimizer.step()\n",
        "\n",
        "        return {\n",
        "            \"policy_loss\": policy_loss.item(),\n",
        "            \"value_loss\": value_loss.item(),\n",
        "            \"entropy\": entropy.mean().item()\n",
        "        }\n",
        "\n",
        "class WorkerAgent:\n",
        "    \"\"\"í•˜ìœ„ ë ˆë²¨ì˜ ì„¸ë¶€ í–‰ë™ì„ ê²°ì •í•˜ëŠ” Worker ì—ì´ì „íŠ¸.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        node_feature_size: int,\n",
        "        global_feature_size: int,\n",
        "        max_cars: int,\n",
        "        max_nodes: int,\n",
        "        config: Config,\n",
        "        device: torch.device\n",
        "    ):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        # v11 ì‹œê·¸ë‹ˆì²˜ì— ë§ì¶° Config ê°ì²´ë§Œ ë„˜ê¸°ë„ë¡ ìˆ˜ì •\n",
        "        self.network = WorkerNetwork(\n",
        "            node_feature_size,\n",
        "            global_feature_size,\n",
        "            max_cars,\n",
        "            max_nodes,\n",
        "            config\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.optimizer = optim.Adam(\n",
        "            self.network.parameters(),\n",
        "            lr=config.WORKER_LR,\n",
        "            eps=1e-5\n",
        "        )\n",
        "\n",
        "        # í–‰ë™ íƒ€ì… â†” ì¸ë±ìŠ¤ ë§¤í•‘\n",
        "        self.type_to_idx = {'LOAD': 0, 'UNLOAD': 1, 'RELOCATE_INTERNAL': 2}\n",
        "        self.idx_to_type = {v: k for k, v in self.type_to_idx.items()}\n",
        "\n",
        "\n",
        "    def get_action_and_value(\n",
        "                              self,\n",
        "                              batch_graph: Batch,           # â† Dataê°€ ì•„ë‹ˆë¼ pre-batched ê·¸ë˜í”„ë¥¼ ë°›ìŠµë‹ˆë‹¤\n",
        "                              legal_actions: list,\n",
        "                              goal_embedding: torch.Tensor,\n",
        "                              greedy: bool = False\n",
        "                            ) -> tuple:\n",
        "\n",
        "        \"\"\"í˜„ì¬ ìƒíƒœì™€ ëª©í‘œì— ë”°ë¼ ì„¸ë¶€ í–‰ë™ì„ ê²°ì •í•©ë‹ˆë‹¤.\"\"\"\n",
        "        self.network.eval() # í‰ê°€ ëª¨ë“œ\n",
        "        with torch.no_grad():\n",
        "            type_logits, load_logits, unload_logits, relocate_logits, value, _ = self.network(batch_graph, goal_embedding)\n",
        "            type_logits, load_logits, unload_logits, relocate_logits = type_logits[0], load_logits[0], unload_logits[0], relocate_logits[0]\n",
        "\n",
        "            # --- í•©ë²•ì ì¸ í–‰ë™(Legal Action)ì— ëŒ€í•œ ë§ˆìŠ¤í‚¹ ---\n",
        "            # ê° í–‰ë™ íƒ€ì…ê³¼ ì°¨ëŸ‰ IDì— ëŒ€í•´ ê°€ëŠ¥í•œ í–‰ë™ë§Œ ë‚¨ê¹ë‹ˆë‹¤.\n",
        "            allowed_types_str = {act for act, _ in legal_actions}\n",
        "            type_mask = torch.full_like(type_logits, -1e9)\n",
        "            for idx, t_str in self.idx_to_type.items():\n",
        "                if t_str in allowed_types_str:\n",
        "                    type_mask[idx] = 0.0\n",
        "\n",
        "            allowed_load_ids = {aid for (act, aid) in legal_actions if act == 'LOAD'}\n",
        "            load_mask = torch.full_like(load_logits, -1e9)\n",
        "            for idx in allowed_load_ids: load_mask[idx] = 0.0\n",
        "\n",
        "            allowed_unload_ids = {aid for (act, aid) in legal_actions if act == 'UNLOAD'}\n",
        "            unload_mask = torch.full_like(unload_logits, -1e9)\n",
        "            for idx in allowed_unload_ids: unload_mask[idx] = 0.0\n",
        "\n",
        "            allowed_relocate_ids = {aid for (act, aid) in legal_actions if act == 'RELOCATE_INTERNAL'}\n",
        "            relocate_mask = torch.full_like(relocate_logits, -1e9)\n",
        "            for idx in allowed_relocate_ids: relocate_mask[idx] = 0.0\n",
        "\n",
        "            # ë§ˆìŠ¤í¬ ì ìš©\n",
        "            masked_type_logits = type_logits + type_mask\n",
        "\n",
        "            # --- ê³„ì¸µì  ìƒ˜í”Œë§ (Hierarchical Sampling) ---\n",
        "            # 1. í–‰ë™ íƒ€ì… ê²°ì •\n",
        "            type_dist = Categorical(logits=masked_type_logits)\n",
        "            type_idx = torch.argmax(masked_type_logits) if greedy else type_dist.sample()\n",
        "            type_str = self.idx_to_type[int(type_idx.item())]\n",
        "\n",
        "            # 2. ê²°ì •ëœ íƒ€ì…ì— ë”°ë¼ ì°¨ëŸ‰ ID ê²°ì •\n",
        "            car_idx_tensor = torch.tensor(-1, device=self.device, dtype=torch.long)\n",
        "            car_dist = None\n",
        "            if type_str == 'LOAD':\n",
        "                masked_logits = load_logits + load_mask\n",
        "                car_dist = Categorical(logits=masked_logits)\n",
        "                car_idx_tensor = torch.argmax(masked_logits) if greedy else car_dist.sample()\n",
        "            elif type_str == 'UNLOAD':\n",
        "                masked_logits = unload_logits + unload_mask\n",
        "                car_dist = Categorical(logits=masked_logits)\n",
        "                car_idx_tensor = torch.argmax(masked_logits) if greedy else car_dist.sample()\n",
        "            elif type_str == 'RELOCATE_INTERNAL':\n",
        "                masked_logits = relocate_logits + relocate_mask\n",
        "                car_dist = Categorical(logits=masked_logits)\n",
        "                car_idx_tensor = torch.argmax(masked_logits) if greedy else car_dist.sample()\n",
        "\n",
        "            action_tuple = (type_str, int(car_idx_tensor.item()))\n",
        "\n",
        "            # í•™ìŠµ ëª¨ë“œì¼ ë•Œë§Œ ë¡œê·¸ í™•ë¥ ê³¼ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°\n",
        "            if greedy:\n",
        "                return action_tuple, None, None, None, value.squeeze(0)\n",
        "\n",
        "            action_tensor = torch.tensor([type_idx.item(), car_idx_tensor.item()], device=self.device)\n",
        "            log_prob_type = type_dist.log_prob(type_idx)\n",
        "            ent_type = type_dist.entropy()\n",
        "\n",
        "            log_prob_car = torch.tensor(0.0, device=self.device)\n",
        "            ent_car = torch.tensor(0.0, device=self.device)\n",
        "            if car_dist is not None:\n",
        "                log_prob_car = car_dist.log_prob(car_idx_tensor)\n",
        "                ent_car = car_dist.entropy()\n",
        "\n",
        "            total_log_prob = log_prob_type + log_prob_car\n",
        "            total_entropy = ent_type + ent_car\n",
        "\n",
        "        self.network.train() # í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜\n",
        "        return action_tuple, action_tensor, total_log_prob, total_entropy, value.squeeze(0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _traj_success(traj):\n",
        "        meta = getattr(traj[-1][0], \"meta\", {})\n",
        "        return meta.get(\"delivered\", -1) == meta.get(\"total\", -2)\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_one(path):\n",
        "        with open(path, \"rb\") as f:\n",
        "            t = pickle.load(f)\n",
        "        if WorkerAgent._traj_success(t):\n",
        "            return t\n",
        "        logging.info(f\"  Â· drop FAILED traj  â†’ {os.path.basename(path)}\")\n",
        "        return []\n",
        "\n",
        "    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    #  (WorkerAgent ë©”ì†Œë“œ)  â”€  Expert-pkl ëª¨ë°© í•™ìŠµ ë£¨í‹´\n",
        "    #â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "    def pretrain_with_imitation(\n",
        "        self,\n",
        "        expert_data_paths: list[str],\n",
        "        epochs: int,\n",
        "        lr: float,\n",
        "        batch_size: int,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        ì „ë¬¸ê°€(pkl) ê¶¤ì ì„ ì´ìš©í•´ Worker ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ì „ í•™ìŠµí•œë‹¤.\n",
        "        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        â–¸ expert_data_paths : '*.pkl' íŒŒì¼ ëª©ë¡\n",
        "        â–¸ ê° pkl = [(state : Data, action : (str, int)), â€¦]\n",
        "        \"\"\"\n",
        "        logging.info(\"[Phase 1] Starting Imitation Learning for Worker Agentâ€¦\")\n",
        "\n",
        "        # 1) ê¶¤ì  ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        expert_pairs: list[tuple[Data, tuple[str, int]]] = []\n",
        "        valid_types = set(self.type_to_idx.keys())      # {'LOAD', â€¦}\n",
        "\n",
        "        for path in expert_data_paths:\n",
        "            if not os.path.exists(path):\n",
        "                logging.warning(f\"  Â· Not found â†’ {path}\")\n",
        "                continue\n",
        "            with open(path, \"rb\") as f:\n",
        "                traj = pickle.load(f)\n",
        "            expert_pairs.extend( (s, a) for s, a in traj if a[0] in valid_types )\n",
        "\n",
        "        if not expert_pairs:\n",
        "            logging.warning(\"  Â· No usable expert samples â†’ skip.\")\n",
        "            return\n",
        "\n",
        "        max_n = getattr(self.config, \"MAX_EXPERT_SAMPLES\", None)\n",
        "        if max_n and len(expert_pairs) > max_n:\n",
        "            random.shuffle(expert_pairs)\n",
        "            expert_pairs = expert_pairs[:max_n]\n",
        "        logging.info(f\"  Â· Total Samples: {len(expert_pairs):,}\")\n",
        "\n",
        "        # 2) ì˜µí‹°ë§ˆì´ì € & í•™ìŠµ ë£¨í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "        optim_ = optim.Adam(self.network.parameters(), lr=lr)\n",
        "        self.network.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            random.shuffle(expert_pairs)\n",
        "            total_loss, nb = 0.0, 0\n",
        "\n",
        "            for idx in range(0, len(expert_pairs), batch_size):\n",
        "                batch = expert_pairs[idx: idx + batch_size]\n",
        "                if not batch: continue\n",
        "\n",
        "                states, acts = zip(*batch)\n",
        "                g = Batch.from_data_list(states).to(self.device)\n",
        "\n",
        "                # action â†’ í…ì„œ\n",
        "                a_types = torch.as_tensor(\n",
        "                    [self.type_to_idx[a[0]] for a in acts],\n",
        "                    device=self.device, dtype=torch.long)\n",
        "                a_cars  = torch.as_tensor(\n",
        "                    [a[1] for a in acts],\n",
        "                    device=self.device, dtype=torch.long)\n",
        "\n",
        "                dummy_goal = torch.zeros(\n",
        "                    g.num_graphs, self.config.GOAL_EMBED_DIM,\n",
        "                    device=self.device)\n",
        "\n",
        "                # â”€â”€ forward & loss â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "                with torch.set_grad_enabled(True):\n",
        "                    t_logit, l_logit, u_logit, r_logit, _v, _ = \\\n",
        "                        self.network(g, dummy_goal)\n",
        "\n",
        "                    loss_type = F.cross_entropy(t_logit, a_types)\n",
        "\n",
        "                    loss_load   = F.cross_entropy(l_logit, a_cars, reduction=\"none\")\n",
        "                    loss_unload = F.cross_entropy(u_logit, a_cars, reduction=\"none\")\n",
        "                    loss_reloc  = F.cross_entropy(r_logit, a_cars, reduction=\"none\")\n",
        "\n",
        "                    m_load     = (a_types == self.type_to_idx['LOAD'])\n",
        "                    m_unload   = (a_types == self.type_to_idx['UNLOAD'])\n",
        "                    m_reloc    = (a_types == self.type_to_idx['RELOCATE_INTERNAL'])\n",
        "\n",
        "                    loss_car = loss_type.new_zeros(1)  # grad X í…ì„œ\n",
        "                    if m_load.any().item():\n",
        "                        loss_car = loss_car + loss_load[m_load].mean()\n",
        "                    if m_unload.any().item():\n",
        "                        loss_car = loss_car + loss_unload[m_unload].mean()\n",
        "                    if m_reloc.any().item():\n",
        "                        loss_car = loss_car + loss_reloc[m_reloc].mean()\n",
        "\n",
        "                    loss = loss_type + loss_car\n",
        "\n",
        "                    # backward\n",
        "                    optim_.zero_grad()\n",
        "                    loss.backward()\n",
        "                    nn.utils.clip_grad_norm_(self.network.parameters(),\n",
        "                                             self.config.PPO_MAX_GRAD_NORM)\n",
        "                    optim_.step()\n",
        "\n",
        "                total_loss += loss.item(); nb += 1\n",
        "\n",
        "            if nb and (epoch + 1) % 10 == 0:\n",
        "                logging.info(f\"    Epoch {epoch+1:4d}/{epochs}  \"\n",
        "                             f\"avg-loss {total_loss/nb:.4f}\")\n",
        "\n",
        "        logging.info(\"[Phase 1] Imitation Learning Finished.\")\n",
        "\n",
        "\n",
        "    def evaluate_actions(self, states: list[Data], actions: torch.Tensor, goal_embedding: torch.Tensor) -> tuple:\n",
        "      batch_data = Batch.from_data_list(states).to(self.device)\n",
        "\n",
        "      \"\"\"\n",
        "      PPO ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´, ì£¼ì–´ì§„ ìƒíƒœ(states)ì—ì„œ íŠ¹ì • í–‰ë™(actions)ì„ í–ˆì„ ë•Œì˜\n",
        "      ë¡œê·¸ í™•ë¥ (log_prob), ì—”íŠ¸ë¡œí”¼(entropy), ê°€ì¹˜(value)ë¥¼ ë‹¤ì‹œ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "\n",
        "      Args:\n",
        "          states (list[Data]): ìƒíƒœ(ê·¸ë˜í”„) ë°ì´í„°ì˜ ë¦¬ìŠ¤íŠ¸.\n",
        "          actions (torch.Tensor): [ì•¡ì…˜ íƒ€ì…, ì°¨ëŸ‰ ID] í˜•íƒœì˜ í–‰ë™ í…ì„œ.\n",
        "          goal_embedding (torch.Tensor): í˜„ì¬ ëª©í‘œì— ëŒ€í•œ ì„ë² ë”© í…ì„œ.\n",
        "\n",
        "      Returns:\n",
        "          tuple: (ë¡œê·¸ í™•ë¥ , ì—”íŠ¸ë¡œí”¼, ê°€ì¹˜) í…ì„œ.\n",
        "      \"\"\"\n",
        "      type_logits, load_logits, unload_logits, relocate_logits, values, graph_emb = \\\n",
        "          self.network(batch_data, goal_embedding)\n",
        "\n",
        "      action_types = actions[:, 0]\n",
        "      action_cars  = actions[:, 1]\n",
        "\n",
        "      # ê° í–‰ë™ í—¤ë“œì— ëŒ€í•œ í™•ë¥  ë¶„í¬ ìƒì„±\n",
        "      type_dist      = Categorical(logits=type_logits)\n",
        "      load_dist      = Categorical(logits=load_logits)\n",
        "      unload_dist    = Categorical(logits=unload_logits)\n",
        "      relocate_dist  = Categorical(logits=relocate_logits)\n",
        "\n",
        "      # í–‰ë™ íƒ€ì… ë¡œê·¸ í™•ë¥ \n",
        "      log_probs_type = type_dist.log_prob(action_types)\n",
        "\n",
        "      # í•˜ìœ„ í–‰ë™ ë¡œê·¸ í™•ë¥  ë¯¸ë¦¬ ê³„ì‚°\n",
        "      log_probs_load     = load_dist.log_prob(action_cars)\n",
        "      log_probs_unload   = unload_dist.log_prob(action_cars)\n",
        "      log_probs_relocate = relocate_dist.log_prob(action_cars)\n",
        "\n",
        "      # ì‹¤ì œ ì·¨í•œ íƒ€ì…ì— í•´ë‹¹í•˜ëŠ” í•˜ìœ„ í–‰ë™ í™•ë¥ ë§Œ ì„ íƒ\n",
        "      mask_load     = (action_types == self.type_to_idx['LOAD']).float()\n",
        "      mask_unload   = (action_types == self.type_to_idx['UNLOAD']).float()\n",
        "      mask_relocate = (action_types == self.type_to_idx['RELOCATE_INTERNAL']).float()\n",
        "\n",
        "      log_probs = log_probs_type \\\n",
        "                  + log_probs_load     * mask_load \\\n",
        "                  + log_probs_unload   * mask_unload \\\n",
        "                  + log_probs_relocate * mask_relocate\n",
        "\n",
        "      # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°\n",
        "      type_probs = F.softmax(type_logits, dim=-1)\n",
        "      entropy = (\n",
        "          type_dist.entropy()\n",
        "          + type_probs[:, self.type_to_idx['LOAD']]     * load_dist.entropy()\n",
        "          + type_probs[:, self.type_to_idx['UNLOAD']]   * unload_dist.entropy()\n",
        "          + type_probs[:, self.type_to_idx['RELOCATE_INTERNAL']] * relocate_dist.entropy()\n",
        "      )\n",
        "\n",
        "      return log_probs, entropy, values\n",
        "\n",
        "    def update(self, storage: PPOStorage, goal_embedding: torch.Tensor) -> dict:\n",
        "        \"\"\"ì €ì¥ëœ Workerì˜ ê²½í—˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ PPO ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\"\"\"\n",
        "        self.network.train()\n",
        "\n",
        "        # 1) ë§ˆì§€ë§‰ ìƒíƒœë¥¼ ë°°ì¹˜ë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.\n",
        "        last_state = storage.obs[storage.step - 1]\n",
        "        last_state_batch = Batch.from_data_list([last_state]).to(self.device)\n",
        "        # ë§ˆì§€ë§‰ ìƒíƒœì˜ ê°€ì¹˜ ê³„ì‚°\n",
        "        with torch.no_grad():\n",
        "            # network ë°˜í™˜: (type_logits, load_logits, unload_logits, relocate_logits, critic_value, graph_emb)\n",
        "            _, _, _, _, critic_value, _ = self.network(last_state_batch, goal_embedding)\n",
        "            last_value = critic_value.squeeze(0)\n",
        "\n",
        "\n",
        "\n",
        "        # GAEì™€ Return ê³„ì‚°\n",
        "        storage.compute_returns_and_advantages(last_value, self.config.WORKER_GAMMA, self.config.PPO_GAE_LAMBDA)\n",
        "\n",
        "        # í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (ë¦¬ìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ ë‘ê³ , í…ì„œëŠ” deviceë¡œ ì´ë™)\n",
        "        b_obs = storage.obs # ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ deviceë¡œ ì˜®ê¸°ì§€ ì•ŠìŒ\n",
        "        b_actions = storage.actions.to(self.device)\n",
        "        b_logprobs = storage.logprobs.to(self.device)\n",
        "        b_returns = storage.returns.to(self.device)\n",
        "        b_advantages = storage.advantages.to(self.device)\n",
        "\n",
        "        batch_size = storage.step\n",
        "        if batch_size == 0: return {}\n",
        "        minibatch_size = max(1, batch_size // self.config.PPO_NUM_MINIBATCHES)\n",
        "\n",
        "        # PPO ì—…ë°ì´íŠ¸ ë£¨í”„\n",
        "        for _ in range(self.config.PPO_UPDATE_EPOCHS):\n",
        "            perm_indices = np.random.permutation(batch_size)\n",
        "            for start in range(0, batch_size, minibatch_size):\n",
        "                end = start + minibatch_size\n",
        "                mb_idx = perm_indices[start:end]\n",
        "\n",
        "                # ë¯¸ë‹ˆë°°ì¹˜ ë°ì´í„° ìƒì„±\n",
        "                mb_states = [b_obs[i] for i in mb_idx]\n",
        "                mb_actions = b_actions[mb_idx]\n",
        "                mb_logprobs = b_logprobs[mb_idx]\n",
        "                mb_returns = b_returns[mb_idx]\n",
        "                mb_advantages = b_advantages[mb_idx]\n",
        "\n",
        "                new_logprobs, entropy, new_values = self.evaluate_actions(mb_states, mb_actions, goal_embedding)\n",
        "\n",
        "                # PPO ì†ì‹¤ ê³„ì‚°\n",
        "                log_ratio = new_logprobs - mb_logprobs\n",
        "                ratio = log_ratio.exp()\n",
        "\n",
        "                surr1 = mb_advantages * ratio\n",
        "                surr2 = mb_advantages * torch.clamp(ratio, 1 - self.config.PPO_CLIP_COEF, 1 + self.config.PPO_CLIP_COEF)\n",
        "\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "                value_loss = F.mse_loss(new_values, mb_returns)\n",
        "\n",
        "                loss = (policy_loss +\n",
        "                        self.config.PPO_VALUE_COEF * value_loss -\n",
        "                        self.config.WORKER_ENTROPY_COEF * entropy.mean())\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.network.parameters(), self.config.PPO_MAX_GRAD_NORM)\n",
        "                self.optimizer.step()\n",
        "\n",
        "        # í•™ìŠµ ë¡œê·¸ë¥¼ ìœ„í•´ ì†ì‹¤ ê°’ ë°˜í™˜\n",
        "        return {\n",
        "            \"policy_loss\": policy_loss.item(),\n",
        "            \"value_loss\": value_loss.item(),\n",
        "            \"entropy\": entropy.mean().item()\n",
        "        }\n",
        "\n",
        "# ==============================================================================\n",
        "# ì„¹ì…˜ 5: í‰ê°€ ë° ë©”ì¸ ë£¨í”„ (ë¦¬íŒ©í† ë§ ë²„ì „)\n",
        "# ==============================================================================\n",
        "import time\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def evaluate_agent(manager_agent: ManagerAgent, worker_agent: WorkerAgent, problems: list[dict], max_num_ports: int, config: Config):\n",
        "    \"\"\"\n",
        "    í˜„ì¬ ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ , ë‹¤ì–‘í•œ ì§€í‘œë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    logging.info(\"=\" * 20 + \" AGENT EVALUATION START \" + \"=\" * 20)\n",
        "    manager_agent.network.eval()\n",
        "    worker_agent.network.eval()\n",
        "\n",
        "    total_success_count = 0\n",
        "    total_costs = []\n",
        "    total_relocations = []\n",
        "    manager_action_counts = collections.defaultdict(int)\n",
        "\n",
        "    original_env_wrapper = manager_agent.env_wrapper\n",
        "    MAX_EVAL_MANAGER_STEPS = 500\n",
        "\n",
        "    # ğŸ’¡ --- [í•µì‹¬] í™˜ê²½ ê°ì²´ë¥¼ ë£¨í”„ ë°–ì—ì„œ í•œ ë²ˆë§Œ ìƒì„± --- ğŸ’¡\n",
        "    # ì²« ë²ˆì§¸ ë¬¸ì œë¡œ h_envë¥¼ ì´ˆê¸°í™”í•˜ê³ , ì´í›„ì—ëŠ” resetìœ¼ë¡œ ì¬ì‚¬ìš©\n",
        "    prob = random.choice(problems)\n",
        "    h_env = HierarchicalEnvWrapper(prob, max_num_ports, worker_agent, config)\n",
        "    manager_agent.env_wrapper = h_env\n",
        "\n",
        "    for episode_num in range(config.EVAL_EPISODES):\n",
        "        # ğŸ’¡ [ìˆ˜ì •] ë§¤ë²ˆ ìƒˆë¡œ ìƒì„±í•˜ëŠ” ëŒ€ì‹ , reset ë©”ì„œë“œë¡œ ë¬¸ì œë§Œ êµì²´\n",
        "        if episode_num > 0:\n",
        "            prob = random.choice(problems)\n",
        "            # h_env.reset()ì€ manager_stateë¥¼ ë°˜í™˜í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ì§ì ‘ ìƒíƒœë¥¼ ê°€ì ¸ì™€ì•¼ í•©ë‹ˆë‹¤.\n",
        "            h_env.reset(prob_data=prob)\n",
        "            manager_state = h_env._get_manager_state()\n",
        "        else:\n",
        "            # ì²« ì—í”¼ì†Œë“œëŠ” ì´ë¯¸ h_env ìƒì„± ì‹œ ë¦¬ì…‹ë¨\n",
        "            manager_state = h_env._get_manager_state()\n",
        "\n",
        "        if hasattr(manager_agent, 'prev_action_idx'):\n",
        "             manager_agent.prev_action_idx.zero_()\n",
        "\n",
        "        overall_done = False\n",
        "        episode_cost = 0.0\n",
        "        current_episode_steps = 0\n",
        "\n",
        "        while not overall_done:\n",
        "            if current_episode_steps > MAX_EVAL_MANAGER_STEPS:\n",
        "                logging.warning(f\"\\nEval Episode [{episode_num+1}] reached max steps limit. Breaking loop.\")\n",
        "                break\n",
        "\n",
        "            with torch.no_grad():\n",
        "                legal_actions = h_env.ship_env.get_legal_actions(for_worker=False)\n",
        "                prev_action_idx = manager_agent.prev_action_idx if hasattr(manager_agent, 'prev_action_idx') else torch.zeros(1, dtype=torch.long, device=manager_agent.device)\n",
        "\n",
        "                manager_action_idx, _, _, _, _ = manager_agent.get_action_and_value(\n",
        "                    manager_state.to(manager_agent.device), legal_actions, prev_action_idx, greedy=True\n",
        "                )\n",
        "\n",
        "            if hasattr(manager_agent, 'prev_action_idx'):\n",
        "                manager_agent.prev_action_idx = torch.tensor(\n",
        "                    [manager_action_idx], dtype=torch.long, device=manager_agent.device\n",
        "                )\n",
        "\n",
        "            manager_action_counts[h_env.manager_action_map[manager_action_idx]] += 1\n",
        "            current_goal_str = h_env.manager_action_map[manager_action_idx]\n",
        "            print(f\"\\r  Eval Ep[{episode_num+1}/{config.EVAL_EPISODES}] Step[{current_episode_steps+1}]: Trying Goal -> {current_goal_str.ljust(25)}\", end=\"\")\n",
        "\n",
        "            manager_state, _, overall_done, info = h_env.step(\n",
        "                manager_action_idx, greedy_worker=True\n",
        "            )\n",
        "            episode_cost += info.get('cost', 0.0)\n",
        "            current_episode_steps += 1\n",
        "\n",
        "        print()\n",
        "\n",
        "        if len(h_env.ship_env.delivered_cars) == h_env.ship_env.total_cars:\n",
        "            total_success_count += 1\n",
        "        total_costs.append(episode_cost)\n",
        "        total_relocations.append(h_env.ship_env.relocations_this_episode)\n",
        "        logging.info(f\"  Eval Episode [{episode_num+1}/{config.EVAL_EPISODES}] Finished. Success: {len(h_env.ship_env.delivered_cars) == h_env.ship_env.total_cars}\")\n",
        "\n",
        "    # í‰ê°€ê°€ ëë‚˜ë©´ ì›ë˜ í™˜ê²½ìœ¼ë¡œ ë³µì›\n",
        "    manager_agent.env_wrapper = original_env_wrapper\n",
        "    manager_agent.network.train()\n",
        "    worker_agent.network.train()\n",
        "\n",
        "    # ìµœì¢… ê²°ê³¼ ê³„ì‚° ë° ë¡œê¹…\n",
        "    success_rate = total_success_count / config.EVAL_EPISODES\n",
        "    avg_cost = np.mean(total_costs) if total_costs else 0.0\n",
        "    avg_relocations = np.mean(total_relocations) if total_relocations else 0.0\n",
        "\n",
        "    logging.info(\"-\" * 54)\n",
        "    logging.info(f\"[EVAL RESULT] Success Rate: {success_rate*100:.1f}%\")\n",
        "    logging.info(f\"[EVAL RESULT] Average Cost: {avg_cost:.2f}\")\n",
        "    logging.info(f\"[EVAL RESULT] Average Relocations: {avg_relocations:.2f}\")\n",
        "\n",
        "    total_actions = sum(manager_action_counts.values())\n",
        "    if total_actions > 0:\n",
        "        logging.info(\"[EVAL RESULT] Manager Action Distribution:\")\n",
        "        for action_idx in sorted(h_env.manager_action_map.keys()):\n",
        "            count = manager_action_counts.get(h_env.manager_action_map[action_idx], 0)\n",
        "            action_str = h_env.manager_action_map[action_idx]\n",
        "            percentage = (count / total_actions) * 100\n",
        "            logging.info(f\"  - {action_str:<25s}: {count} times ({percentage:.1f}%)\")\n",
        "\n",
        "    logging.info(\"=\" * 22 + \" EVALUATION END \" + \"=\" * 22 + \"\\n\")\n",
        "\n",
        "    return {\n",
        "        \"success_rate\": success_rate,\n",
        "        \"avg_cost\": avg_cost,\n",
        "        \"avg_relocations\": avg_relocations\n",
        "    }\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    mp.set_start_method(\"spawn\", force=True) # ë©€í‹° í”„ë¡œì„¸\n",
        "    # --- 1. ì´ˆê¸° ì„¤ì • ë° í™˜ê²½ êµ¬ì„± ---\n",
        "    setup_logger()\n",
        "    config = Config()\n",
        "    writer = SummaryWriter(log_dir=config.LOG_DIR)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    logging.info(f\"Using device: {device}\")\n",
        "\n",
        "    assert torch.cuda.is_available(), \"CUDAê°€ í™œì„±í™”ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤!\"\n",
        "    print(\"âœ… CUDA í™œì„±í™” í™•ì¸\")\n",
        "\n",
        "    all_problem_files = [os.path.join(config.PROBLEM_DIR, f) for f in os.listdir(config.PROBLEM_DIR) if f.endswith('.json')]\n",
        "    easy_problem_files = [f for f in all_problem_files if any(name in f for name in ['prob1', 'prob2', 'prob4'])]\n",
        "    easy_problems = [json.load(open(f)) for f in easy_problem_files if os.path.exists(f)]\n",
        "    all_problems = [json.load(open(f)) for f in all_problem_files if os.path.exists(f)]\n",
        "    if not all_problems:\n",
        "        logging.error(f\"No problem files found in {config.PROBLEM_DIR}. Exiting.\")\n",
        "        exit()\n",
        "    if not easy_problems:\n",
        "        logging.warning(f\"No easy problems found. Starting with all problems.\")\n",
        "        easy_problems = all_problems\n",
        "\n",
        "    max_cars = max(sum(q for _, q in p.get('K', [])) for p in all_problems)\n",
        "    max_num_ports = max(p.get('P', 1) for p in all_problems)\n",
        "    max_nodes = max(p.get('N', 1) for p in all_problems)\n",
        "    node_feature_size = 4\n",
        "    global_feature_size = 3 + max_num_ports\n",
        "\n",
        "    # --- 2. ì—ì´ì „íŠ¸ ë° í™˜ê²½ ì´ˆê¸°í™” ---\n",
        "    worker_agent = WorkerAgent(node_feature_size, global_feature_size, max_cars, max_nodes, config, device)\n",
        "    current_prob = random.choice(easy_problems)\n",
        "    h_env = HierarchicalEnvWrapper(current_prob, max_num_ports, worker_agent, config)\n",
        "    manager_agent = ManagerAgent(config, device, h_env)\n",
        "    manager_agent.prev_action_idx = torch.zeros(1, dtype=torch.long, device=device)\n",
        "\n",
        "    # --- 3. Worker ì‚¬ì „ í›ˆë ¨ (ëª¨ë°© í•™ìŠµ) ---\n",
        "    print(\"\\nDEBUG: >>>>>>>>>> STEP 1: Calling pretrain_with_imitation NOW...\")\n",
        "    worker_agent.pretrain_with_imitation(\n",
        "        config.EXPERT_DATA_PATHS, config.IMITATION_LEARNING_EPOCHS, config.IMITATION_LR, config.IMITATION_BATCH_SIZE\n",
        "    )\n",
        "    print(\"DEBUG: <<<<<<<<<< STEP 1: pretrain_with_imitation FINISHED.\\n\")\n",
        "\n",
        "    # --- 4. ê³„ì¸µì  ê°•í™”í•™ìŠµ ë©”ì¸ ë£¨í”„ ---\n",
        "    logging.info(\"\\n[Phase 2] Starting Hierarchical Reinforcement Learning...\")\n",
        "\n",
        "    manager_state = h_env.reset(prob_data=current_prob)\n",
        "    manager_storage = PPOStorage(config.MANAGER_NUM_STEPS_PER_UPDATE, (1,), device,\n",
        "                                 state_shape=(config.MANAGER_STATE_DIM,),\n",
        "                                 manager_action_dim=config.MANAGER_ACTION_DIM)\n",
        "\n",
        "    episode_rewards, episode_costs, episode_manager_steps = 0, 0, 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Interval í†µê³„ë¥¼ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”\n",
        "    interval_rewards = 0.0\n",
        "    interval_goals = defaultdict(int)\n",
        "    interval_successes = 0\n",
        "    interval_costs = 0.0\n",
        "    interval_worker_rewards = 0.0\n",
        "    interval_worker_steps = 0\n",
        "\n",
        "    for manager_step in range(1, config.TOTAL_MANAGER_STEPS + 1):\n",
        "        is_curriculum_phase = manager_step < config.CURRICULUM_STEPS\n",
        "\n",
        "        if is_curriculum_phase:\n",
        "            possible_goals = []\n",
        "            if not h_env._is_goal_achieved('FINISH_UNLOAD'): possible_goals.append(1)\n",
        "            if not h_env._is_goal_achieved('CLEAR_TEMP'): possible_goals.append(3)\n",
        "            if not h_env._is_goal_achieved('FINISH_LOAD'): possible_goals.append(2)\n",
        "            if possible_goals: manager_action_idx = random.choice(possible_goals)\n",
        "            else: manager_action_idx = 4\n",
        "            manager_action_tensor = torch.tensor([manager_action_idx], device=device)\n",
        "            m_log_prob, m_value, m_mask = torch.tensor(0.0), torch.tensor(0.0), None\n",
        "            manager_agent.prev_action_idx = manager_action_tensor.clone()\n",
        "        else:\n",
        "            legal_actions = h_env.ship_env.get_legal_actions(for_worker=False)\n",
        "            prev_idx = manager_agent.prev_action_idx\n",
        "            with torch.no_grad():\n",
        "                manager_action_tensor, m_log_prob, _, m_value, m_mask = manager_agent.get_action_and_value(\n",
        "                    manager_state, legal_actions, prev_idx, greedy=False\n",
        "                )\n",
        "            manager_action_idx = manager_action_tensor.item()\n",
        "            manager_agent.prev_action_idx = manager_action_tensor.clone()\n",
        "\n",
        "        next_manager_state, manager_reward, overall_done, worker_info = h_env.step(manager_action_idx)\n",
        "\n",
        "        episode_rewards += manager_reward\n",
        "        episode_costs += worker_info.get('cost', 0.0)\n",
        "        episode_manager_steps += 1\n",
        "\n",
        "        # ë§¤ ìŠ¤í…ë§ˆë‹¤ Interval í†µê³„ ëˆ„ì \n",
        "        interval_rewards += manager_reward\n",
        "        interval_goals[worker_info.get('goal', 'N/A')] += 1\n",
        "        if worker_info.get('success', False):\n",
        "            interval_successes += 1\n",
        "        interval_costs += worker_info.get('cost', 0.0)\n",
        "        interval_worker_rewards += worker_info.get('worker_total_reward', 0.0) # h_env.stepì—ì„œ ì´ ê°’ì„ ë°˜í™˜í•´ì•¼ í•¨\n",
        "        interval_worker_steps += worker_info.get('steps', 0)\n",
        "\n",
        "        if not is_curriculum_phase:\n",
        "            manager_storage.add(manager_state, manager_action_tensor, m_log_prob, manager_reward, overall_done, m_value, m_mask)\n",
        "\n",
        "        manager_state = next_manager_state\n",
        "\n",
        "        if overall_done:\n",
        "            s = h_env.ship_env\n",
        "            success_ratio = len(s.delivered_cars) / s.total_cars if s.total_cars > 0 else 0\n",
        "            logging.info(f\"EPISODE DONE (M-Step: {manager_step}) | Success: {success_ratio*100:.1f}% | Total Reward: {episode_rewards:.2f} | Total Cost: {episode_costs:.2f} | Length: {episode_manager_steps} steps\")\n",
        "            writer.add_scalar(\"Episode/TotalReward\", episode_rewards, manager_step)\n",
        "            writer.add_scalar(\"Episode/TotalCost\", episode_costs, manager_step)\n",
        "            writer.add_scalar(\"Episode/SuccessRatio\", success_ratio, manager_step)\n",
        "            writer.add_scalar(\"Episode/Length\", episode_manager_steps, manager_step)\n",
        "\n",
        "            if manager_step < config.CURRICULUM_TRANSITION_STEP:\n",
        "                current_prob = random.choice(easy_problems)\n",
        "            else:\n",
        "                if manager_step - episode_manager_steps < config.CURRICULUM_TRANSITION_STEP:\n",
        "                     logging.info(\"=\"*20 + \" SWITCHING TO FULL PROBLEM SET \" + \"=\"*20)\n",
        "                current_prob = random.choice(all_problems)\n",
        "\n",
        "            manager_state = h_env.reset(prob_data=current_prob)\n",
        "            manager_agent.prev_action_idx.zero_()\n",
        "            episode_rewards, episode_costs, episode_manager_steps = 0, 0, 0\n",
        "\n",
        "        if not is_curriculum_phase and manager_storage.is_full():\n",
        "            loss_info = manager_agent.update(manager_storage)\n",
        "            if loss_info:\n",
        "                writer.add_scalar(\"Train/Manager_PolicyLoss\", loss_info[\"policy_loss\"], manager_step)\n",
        "                writer.add_scalar(\"Train/Manager_ValueLoss\", loss_info[\"value_loss\"], manager_step)\n",
        "                writer.add_scalar(\"Train/Manager_Entropy\", loss_info[\"entropy\"], manager_step)\n",
        "            manager_storage.reset()\n",
        "\n",
        "        # ğŸ’¡ --- [ìˆ˜ì •] ì£¼ê¸°ì  ë¡œê·¸ ì¶œë ¥ (ì¤‘ë³µ ì œê±° ë° ìµœì¢… ë²„ì „) --- ğŸ’¡\n",
        "        if manager_step % config.PRINT_INTERVAL_MANAGER_STEPS == 0:\n",
        "            elapsed_time = time.time() - start_time\n",
        "            steps_per_sec = config.PRINT_INTERVAL_MANAGER_STEPS / elapsed_time if elapsed_time > 0 else 0\n",
        "\n",
        "            avg_reward = interval_rewards / config.PRINT_INTERVAL_MANAGER_STEPS\n",
        "            interval_success_rate = (interval_successes / config.PRINT_INTERVAL_MANAGER_STEPS) * 100\n",
        "            avg_cost_per_step = interval_costs / config.PRINT_INTERVAL_MANAGER_STEPS\n",
        "            # ğŸ’¡ [ì¶”ê°€] Worker í‰ê·  ë³´ìƒ ê³„ì‚°\n",
        "            avg_worker_rew = interval_worker_rewards / interval_worker_steps if interval_worker_steps > 0 else 0\n",
        "\n",
        "            goal_dist_str = \", \".join([f\"{k.split('_')[-1]}:{v}\" for k, v in sorted(interval_goals.items())])\n",
        "\n",
        "            logging.info(\n",
        "                f\"M-Step {manager_step:6d} | Avg M-Rew: {avg_reward:7.2f} | \"\n",
        "                f\"W-Rew: {avg_worker_rew:6.3f} | \" # Worker í‰ê·  ë³´ìƒ ì¶œë ¥\n",
        "                f\"Success: {interval_success_rate:3.0f}% | Avg Cost: {avg_cost_per_step:8.1f} | \"\n",
        "                f\"Goals: [{goal_dist_str}] | SPS: {steps_per_sec:.2f}\"\n",
        "            )\n",
        "            goal_dist_str = \", \".join([f\"{k.split('_')[-1]}:{v}\" for k, v in sorted(interval_goals.items())])\n",
        "\n",
        "            logging.info(\n",
        "                f\"M-Step {manager_step:6d} | Avg Rew: {avg_reward:7.2f} | \"\n",
        "                f\"Success: {interval_success_rate:3.0f}% | Avg Cost: {avg_cost_per_step:8.1f} | \"\n",
        "                f\"Goals: [{goal_dist_str}] | SPS: {steps_per_sec:.2f}\"\n",
        "            )\n",
        "\n",
        "            # ë‹¤ìŒ Intervalì„ ìœ„í•´ í†µê³„ ë³€ìˆ˜ ì´ˆê¸°í™”\n",
        "            interval_worker_rewards = 0.0\n",
        "            interval_worker_steps = 0\n",
        "            interval_rewards = 0.0\n",
        "            interval_goals.clear()\n",
        "            interval_successes = 0\n",
        "            interval_costs = 0.0\n",
        "            start_time = time.time()\n",
        "\n",
        "        # ì£¼ê¸°ì  í‰ê°€ ë° ëª¨ë¸ ì €ì¥\n",
        "        if manager_step > 0 and manager_step % config.EVAL_INTERVAL_MANAGER_STEPS == 0:\n",
        "            if manager_step == config.CURRICULUM_STEPS:\n",
        "                logging.info(\"=\"*20 + \" CURRICULUM FINISHED \" + \"=\"*20)\n",
        "\n",
        "            eval_results = evaluate_agent(manager_agent, worker_agent, all_problems, max_num_ports, config)\n",
        "            writer.add_scalar(\"Eval/SuccessRate\", eval_results[\"success_rate\"] * 100.0, manager_step)\n",
        "            writer.add_scalar(\"Eval/AvgCost\", eval_results[\"avg_cost\"], manager_step)\n",
        "            writer.add_scalar(\"Eval/AvgRelocations\", eval_results[\"avg_relocations\"], manager_step)\n",
        "\n",
        "            torch.save(worker_agent.network.state_dict(), os.path.join(config.MODEL_DIR, f\"worker_model_step_{manager_step}.pth\"))\n",
        "            torch.save(manager_agent.network.state_dict(), os.path.join(config.MODEL_DIR, f\"manager_model_step_{manager_step}.pth\"))\n",
        "\n",
        "    writer.close()\n",
        "    logging.info(\"--- V12 Refactored Training Completed ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KWwYc4yKt_-G"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOb829Bfpa2/MYyVPyOKucp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}